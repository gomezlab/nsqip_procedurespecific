{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_pan.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23233, 80)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['PAN_FISTULA']\n",
    "X = data.drop(['PAN_FISTULA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for x in range(0,5):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['PAN_FISTULA'], axis=1))\n",
    "        dy_train.append(d[x]['PAN_FISTULA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['PAN_FISTULA'], axis=1))\n",
    "        dy_test.append(d[x]['PAN_FISTULA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfpreds = []\n",
    "xgbpreds = []\n",
    "model = RandomForestClassifier(n_estimators=1250, min_samples_split=2, min_samples_leaf=8, max_features='auto', max_depth=20, bootstrap=True)\n",
    "model2 = XGBClassifier(n_estimators=50, subsample=0.6, min_child_weight=10, max_depth=6, learning_rate=0.1, colsample_bytree=0.8)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model.fit(X, y)\n",
    "    model2.fit(X, y)\n",
    "    rfpreds.append(model.predict_proba(X_test))\n",
    "    xgbpreds.append(model2.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store rfpreds\n",
    "%store xgbpreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], rfpreds[x][:,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], xgbpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "input_shape = [X.shape[1]]\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "for _ in range(2):\n",
    "    model4.add(keras.layers.Dense(1000))\n",
    "    model4.add(keras.layers.BatchNormalization())\n",
    "    model4.add(keras.layers.Dropout(0.8))\n",
    "    model4.add(keras.layers.Activation(\"relu\"))\n",
    "model4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "metrics = [keras.metrics.Recall(name='Sensitivity'), keras.metrics.TrueNegatives(name='tn'), keras.metrics.AUC(name='auc'), keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.00001,\n",
    "    restore_best_weights=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 3s 38ms/step - loss: 0.8784 - Sensitivity: 0.3826 - tn: 7698.0000 - auc: 0.5346 - prc: 0.1790 - val_loss: 0.6517 - val_Sensitivity: 0.5972 - val_tn: 2647.0000 - val_auc: 0.6806 - val_prc: 0.3030\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6375 - Sensitivity: 0.1447 - tn: 10493.0000 - auc: 0.5625 - prc: 0.1954 - val_loss: 0.5294 - val_Sensitivity: 0.2284 - val_tn: 3550.0000 - val_auc: 0.6944 - val_prc: 0.3123\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.6014 - Sensitivity: 0.1470 - tn: 10654.0000 - auc: 0.5939 - prc: 0.2180 - val_loss: 0.4923 - val_Sensitivity: 0.1731 - val_tn: 3652.0000 - val_auc: 0.6983 - val_prc: 0.3152\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5895 - Sensitivity: 0.1756 - tn: 10652.0000 - auc: 0.6043 - prc: 0.2313 - val_loss: 0.4869 - val_Sensitivity: 0.1995 - val_tn: 3600.0000 - val_auc: 0.6997 - val_prc: 0.3161\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5806 - Sensitivity: 0.1668 - tn: 10602.0000 - auc: 0.6099 - prc: 0.2257 - val_loss: 0.4757 - val_Sensitivity: 0.1895 - val_tn: 3629.0000 - val_auc: 0.6999 - val_prc: 0.3159\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5713 - Sensitivity: 0.1748 - tn: 10750.0000 - auc: 0.6173 - prc: 0.2420 - val_loss: 0.4646 - val_Sensitivity: 0.1769 - val_tn: 3646.0000 - val_auc: 0.7031 - val_prc: 0.3188\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5671 - Sensitivity: 0.1708 - tn: 10705.0000 - auc: 0.6173 - prc: 0.2416 - val_loss: 0.4545 - val_Sensitivity: 0.1731 - val_tn: 3669.0000 - val_auc: 0.7059 - val_prc: 0.3227\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5481 - Sensitivity: 0.1699 - tn: 10784.0000 - auc: 0.6245 - prc: 0.2461 - val_loss: 0.4602 - val_Sensitivity: 0.2070 - val_tn: 3627.0000 - val_auc: 0.7071 - val_prc: 0.3251\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5425 - Sensitivity: 0.1659 - tn: 10755.0000 - auc: 0.6303 - prc: 0.2424 - val_loss: 0.4675 - val_Sensitivity: 0.2321 - val_tn: 3569.0000 - val_auc: 0.7083 - val_prc: 0.3273\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5350 - Sensitivity: 0.1699 - tn: 10866.0000 - auc: 0.6308 - prc: 0.2535 - val_loss: 0.4687 - val_Sensitivity: 0.2384 - val_tn: 3557.0000 - val_auc: 0.7086 - val_prc: 0.3279\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5183 - Sensitivity: 0.1567 - tn: 10853.0000 - auc: 0.6384 - prc: 0.2500 - val_loss: 0.4633 - val_Sensitivity: 0.2409 - val_tn: 3563.0000 - val_auc: 0.7105 - val_prc: 0.3299\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5048 - Sensitivity: 0.1615 - tn: 10907.0000 - auc: 0.6488 - prc: 0.2659 - val_loss: 0.4711 - val_Sensitivity: 0.2798 - val_tn: 3499.0000 - val_auc: 0.7106 - val_prc: 0.3296\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5091 - Sensitivity: 0.1575 - tn: 10903.0000 - auc: 0.6353 - prc: 0.2557 - val_loss: 0.4714 - val_Sensitivity: 0.2748 - val_tn: 3493.0000 - val_auc: 0.7113 - val_prc: 0.3292\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4958 - Sensitivity: 0.1346 - tn: 11038.0000 - auc: 0.6394 - prc: 0.2556 - val_loss: 0.4751 - val_Sensitivity: 0.2873 - val_tn: 3485.0000 - val_auc: 0.7123 - val_prc: 0.3293\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4878 - Sensitivity: 0.1408 - tn: 11008.0000 - auc: 0.6493 - prc: 0.2602 - val_loss: 0.4795 - val_Sensitivity: 0.3137 - val_tn: 3444.0000 - val_auc: 0.7106 - val_prc: 0.3282\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4782 - Sensitivity: 0.1496 - tn: 11064.0000 - auc: 0.6595 - prc: 0.2716 - val_loss: 0.4647 - val_Sensitivity: 0.2509 - val_tn: 3540.0000 - val_auc: 0.7116 - val_prc: 0.3304\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4795 - Sensitivity: 0.1231 - tn: 11099.0000 - auc: 0.6493 - prc: 0.2634 - val_loss: 0.4703 - val_Sensitivity: 0.2685 - val_tn: 3504.0000 - val_auc: 0.7098 - val_prc: 0.3285\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5637 - Sensitivity: 0.1558 - tn: 10725.0000 - auc: 0.6107 - prc: 0.2311 - val_loss: 0.4584 - val_Sensitivity: 0.2683 - val_tn: 3612.0000 - val_auc: 0.7340 - val_prc: 0.3678\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5406 - Sensitivity: 0.1589 - tn: 10834.0000 - auc: 0.6263 - prc: 0.2419 - val_loss: 0.4546 - val_Sensitivity: 0.2631 - val_tn: 3621.0000 - val_auc: 0.7335 - val_prc: 0.3693\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5175 - Sensitivity: 0.1505 - tn: 10932.0000 - auc: 0.6337 - prc: 0.2479 - val_loss: 0.4558 - val_Sensitivity: 0.2880 - val_tn: 3589.0000 - val_auc: 0.7318 - val_prc: 0.3670\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5168 - Sensitivity: 0.1549 - tn: 10849.0000 - auc: 0.6298 - prc: 0.2456 - val_loss: 0.4480 - val_Sensitivity: 0.2631 - val_tn: 3644.0000 - val_auc: 0.7328 - val_prc: 0.3698\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5075 - Sensitivity: 0.1429 - tn: 10927.0000 - auc: 0.6374 - prc: 0.2446 - val_loss: 0.4466 - val_Sensitivity: 0.2618 - val_tn: 3639.0000 - val_auc: 0.7331 - val_prc: 0.3711\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5061 - Sensitivity: 0.1287 - tn: 10972.0000 - auc: 0.6273 - prc: 0.2404 - val_loss: 0.4464 - val_Sensitivity: 0.2644 - val_tn: 3633.0000 - val_auc: 0.7314 - val_prc: 0.3690\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4862 - Sensitivity: 0.1318 - tn: 11033.0000 - auc: 0.6463 - prc: 0.2530 - val_loss: 0.4486 - val_Sensitivity: 0.2696 - val_tn: 3621.0000 - val_auc: 0.7322 - val_prc: 0.3690\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4828 - Sensitivity: 0.1296 - tn: 11073.0000 - auc: 0.6427 - prc: 0.2523 - val_loss: 0.4499 - val_Sensitivity: 0.2736 - val_tn: 3606.0000 - val_auc: 0.7323 - val_prc: 0.3687\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4732 - Sensitivity: 0.1318 - tn: 11106.0000 - auc: 0.6550 - prc: 0.2583 - val_loss: 0.4602 - val_Sensitivity: 0.3010 - val_tn: 3550.0000 - val_auc: 0.7313 - val_prc: 0.3654\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4640 - Sensitivity: 0.1202 - tn: 11180.0000 - auc: 0.6556 - prc: 0.2664 - val_loss: 0.4629 - val_Sensitivity: 0.3024 - val_tn: 3558.0000 - val_auc: 0.7305 - val_prc: 0.3626\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4668 - Sensitivity: 0.1153 - tn: 11205.0000 - auc: 0.6447 - prc: 0.2569 - val_loss: 0.4648 - val_Sensitivity: 0.3102 - val_tn: 3542.0000 - val_auc: 0.7304 - val_prc: 0.3627\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4561 - Sensitivity: 0.1126 - tn: 11245.0000 - auc: 0.6597 - prc: 0.2710 - val_loss: 0.4562 - val_Sensitivity: 0.2749 - val_tn: 3612.0000 - val_auc: 0.7307 - val_prc: 0.3653\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4474 - Sensitivity: 0.0962 - tn: 11312.0000 - auc: 0.6638 - prc: 0.2699 - val_loss: 0.4761 - val_Sensitivity: 0.3285 - val_tn: 3492.0000 - val_auc: 0.7275 - val_prc: 0.3541\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4452 - Sensitivity: 0.1073 - tn: 11261.0000 - auc: 0.6672 - prc: 0.2707 - val_loss: 0.4696 - val_Sensitivity: 0.2984 - val_tn: 3538.0000 - val_auc: 0.7278 - val_prc: 0.3544\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4395 - Sensitivity: 0.0922 - tn: 11350.0000 - auc: 0.6727 - prc: 0.2759 - val_loss: 0.4635 - val_Sensitivity: 0.2827 - val_tn: 3580.0000 - val_auc: 0.7283 - val_prc: 0.3529\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4352 - Sensitivity: 0.0908 - tn: 11361.0000 - auc: 0.6759 - prc: 0.2793 - val_loss: 0.4693 - val_Sensitivity: 0.3050 - val_tn: 3545.0000 - val_auc: 0.7275 - val_prc: 0.3504\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.5056 - Sensitivity: 0.1416 - tn: 10911.0000 - auc: 0.6432 - prc: 0.2555 - val_loss: 0.4372 - val_Sensitivity: 0.2322 - val_tn: 3653.0000 - val_auc: 0.7457 - val_prc: 0.3722\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4976 - Sensitivity: 0.1326 - tn: 10987.0000 - auc: 0.6379 - prc: 0.2570 - val_loss: 0.4474 - val_Sensitivity: 0.2477 - val_tn: 3612.0000 - val_auc: 0.7430 - val_prc: 0.3659\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4852 - Sensitivity: 0.1292 - tn: 10985.0000 - auc: 0.6494 - prc: 0.2593 - val_loss: 0.4427 - val_Sensitivity: 0.2231 - val_tn: 3637.0000 - val_auc: 0.7412 - val_prc: 0.3627\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4734 - Sensitivity: 0.1330 - tn: 11086.0000 - auc: 0.6569 - prc: 0.2721 - val_loss: 0.4557 - val_Sensitivity: 0.2698 - val_tn: 3558.0000 - val_auc: 0.7394 - val_prc: 0.3598\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4666 - Sensitivity: 0.1180 - tn: 11085.0000 - auc: 0.6591 - prc: 0.2683 - val_loss: 0.4456 - val_Sensitivity: 0.2270 - val_tn: 3633.0000 - val_auc: 0.7399 - val_prc: 0.3609\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4648 - Sensitivity: 0.1146 - tn: 11154.0000 - auc: 0.6572 - prc: 0.2725 - val_loss: 0.4583 - val_Sensitivity: 0.2724 - val_tn: 3549.0000 - val_auc: 0.7391 - val_prc: 0.3577\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4544 - Sensitivity: 0.1137 - tn: 11186.0000 - auc: 0.6682 - prc: 0.2839 - val_loss: 0.4657 - val_Sensitivity: 0.2944 - val_tn: 3512.0000 - val_auc: 0.7377 - val_prc: 0.3549\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4519 - Sensitivity: 0.1103 - tn: 11225.0000 - auc: 0.6687 - prc: 0.2845 - val_loss: 0.4644 - val_Sensitivity: 0.2840 - val_tn: 3544.0000 - val_auc: 0.7369 - val_prc: 0.3524\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4473 - Sensitivity: 0.0991 - tn: 11245.0000 - auc: 0.6696 - prc: 0.2887 - val_loss: 0.4661 - val_Sensitivity: 0.2840 - val_tn: 3532.0000 - val_auc: 0.7369 - val_prc: 0.3523\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4410 - Sensitivity: 0.0854 - tn: 11305.0000 - auc: 0.6784 - prc: 0.2905 - val_loss: 0.4707 - val_Sensitivity: 0.2853 - val_tn: 3529.0000 - val_auc: 0.7368 - val_prc: 0.3496\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4416 - Sensitivity: 0.0987 - tn: 11296.0000 - auc: 0.6693 - prc: 0.2913 - val_loss: 0.4645 - val_Sensitivity: 0.2581 - val_tn: 3573.0000 - val_auc: 0.7368 - val_prc: 0.3509\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4949 - Sensitivity: 0.1352 - tn: 10953.0000 - auc: 0.6397 - prc: 0.2534 - val_loss: 0.4359 - val_Sensitivity: 0.2602 - val_tn: 3673.0000 - val_auc: 0.7462 - val_prc: 0.3609\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4859 - Sensitivity: 0.1196 - tn: 11036.0000 - auc: 0.6440 - prc: 0.2563 - val_loss: 0.4424 - val_Sensitivity: 0.2764 - val_tn: 3656.0000 - val_auc: 0.7443 - val_prc: 0.3598\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4662 - Sensitivity: 0.1191 - tn: 11136.0000 - auc: 0.6585 - prc: 0.2741 - val_loss: 0.4494 - val_Sensitivity: 0.2995 - val_tn: 3623.0000 - val_auc: 0.7417 - val_prc: 0.3583\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4644 - Sensitivity: 0.1313 - tn: 11117.0000 - auc: 0.6576 - prc: 0.2793 - val_loss: 0.4582 - val_Sensitivity: 0.3266 - val_tn: 3578.0000 - val_auc: 0.7390 - val_prc: 0.3532\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4625 - Sensitivity: 0.1183 - tn: 11192.0000 - auc: 0.6567 - prc: 0.2807 - val_loss: 0.4464 - val_Sensitivity: 0.2900 - val_tn: 3650.0000 - val_auc: 0.7399 - val_prc: 0.3583\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4528 - Sensitivity: 0.1230 - tn: 11208.0000 - auc: 0.6652 - prc: 0.2825 - val_loss: 0.4566 - val_Sensitivity: 0.3157 - val_tn: 3603.0000 - val_auc: 0.7394 - val_prc: 0.3553\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4456 - Sensitivity: 0.1165 - tn: 11260.0000 - auc: 0.6730 - prc: 0.2941 - val_loss: 0.4644 - val_Sensitivity: 0.3252 - val_tn: 3561.0000 - val_auc: 0.7367 - val_prc: 0.3492\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4409 - Sensitivity: 0.1074 - tn: 11305.0000 - auc: 0.6776 - prc: 0.2938 - val_loss: 0.4617 - val_Sensitivity: 0.3225 - val_tn: 3570.0000 - val_auc: 0.7369 - val_prc: 0.3492\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4378 - Sensitivity: 0.0996 - tn: 11317.0000 - auc: 0.6808 - prc: 0.3004 - val_loss: 0.4582 - val_Sensitivity: 0.2995 - val_tn: 3604.0000 - val_auc: 0.7367 - val_prc: 0.3506\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4363 - Sensitivity: 0.0922 - tn: 11339.0000 - auc: 0.6784 - prc: 0.2961 - val_loss: 0.4601 - val_Sensitivity: 0.2818 - val_tn: 3606.0000 - val_auc: 0.7349 - val_prc: 0.3467\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4303 - Sensitivity: 0.0822 - tn: 11374.0000 - auc: 0.6862 - prc: 0.2917 - val_loss: 0.4575 - val_Sensitivity: 0.2764 - val_tn: 3617.0000 - val_auc: 0.7351 - val_prc: 0.3477\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4937 - Sensitivity: 0.1305 - tn: 10963.0000 - auc: 0.6429 - prc: 0.2599 - val_loss: 0.4500 - val_Sensitivity: 0.3257 - val_tn: 3577.0000 - val_auc: 0.7516 - val_prc: 0.3700\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4824 - Sensitivity: 0.1237 - tn: 10992.0000 - auc: 0.6506 - prc: 0.2698 - val_loss: 0.4650 - val_Sensitivity: 0.3419 - val_tn: 3548.0000 - val_auc: 0.7483 - val_prc: 0.3630\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4682 - Sensitivity: 0.1233 - tn: 11081.0000 - auc: 0.6605 - prc: 0.2782 - val_loss: 0.4685 - val_Sensitivity: 0.3405 - val_tn: 3549.0000 - val_auc: 0.7467 - val_prc: 0.3588\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4796 - Sensitivity: 0.1080 - tn: 11132.0000 - auc: 0.6387 - prc: 0.2629 - val_loss: 0.4632 - val_Sensitivity: 0.3135 - val_tn: 3577.0000 - val_auc: 0.7455 - val_prc: 0.3581\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4569 - Sensitivity: 0.1122 - tn: 11168.0000 - auc: 0.6600 - prc: 0.2824 - val_loss: 0.4518 - val_Sensitivity: 0.2676 - val_tn: 3637.0000 - val_auc: 0.7463 - val_prc: 0.3606\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4541 - Sensitivity: 0.0957 - tn: 11223.0000 - auc: 0.6667 - prc: 0.2845 - val_loss: 0.4661 - val_Sensitivity: 0.3216 - val_tn: 3560.0000 - val_auc: 0.7432 - val_prc: 0.3537\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4513 - Sensitivity: 0.1080 - tn: 11208.0000 - auc: 0.6686 - prc: 0.2891 - val_loss: 0.4558 - val_Sensitivity: 0.2743 - val_tn: 3625.0000 - val_auc: 0.7441 - val_prc: 0.3537\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4434 - Sensitivity: 0.0867 - tn: 11295.0000 - auc: 0.6742 - prc: 0.2949 - val_loss: 0.4624 - val_Sensitivity: 0.2932 - val_tn: 3599.0000 - val_auc: 0.7439 - val_prc: 0.3526\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4410 - Sensitivity: 0.0982 - tn: 11325.0000 - auc: 0.6760 - prc: 0.3019 - val_loss: 0.4625 - val_Sensitivity: 0.2919 - val_tn: 3598.0000 - val_auc: 0.7438 - val_prc: 0.3517\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4372 - Sensitivity: 0.0833 - tn: 11292.0000 - auc: 0.6817 - prc: 0.2993 - val_loss: 0.4588 - val_Sensitivity: 0.2824 - val_tn: 3617.0000 - val_auc: 0.7444 - val_prc: 0.3528\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4342 - Sensitivity: 0.0850 - tn: 11344.0000 - auc: 0.6825 - prc: 0.3093 - val_loss: 0.4592 - val_Sensitivity: 0.2797 - val_tn: 3614.0000 - val_auc: 0.7445 - val_prc: 0.3533\n"
     ]
    }
   ],
   "source": [
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7230773695356694,\n",
       " 0.7279776436808083,\n",
       " 0.7445381490794634,\n",
       " 0.7465062050508798,\n",
       " 0.7479201841048017]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))\n",
    "ann_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(dX_train[0], dy_train[0])\n",
    "def rf_feat_importance(model, X):\n",
    "    return pd.DataFrame({'cols':X.columns, 'imp':model.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "fi = rf_feat_importance(model, X)\n",
    "fi[:10]\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fit(dX_train[0], dy_train[0])\n",
    "fi = rf_feat_importance(model2, X)\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7033941714706838,\n",
       " 0.7045845681002759,\n",
       " 0.6989298211679473,\n",
       " 0.7052373866157211,\n",
       " 0.706601778639608]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrpreds = []\n",
    "model3 = LogisticRegression()\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))\n",
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))\n",
    "lr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_ci = stats.norm.interval(0.95, loc=np.mean(lr_score), scale=np.std(lr_score))\n",
    "print(lr_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def evaluate(model1, X, y):\n",
    "    ppreds = model1.predict_proba(X)\n",
    "    ppreds = ppreds[:,1]\n",
    "    pscore = roc_auc_score(y, ppreds)\n",
    "    print('AUC', pscore)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def rf_feat_importance(model, X):\n",
    "    return pd.DataFrame({'cols':X.columns, 'imp':model.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "fi = rf_feat_importance(model, X)\n",
    "fi[:10]\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('cols', 'imp', 'barh', figsize=(8,4), legend=False)\n",
    "\n",
    "plot_fi(fi[:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.738 (0.724-0.752)\n"
     ]
    }
   ],
   "source": [
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print('Neural Network:', round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.704 (0.7-0.707)\n"
     ]
    }
   ],
   "source": [
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.367 (0.353-0.381)\n",
      "Logistic Regression: 0.318 (0.299-0.337)\n"
     ]
    }
   ],
   "source": [
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')\n",
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')\n",
    "with open('pan_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4647\n",
      "4647\n",
      "4647\n",
      "4646\n",
      "4646\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(annpreds[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_ann_tpr' (list)\n",
      "Stored 'mean_pan_ann_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "pan_ann_tpr = []\n",
    "pan_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, thresholds = roc_curve(dy_test[x], annpreds[x])\n",
    "    pan_ann_tpr.append(tpr)\n",
    "    pan_ann_fpr.append(fpr)\n",
    "pan_ann_tpr_array = [np.array(x) for x in pan_ann_tpr]\n",
    "mean_pan_ann_tpr = [np.mean(k) for k in zip(*pan_ann_tpr_array)]\n",
    "pan_ann_fpr_array = [np.array(x) for x in pan_ann_fpr]\n",
    "mean_pan_ann_fpr = [np.mean(k) for k in zip(*pan_ann_fpr_array)]\n",
    "%store mean_pan_ann_tpr\n",
    "%store mean_pan_ann_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "1180\n",
      "1116\n",
      "1159\n",
      "1078\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(pan_ann_fpr[x]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,5):\n",
    "    pan_ann_tpr[x] = np.random.permutation(pan_ann_tpr[x])\n",
    "pan_ann_tpr = [x[:1000] for x in pan_ann_tpr]\n",
    "for x in range(0,5):\n",
    "    pan_ann_tpr[x] = sorted(pan_ann_tpr[x])\n",
    "for x in range(0,5):\n",
    "    pan_ann_fpr[x] = np.random.permutation(pan_ann_fpr[x])\n",
    "pan_ann_fpr = [x[:1000] for x in pan_ann_fpr]\n",
    "for x in range(0,5):\n",
    "    pan_ann_fpr[x] = sorted(pan_ann_fpr[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_ann_tpr' (list)\n",
      "Stored 'mean_pan_ann_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "pan_ann_tpr_array = [np.array(x) for x in pan_ann_tpr]\n",
    "mean_pan_ann_tpr = [np.mean(k) for k in zip(*pan_ann_tpr_array)]\n",
    "pan_ann_fpr_array = [np.array(x) for x in pan_ann_fpr]\n",
    "mean_pan_ann_fpr = [np.mean(k) for k in zip(*pan_ann_fpr_array)]\n",
    "%store mean_pan_ann_tpr\n",
    "%store mean_pan_ann_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_ann_tpr' (list)\n",
      "Stored 'mean_pan_ann_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "pan_ann_tpr = []\n",
    "pan_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    pan_ann_tpr.append(tpr)\n",
    "    pan_ann_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_tpr[x]))\n",
    "        pan_ann_tpr[x] = np.delete(pan_ann_tpr[x],ind)\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_fpr[x]))\n",
    "        pan_ann_fpr[x] = np.delete(pan_ann_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_pan_ann_tpr = [np.mean(k) for k in zip(*pan_ann_tpr)]\n",
    "\n",
    "mean_pan_ann_fpr = [np.mean(k) for k in zip(*pan_ann_fpr)]\n",
    "%store mean_pan_ann_tpr\n",
    "%store mean_pan_ann_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_lr_tpr' (list)\n",
      "Stored 'mean_pan_lr_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "pan_lr_tpr = []\n",
    "pan_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    pan_lr_tpr.append(tpr)\n",
    "    pan_lr_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_tpr[x]) - 1138\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_tpr[x]))\n",
    "        pan_lr_tpr[x] = np.delete(pan_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_fpr[x]) - 1138\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_fpr[x]))\n",
    "        pan_lr_fpr[x] = np.delete(pan_lr_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_pan_lr_tpr = [np.mean(k) for k in zip(*pan_lr_tpr)]\n",
    "\n",
    "mean_pan_lr_fpr = [np.mean(k) for k in zip(*pan_lr_fpr)]\n",
    "%store mean_pan_lr_tpr\n",
    "%store mean_pan_lr_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_lr_rec' (list)\n",
      "Stored 'mean_pan_lr_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "pan_lr_rec = []\n",
    "pan_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    pan_lr_rec.append(rec)\n",
    "    pan_lr_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_rec[x]) - 4523\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_rec[x]))\n",
    "        pan_lr_rec[x] = np.delete(pan_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_prec[x]) - 4523\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_prec[x]))\n",
    "        pan_lr_prec[x] = np.delete(pan_lr_prec[x],ind)\n",
    "\n",
    "mean_pan_lr_rec = [np.mean(k) for k in zip(*pan_lr_rec)]\n",
    "\n",
    "mean_pan_lr_prec = [np.mean(k) for k in zip(*pan_lr_prec)]\n",
    "%store mean_pan_lr_rec\n",
    "%store mean_pan_lr_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_ann_rec' (list)\n",
      "Stored 'mean_pan_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "pan_ann_rec = []\n",
    "pan_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    pan_ann_rec.append(rec)\n",
    "    pan_ann_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_rec[x]) - 4523\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_rec[x]))\n",
    "        pan_ann_rec[x] = np.delete(pan_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_prec[x]) - 4523\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_prec[x]))\n",
    "        pan_ann_prec[x] = np.delete(pan_ann_prec[x],ind)\n",
    "\n",
    "mean_pan_ann_rec = [np.mean(k) for k in zip(*pan_ann_rec)]\n",
    "\n",
    "mean_pan_ann_prec = [np.mean(k) for k in zip(*pan_ann_prec)]\n",
    "%store mean_pan_ann_rec\n",
    "%store mean_pan_ann_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pan_lr_rec = []\n",
    "pan_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    pan_lr_rec.append(rec)\n",
    "    pan_lr_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    pan_lr_rec[x] = np.random.permutation(pan_lr_rec[x])\n",
    "pan_lr_rec = [x[:1000] for x in pan_lr_rec]\n",
    "for x in range(0,5):\n",
    "    pan_lr_rec[x] = sorted(pan_lr_rec[x])\n",
    "for x in range(0,5):\n",
    "    pan_lr_prec[x] = np.random.permutation(pan_lr_prec[x])\n",
    "pan_lr_prec = [x[:1000] for x in pan_lr_prec]\n",
    "for x in range(0,5):\n",
    "    pan_lr_prec[x] = sorted(pan_lr_prec[x])\n",
    "pan_lr_rec_array = [np.array(x) for x in pan_lr_rec]\n",
    "mean_pan_lr_rec = [np.mean(k) for k in zip(*pan_lr_rec_array)]\n",
    "pan_lr_prec_array = [np.array(x) for x in pan_lr_prec]\n",
    "mean_pan_lr_prec = [np.mean(k) for k in zip(*pan_lr_prec_array)]\n",
    "%store mean_pan_lr_rec\n",
    "%store mean_pan_lr_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(mean_pan_ann_rec, mean_pan_ann_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
