{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, recall_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_pan.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRNCPTX</th>\n",
       "      <th>CPT</th>\n",
       "      <th>CONCPT1</th>\n",
       "      <th>OTHERCPT1</th>\n",
       "      <th>OPERYR</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>RACE_NEW</th>\n",
       "      <th>ETHNICITY_HISPANIC</th>\n",
       "      <th>TRANST</th>\n",
       "      <th>WNDCLAS</th>\n",
       "      <th>ASACLAS</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>DYSPNEA</th>\n",
       "      <th>FNSTATUS2</th>\n",
       "      <th>VENTILAT</th>\n",
       "      <th>HXCOPD</th>\n",
       "      <th>ASCITES</th>\n",
       "      <th>HXCHF</th>\n",
       "      <th>HYPERMED</th>\n",
       "      <th>RENAFAIL</th>\n",
       "      <th>DIALYSIS</th>\n",
       "      <th>STEROID</th>\n",
       "      <th>WTLOSS</th>\n",
       "      <th>BLEEDDIS</th>\n",
       "      <th>TRANSFUS</th>\n",
       "      <th>PRSEPIS</th>\n",
       "      <th>PRSODM</th>\n",
       "      <th>PRBUN</th>\n",
       "      <th>PRCREAT</th>\n",
       "      <th>PRALBUM</th>\n",
       "      <th>PRBILI</th>\n",
       "      <th>PRALKPH</th>\n",
       "      <th>PRWBC</th>\n",
       "      <th>PRHCT</th>\n",
       "      <th>PRPLATE</th>\n",
       "      <th>PRPTT</th>\n",
       "      <th>PRINR</th>\n",
       "      <th>SEPSHOCKPATOS</th>\n",
       "      <th>SSSIPATOS</th>\n",
       "      <th>DSSIPATOS</th>\n",
       "      <th>OSSIPATOS</th>\n",
       "      <th>PNAPATOS</th>\n",
       "      <th>VENTPATOS</th>\n",
       "      <th>UTIPATOS</th>\n",
       "      <th>SEPSISPATOS</th>\n",
       "      <th>OPTIME</th>\n",
       "      <th>PAN_INDICATION</th>\n",
       "      <th>PAN_JAUNDICE</th>\n",
       "      <th>PAN_BILIARYSTENT</th>\n",
       "      <th>PAN_CHEMO</th>\n",
       "      <th>PAN_RADIO</th>\n",
       "      <th>PAN_INTRA_ANTIBIOTICS</th>\n",
       "      <th>PAN_APPROACH</th>\n",
       "      <th>PAN_OINCIS_TYPE</th>\n",
       "      <th>PAN_WOUNDPROT</th>\n",
       "      <th>PAN_DUCTSIZE</th>\n",
       "      <th>PAN_GLANDTEXT</th>\n",
       "      <th>PAN_RECONSTRUCTION</th>\n",
       "      <th>PAN_GASTDUO</th>\n",
       "      <th>PAN_DRAINS</th>\n",
       "      <th>PAN_DRAINS_TYPE</th>\n",
       "      <th>PAN_DRAINSYS_TYPE</th>\n",
       "      <th>PAN_DRAINSYS_SUCTN</th>\n",
       "      <th>PAN_RESECTION</th>\n",
       "      <th>PAN_AMYLASE_POD1</th>\n",
       "      <th>PAN_FISTULA</th>\n",
       "      <th>PAN_NPWT</th>\n",
       "      <th>PAN_TSTAGE</th>\n",
       "      <th>PAN_NSTAGE</th>\n",
       "      <th>PAN_MSTAGE</th>\n",
       "      <th>PAN_BENIGN_TUMORSIZE</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASEID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6522035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>467</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.240741</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-1.071429</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.808</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>-0.303030</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.396970</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.497005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523494</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.206897</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>-1.8750</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.427988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6537888</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>277</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>-0.357143</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.827586</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.348485</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.057034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540993</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>467</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.151515</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>-3.4375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.069697</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>100.085714</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.382758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545348</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>195</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.351852</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-1.275862</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>-0.849057</td>\n",
       "      <td>-2.8125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821212</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18.742857</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.046548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PRNCPTX  CPT  CONCPT1  OTHERCPT1  OPERYR  SEX       AGE    HEIGHT  \\\n",
       "CASEID                                                                       \n",
       "6522035        0    0      190        467       3    1 -0.666667  1.333333   \n",
       "6523494        0    0      190        412       3    1  0.666667  0.666667   \n",
       "6537888        0    0      190        277       3    0 -0.800000 -0.333333   \n",
       "6540993        1    1      190        467       3    1 -0.600000  0.500000   \n",
       "6545348        0    0      190        195       3    0  0.266667 -0.500000   \n",
       "\n",
       "           WEIGHT  RACE_NEW  ETHNICITY_HISPANIC  TRANST  WNDCLAS  ASACLAS  \\\n",
       "CASEID                                                                      \n",
       "6522035  1.240741         5                   0       1        1        2   \n",
       "6523494 -0.055556         5                   0       1        1        2   \n",
       "6537888  0.574074         4                   2       1        2        2   \n",
       "6540993  0.574074         5                   2       1        1        2   \n",
       "6545348 -0.351852         5                   0       1        1        2   \n",
       "\n",
       "         DIABETES  SMOKE  DYSPNEA  FNSTATUS2  VENTILAT  HXCOPD  ASCITES  \\\n",
       "CASEID                                                                    \n",
       "6522035         0      0        2          0         0       0        0   \n",
       "6523494         1      0        2          0         0       0        0   \n",
       "6537888         1      0        2          0         0       0        0   \n",
       "6540993         1      1        2          0         0       0        0   \n",
       "6545348         1      0        2          0         0       0        0   \n",
       "\n",
       "         HXCHF  HYPERMED  RENAFAIL  DIALYSIS  STEROID  WTLOSS  BLEEDDIS  \\\n",
       "CASEID                                                                    \n",
       "6522035      0         1         0         0        0       1         0   \n",
       "6523494      0         0         0         0        0       0         0   \n",
       "6537888      0         0         0         0        0       0         0   \n",
       "6540993      0         0         0         0        0       0         0   \n",
       "6545348      0         1         0         0        0       0         0   \n",
       "\n",
       "         TRANSFUS  PRSEPIS  PRSODM     PRBUN   PRCREAT   PRALBUM    PRBILI  \\\n",
       "CASEID                                                                       \n",
       "6522035         0        0   -1.75 -0.571429 -1.071429 -0.857143  0.000000   \n",
       "6523494         0        0   -3.00 -0.857143  0.000000  0.285714  1.333333   \n",
       "6537888         0        0    0.25 -0.142857 -0.357143 -0.142857 -0.222222   \n",
       "6540993         0        0    0.00  0.142857  0.000000  0.428571  0.222222   \n",
       "6545348         0        0    0.75 -0.285714 -0.428571 -0.714286 -0.222222   \n",
       "\n",
       "         PRALKPH     PRWBC     PRHCT   PRPLATE   PRPTT  PRINR  SEPSHOCKPATOS  \\\n",
       "CASEID                                                                         \n",
       "6522035    5.808  0.586207 -0.303030  0.547170 -0.3125    3.0              0   \n",
       "6523494    0.504 -0.206897  0.590909  0.905660 -1.8750    0.3              0   \n",
       "6537888   -0.104 -0.827586  0.196970  0.405660  0.0000    0.0              0   \n",
       "6540993    0.096  0.000000 -0.151515  0.150943 -3.4375    0.0              0   \n",
       "6545348   -0.272 -1.275862 -0.030303 -0.849057 -2.8125    0.2              0   \n",
       "\n",
       "         SSSIPATOS  DSSIPATOS  OSSIPATOS  PNAPATOS  VENTPATOS  UTIPATOS  \\\n",
       "CASEID                                                                    \n",
       "6522035          0          0          0         0          0         0   \n",
       "6523494          0          0          0         0          0         0   \n",
       "6537888          0          0          0         0          0         0   \n",
       "6540993          0          0          0         0          0         0   \n",
       "6545348          0          0          0         0          0         0   \n",
       "\n",
       "         SEPSISPATOS    OPTIME  PAN_INDICATION  PAN_JAUNDICE  \\\n",
       "CASEID                                                         \n",
       "6522035            0  1.396970              12             2   \n",
       "6523494            0  0.863636               4             2   \n",
       "6537888            0  1.348485              12             0   \n",
       "6540993            0  1.069697               1             2   \n",
       "6545348            0  0.821212              12             0   \n",
       "\n",
       "         PAN_BILIARYSTENT  PAN_CHEMO  PAN_RADIO  PAN_INTRA_ANTIBIOTICS  \\\n",
       "CASEID                                                                   \n",
       "6522035                 1          0          0                      1   \n",
       "6523494                 0          2          0                      0   \n",
       "6537888                 0          2          2                      0   \n",
       "6540993                 1          0          0                      1   \n",
       "6545348                 4          2          2                      2   \n",
       "\n",
       "         PAN_APPROACH  PAN_OINCIS_TYPE  PAN_WOUNDPROT  PAN_DUCTSIZE  \\\n",
       "CASEID                                                                \n",
       "6522035             1                1              2             1   \n",
       "6523494             1                4              2             0   \n",
       "6537888             1                4              2             0   \n",
       "6540993             1                1              2             1   \n",
       "6545348             1                4              2             0   \n",
       "\n",
       "         PAN_GLANDTEXT  PAN_RECONSTRUCTION  PAN_GASTDUO  PAN_DRAINS  \\\n",
       "CASEID                                                                \n",
       "6522035              1                   2            3           2   \n",
       "6523494              2                   2            3           0   \n",
       "6537888              0                   2            3           0   \n",
       "6540993              2                   2            3           2   \n",
       "6545348              1                   2            3           2   \n",
       "\n",
       "         PAN_DRAINS_TYPE  PAN_DRAINSYS_TYPE  PAN_DRAINSYS_SUCTN  \\\n",
       "CASEID                                                            \n",
       "6522035                5                  0                   2   \n",
       "6523494                5                  3                   1   \n",
       "6537888                5                  3                   1   \n",
       "6540993                5                  0                   2   \n",
       "6545348                5                  0                   2   \n",
       "\n",
       "         PAN_RESECTION  PAN_AMYLASE_POD1  PAN_FISTULA  PAN_NPWT  PAN_TSTAGE  \\\n",
       "CASEID                                                                        \n",
       "6522035              1         24.400000        False         2           4   \n",
       "6523494              1          0.000000        False         2           3   \n",
       "6537888              1          0.000000        False         2           3   \n",
       "6540993              1        100.085714         True         2           2   \n",
       "6545348              3         18.742857        False         2           3   \n",
       "\n",
       "         PAN_NSTAGE  PAN_MSTAGE  PAN_BENIGN_TUMORSIZE       BMI  \n",
       "CASEID                                                           \n",
       "6522035           0           0                     3  0.497005  \n",
       "6523494           1           0                     3 -0.427988  \n",
       "6537888           0           0                     3  1.057034  \n",
       "6540993           0           0                     3  0.382758  \n",
       "6545348           0           2                     3 -0.046548  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.drop('CASEID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23233, 76)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['PAN_FISTULA']\n",
    "X = data.drop(['PAN_FISTULA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for x in range(0,5):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['PAN_FISTULA'], axis=1))\n",
    "        dy_train.append(d[x]['PAN_FISTULA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['PAN_FISTULA'], axis=1))\n",
    "        dy_test.append(d[x]['PAN_FISTULA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "input_shape = [X.shape[1]]\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "for _ in range(1):\n",
    "    model4.add(keras.layers.Dense(200))\n",
    "    model4.add(keras.layers.BatchNormalization())\n",
    "    model4.add(keras.layers.Dropout(0.8))\n",
    "    model4.add(keras.layers.Activation(\"relu\"))\n",
    "model4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "metrics = [keras.metrics.Recall(name='Sensitivity'), keras.metrics.TrueNegatives(name='tn'), keras.metrics.AUC(name='auc'), keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=25,\n",
    "    min_delta=1e-6,\n",
    "    restore_best_weights=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save untrained model\n",
    "model4.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 31ms/step - loss: 1.0218 - Sensitivity: 0.4738 - tn: 5944.0000 - auc: 0.4955 - prc: 0.1663 - val_loss: 0.4472 - val_Sensitivity: 0.0041 - val_tn: 3907.0000 - val_auc: 0.5531 - val_prc: 0.1867\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8241 - Sensitivity: 0.3819 - tn: 7551.0000 - auc: 0.5249 - prc: 0.1811 - val_loss: 0.4291 - val_Sensitivity: 0.0027 - val_tn: 3913.0000 - val_auc: 0.6124 - val_prc: 0.2269\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.7141 - Sensitivity: 0.2912 - tn: 8788.0000 - auc: 0.5411 - prc: 0.1885 - val_loss: 0.4197 - val_Sensitivity: 0.0041 - val_tn: 3914.0000 - val_auc: 0.6450 - val_prc: 0.2536\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6618 - Sensitivity: 0.2290 - tn: 9472.0000 - auc: 0.5498 - prc: 0.1923 - val_loss: 0.4138 - val_Sensitivity: 0.0014 - val_tn: 3914.0000 - val_auc: 0.6668 - val_prc: 0.2720\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.6328 - Sensitivity: 0.2131 - tn: 9812.0000 - auc: 0.5592 - prc: 0.1974 - val_loss: 0.4092 - val_Sensitivity: 0.0027 - val_tn: 3914.0000 - val_auc: 0.6833 - val_prc: 0.2885\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5968 - Sensitivity: 0.1972 - tn: 10085.0000 - auc: 0.5806 - prc: 0.2119 - val_loss: 0.4061 - val_Sensitivity: 0.0027 - val_tn: 3913.0000 - val_auc: 0.6948 - val_prc: 0.3001\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5934 - Sensitivity: 0.1710 - tn: 10218.0000 - auc: 0.5768 - prc: 0.2098 - val_loss: 0.4032 - val_Sensitivity: 0.0027 - val_tn: 3912.0000 - val_auc: 0.7038 - val_prc: 0.3129\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5884 - Sensitivity: 0.1808 - tn: 10347.0000 - auc: 0.5826 - prc: 0.2137 - val_loss: 0.4008 - val_Sensitivity: 0.0055 - val_tn: 3912.0000 - val_auc: 0.7107 - val_prc: 0.3241\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5654 - Sensitivity: 0.1645 - tn: 10424.0000 - auc: 0.5958 - prc: 0.2177 - val_loss: 0.3988 - val_Sensitivity: 0.0095 - val_tn: 3911.0000 - val_auc: 0.7162 - val_prc: 0.3330\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5570 - Sensitivity: 0.1770 - tn: 10480.0000 - auc: 0.6075 - prc: 0.2316 - val_loss: 0.3975 - val_Sensitivity: 0.0164 - val_tn: 3910.0000 - val_auc: 0.7202 - val_prc: 0.3403\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5409 - Sensitivity: 0.1649 - tn: 10619.0000 - auc: 0.6175 - prc: 0.2387 - val_loss: 0.3962 - val_Sensitivity: 0.0218 - val_tn: 3910.0000 - val_auc: 0.7228 - val_prc: 0.3447\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5366 - Sensitivity: 0.1731 - tn: 10604.0000 - auc: 0.6198 - prc: 0.2428 - val_loss: 0.3956 - val_Sensitivity: 0.0246 - val_tn: 3910.0000 - val_auc: 0.7249 - val_prc: 0.3480\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5238 - Sensitivity: 0.1662 - tn: 10637.0000 - auc: 0.6282 - prc: 0.2410 - val_loss: 0.3944 - val_Sensitivity: 0.0327 - val_tn: 3909.0000 - val_auc: 0.7267 - val_prc: 0.3500\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5175 - Sensitivity: 0.1624 - tn: 10757.0000 - auc: 0.6269 - prc: 0.2534 - val_loss: 0.3944 - val_Sensitivity: 0.0300 - val_tn: 3908.0000 - val_auc: 0.7275 - val_prc: 0.3512\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5140 - Sensitivity: 0.1542 - tn: 10739.0000 - auc: 0.6317 - prc: 0.2558 - val_loss: 0.3936 - val_Sensitivity: 0.0355 - val_tn: 3906.0000 - val_auc: 0.7285 - val_prc: 0.3526\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5073 - Sensitivity: 0.1602 - tn: 10825.0000 - auc: 0.6323 - prc: 0.2550 - val_loss: 0.3932 - val_Sensitivity: 0.0355 - val_tn: 3906.0000 - val_auc: 0.7284 - val_prc: 0.3521\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.5061 - Sensitivity: 0.1521 - tn: 10835.0000 - auc: 0.6279 - prc: 0.2489 - val_loss: 0.3923 - val_Sensitivity: 0.0409 - val_tn: 3902.0000 - val_auc: 0.7289 - val_prc: 0.3533\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5001 - Sensitivity: 0.1439 - tn: 10869.0000 - auc: 0.6355 - prc: 0.2555 - val_loss: 0.3925 - val_Sensitivity: 0.0423 - val_tn: 3902.0000 - val_auc: 0.7291 - val_prc: 0.3518\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4920 - Sensitivity: 0.1525 - tn: 10866.0000 - auc: 0.6390 - prc: 0.2618 - val_loss: 0.3927 - val_Sensitivity: 0.0368 - val_tn: 3902.0000 - val_auc: 0.7285 - val_prc: 0.3507\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4813 - Sensitivity: 0.1568 - tn: 10914.0000 - auc: 0.6535 - prc: 0.2728 - val_loss: 0.3925 - val_Sensitivity: 0.0423 - val_tn: 3899.0000 - val_auc: 0.7282 - val_prc: 0.3501\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4838 - Sensitivity: 0.1426 - tn: 10976.0000 - auc: 0.6428 - prc: 0.2665 - val_loss: 0.3929 - val_Sensitivity: 0.0396 - val_tn: 3900.0000 - val_auc: 0.7278 - val_prc: 0.3489\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4817 - Sensitivity: 0.1370 - tn: 10951.0000 - auc: 0.6472 - prc: 0.2612 - val_loss: 0.3928 - val_Sensitivity: 0.0409 - val_tn: 3900.0000 - val_auc: 0.7273 - val_prc: 0.3474\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4791 - Sensitivity: 0.1357 - tn: 10982.0000 - auc: 0.6444 - prc: 0.2630 - val_loss: 0.3928 - val_Sensitivity: 0.0396 - val_tn: 3899.0000 - val_auc: 0.7275 - val_prc: 0.3476\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4693 - Sensitivity: 0.1276 - tn: 11061.0000 - auc: 0.6559 - prc: 0.2764 - val_loss: 0.3926 - val_Sensitivity: 0.0396 - val_tn: 3900.0000 - val_auc: 0.7276 - val_prc: 0.3469\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4634 - Sensitivity: 0.1091 - tn: 11105.0000 - auc: 0.6594 - prc: 0.2715 - val_loss: 0.3923 - val_Sensitivity: 0.0368 - val_tn: 3900.0000 - val_auc: 0.7277 - val_prc: 0.3473\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4635 - Sensitivity: 0.1194 - tn: 11103.0000 - auc: 0.6583 - prc: 0.2728 - val_loss: 0.3919 - val_Sensitivity: 0.0409 - val_tn: 3900.0000 - val_auc: 0.7282 - val_prc: 0.3484\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4566 - Sensitivity: 0.1340 - tn: 11103.0000 - auc: 0.6647 - prc: 0.2812 - val_loss: 0.3919 - val_Sensitivity: 0.0382 - val_tn: 3899.0000 - val_auc: 0.7283 - val_prc: 0.3487\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4567 - Sensitivity: 0.1113 - tn: 11133.0000 - auc: 0.6657 - prc: 0.2730 - val_loss: 0.3917 - val_Sensitivity: 0.0382 - val_tn: 3900.0000 - val_auc: 0.7287 - val_prc: 0.3488\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4593 - Sensitivity: 0.1035 - tn: 11189.0000 - auc: 0.6551 - prc: 0.2733 - val_loss: 0.3917 - val_Sensitivity: 0.0382 - val_tn: 3901.0000 - val_auc: 0.7283 - val_prc: 0.3485\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4459 - Sensitivity: 0.1147 - tn: 11219.0000 - auc: 0.6717 - prc: 0.2949 - val_loss: 0.3914 - val_Sensitivity: 0.0382 - val_tn: 3901.0000 - val_auc: 0.7287 - val_prc: 0.3496\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4382 - Sensitivity: 0.1181 - tn: 11253.0000 - auc: 0.6801 - prc: 0.3008 - val_loss: 0.3910 - val_Sensitivity: 0.0396 - val_tn: 3901.0000 - val_auc: 0.7291 - val_prc: 0.3509\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4465 - Sensitivity: 0.1027 - tn: 11230.0000 - auc: 0.6706 - prc: 0.2775 - val_loss: 0.3909 - val_Sensitivity: 0.0409 - val_tn: 3902.0000 - val_auc: 0.7291 - val_prc: 0.3508\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4436 - Sensitivity: 0.0889 - tn: 11286.0000 - auc: 0.6698 - prc: 0.2839 - val_loss: 0.3908 - val_Sensitivity: 0.0382 - val_tn: 3903.0000 - val_auc: 0.7294 - val_prc: 0.3518\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4417 - Sensitivity: 0.0868 - tn: 11300.0000 - auc: 0.6741 - prc: 0.2892 - val_loss: 0.3906 - val_Sensitivity: 0.0382 - val_tn: 3903.0000 - val_auc: 0.7296 - val_prc: 0.3517\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4397 - Sensitivity: 0.0872 - tn: 11294.0000 - auc: 0.6752 - prc: 0.2882 - val_loss: 0.3906 - val_Sensitivity: 0.0314 - val_tn: 3903.0000 - val_auc: 0.7295 - val_prc: 0.3528\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4378 - Sensitivity: 0.0825 - tn: 11337.0000 - auc: 0.6756 - prc: 0.2952 - val_loss: 0.3906 - val_Sensitivity: 0.0314 - val_tn: 3904.0000 - val_auc: 0.7296 - val_prc: 0.3522\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4401 - Sensitivity: 0.0838 - tn: 11362.0000 - auc: 0.6712 - prc: 0.2882 - val_loss: 0.3902 - val_Sensitivity: 0.0341 - val_tn: 3903.0000 - val_auc: 0.7303 - val_prc: 0.3532\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4324 - Sensitivity: 0.0868 - tn: 11355.0000 - auc: 0.6835 - prc: 0.3049 - val_loss: 0.3900 - val_Sensitivity: 0.0355 - val_tn: 3903.0000 - val_auc: 0.7310 - val_prc: 0.3541\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4325 - Sensitivity: 0.0799 - tn: 11363.0000 - auc: 0.6795 - prc: 0.2985 - val_loss: 0.3900 - val_Sensitivity: 0.0327 - val_tn: 3904.0000 - val_auc: 0.7304 - val_prc: 0.3542\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4297 - Sensitivity: 0.0812 - tn: 11393.0000 - auc: 0.6830 - prc: 0.3065 - val_loss: 0.3900 - val_Sensitivity: 0.0327 - val_tn: 3905.0000 - val_auc: 0.7305 - val_prc: 0.3548\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4280 - Sensitivity: 0.0730 - tn: 11394.0000 - auc: 0.6891 - prc: 0.3072 - val_loss: 0.3897 - val_Sensitivity: 0.0341 - val_tn: 3905.0000 - val_auc: 0.7311 - val_prc: 0.3563\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4279 - Sensitivity: 0.0747 - tn: 11415.0000 - auc: 0.6884 - prc: 0.3088 - val_loss: 0.3895 - val_Sensitivity: 0.0327 - val_tn: 3906.0000 - val_auc: 0.7314 - val_prc: 0.3569\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4255 - Sensitivity: 0.0743 - tn: 11425.0000 - auc: 0.6912 - prc: 0.3163 - val_loss: 0.3893 - val_Sensitivity: 0.0327 - val_tn: 3906.0000 - val_auc: 0.7319 - val_prc: 0.3579\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4247 - Sensitivity: 0.0687 - tn: 11444.0000 - auc: 0.6919 - prc: 0.3149 - val_loss: 0.3892 - val_Sensitivity: 0.0327 - val_tn: 3905.0000 - val_auc: 0.7324 - val_prc: 0.3585\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4270 - Sensitivity: 0.0546 - tn: 11454.0000 - auc: 0.6884 - prc: 0.3040 - val_loss: 0.3891 - val_Sensitivity: 0.0300 - val_tn: 3908.0000 - val_auc: 0.7327 - val_prc: 0.3588\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4249 - Sensitivity: 0.0597 - tn: 11434.0000 - auc: 0.6927 - prc: 0.3078 - val_loss: 0.3889 - val_Sensitivity: 0.0327 - val_tn: 3905.0000 - val_auc: 0.7329 - val_prc: 0.3595\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4231 - Sensitivity: 0.0657 - tn: 11483.0000 - auc: 0.6927 - prc: 0.3174 - val_loss: 0.3889 - val_Sensitivity: 0.0300 - val_tn: 3907.0000 - val_auc: 0.7329 - val_prc: 0.3601\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4157 - Sensitivity: 0.0546 - tn: 11492.0000 - auc: 0.7055 - prc: 0.3263 - val_loss: 0.3888 - val_Sensitivity: 0.0300 - val_tn: 3908.0000 - val_auc: 0.7331 - val_prc: 0.3603\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4191 - Sensitivity: 0.0619 - tn: 11490.0000 - auc: 0.7028 - prc: 0.3200 - val_loss: 0.3887 - val_Sensitivity: 0.0300 - val_tn: 3908.0000 - val_auc: 0.7331 - val_prc: 0.3606\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4181 - Sensitivity: 0.0511 - tn: 11473.0000 - auc: 0.7069 - prc: 0.3245 - val_loss: 0.3886 - val_Sensitivity: 0.0300 - val_tn: 3908.0000 - val_auc: 0.7333 - val_prc: 0.3603\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4165 - Sensitivity: 0.0614 - tn: 11488.0000 - auc: 0.7062 - prc: 0.3310 - val_loss: 0.3885 - val_Sensitivity: 0.0300 - val_tn: 3907.0000 - val_auc: 0.7338 - val_prc: 0.3613\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4206 - Sensitivity: 0.0601 - tn: 11480.0000 - auc: 0.6975 - prc: 0.3122 - val_loss: 0.3884 - val_Sensitivity: 0.0314 - val_tn: 3907.0000 - val_auc: 0.7340 - val_prc: 0.3615\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4168 - Sensitivity: 0.0528 - tn: 11495.0000 - auc: 0.7050 - prc: 0.3294 - val_loss: 0.3884 - val_Sensitivity: 0.0300 - val_tn: 3908.0000 - val_auc: 0.7338 - val_prc: 0.3609\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4156 - Sensitivity: 0.0490 - tn: 11500.0000 - auc: 0.7090 - prc: 0.3256 - val_loss: 0.3883 - val_Sensitivity: 0.0314 - val_tn: 3907.0000 - val_auc: 0.7341 - val_prc: 0.3611\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4177 - Sensitivity: 0.0515 - tn: 11509.0000 - auc: 0.7029 - prc: 0.3277 - val_loss: 0.3883 - val_Sensitivity: 0.0300 - val_tn: 3910.0000 - val_auc: 0.7342 - val_prc: 0.3610\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4143 - Sensitivity: 0.0541 - tn: 11513.0000 - auc: 0.7086 - prc: 0.3330 - val_loss: 0.3883 - val_Sensitivity: 0.0300 - val_tn: 3910.0000 - val_auc: 0.7340 - val_prc: 0.3607\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4143 - Sensitivity: 0.0498 - tn: 11516.0000 - auc: 0.7078 - prc: 0.3335 - val_loss: 0.3881 - val_Sensitivity: 0.0286 - val_tn: 3910.0000 - val_auc: 0.7345 - val_prc: 0.3619\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4158 - Sensitivity: 0.0507 - tn: 11527.0000 - auc: 0.7065 - prc: 0.3359 - val_loss: 0.3880 - val_Sensitivity: 0.0286 - val_tn: 3910.0000 - val_auc: 0.7352 - val_prc: 0.3623\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4152 - Sensitivity: 0.0537 - tn: 11507.0000 - auc: 0.7067 - prc: 0.3390 - val_loss: 0.3878 - val_Sensitivity: 0.0314 - val_tn: 3909.0000 - val_auc: 0.7349 - val_prc: 0.3632\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4132 - Sensitivity: 0.0511 - tn: 11527.0000 - auc: 0.7126 - prc: 0.3399 - val_loss: 0.3876 - val_Sensitivity: 0.0314 - val_tn: 3908.0000 - val_auc: 0.7352 - val_prc: 0.3635\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4116 - Sensitivity: 0.0494 - tn: 11525.0000 - auc: 0.7141 - prc: 0.3394 - val_loss: 0.3875 - val_Sensitivity: 0.0314 - val_tn: 3908.0000 - val_auc: 0.7354 - val_prc: 0.3641\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4137 - Sensitivity: 0.0481 - tn: 11536.0000 - auc: 0.7104 - prc: 0.3394 - val_loss: 0.3874 - val_Sensitivity: 0.0327 - val_tn: 3908.0000 - val_auc: 0.7360 - val_prc: 0.3642\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4105 - Sensitivity: 0.0498 - tn: 11512.0000 - auc: 0.7164 - prc: 0.3459 - val_loss: 0.3872 - val_Sensitivity: 0.0327 - val_tn: 3908.0000 - val_auc: 0.7360 - val_prc: 0.3642\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4126 - Sensitivity: 0.0567 - tn: 11519.0000 - auc: 0.7120 - prc: 0.3407 - val_loss: 0.3872 - val_Sensitivity: 0.0327 - val_tn: 3908.0000 - val_auc: 0.7359 - val_prc: 0.3644\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4149 - Sensitivity: 0.0477 - tn: 11524.0000 - auc: 0.7082 - prc: 0.3357 - val_loss: 0.3872 - val_Sensitivity: 0.0327 - val_tn: 3908.0000 - val_auc: 0.7358 - val_prc: 0.3650\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4114 - Sensitivity: 0.0434 - tn: 11526.0000 - auc: 0.7165 - prc: 0.3325 - val_loss: 0.3872 - val_Sensitivity: 0.0341 - val_tn: 3908.0000 - val_auc: 0.7360 - val_prc: 0.3651\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4119 - Sensitivity: 0.0498 - tn: 11535.0000 - auc: 0.7139 - prc: 0.3390 - val_loss: 0.3870 - val_Sensitivity: 0.0327 - val_tn: 3908.0000 - val_auc: 0.7365 - val_prc: 0.3661\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4114 - Sensitivity: 0.0507 - tn: 11513.0000 - auc: 0.7150 - prc: 0.3388 - val_loss: 0.3868 - val_Sensitivity: 0.0327 - val_tn: 3907.0000 - val_auc: 0.7367 - val_prc: 0.3664\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4088 - Sensitivity: 0.0524 - tn: 11528.0000 - auc: 0.7194 - prc: 0.3501 - val_loss: 0.3868 - val_Sensitivity: 0.0327 - val_tn: 3908.0000 - val_auc: 0.7369 - val_prc: 0.3668\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4075 - Sensitivity: 0.0468 - tn: 11523.0000 - auc: 0.7228 - prc: 0.3451 - val_loss: 0.3866 - val_Sensitivity: 0.0327 - val_tn: 3905.0000 - val_auc: 0.7375 - val_prc: 0.3669\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4100 - Sensitivity: 0.0485 - tn: 11526.0000 - auc: 0.7191 - prc: 0.3459 - val_loss: 0.3865 - val_Sensitivity: 0.0341 - val_tn: 3905.0000 - val_auc: 0.7375 - val_prc: 0.3669\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4093 - Sensitivity: 0.0524 - tn: 11529.0000 - auc: 0.7186 - prc: 0.3461 - val_loss: 0.3864 - val_Sensitivity: 0.0355 - val_tn: 3905.0000 - val_auc: 0.7377 - val_prc: 0.3671\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4115 - Sensitivity: 0.0498 - tn: 11516.0000 - auc: 0.7151 - prc: 0.3405 - val_loss: 0.3863 - val_Sensitivity: 0.0382 - val_tn: 3903.0000 - val_auc: 0.7377 - val_prc: 0.3678\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4099 - Sensitivity: 0.0498 - tn: 11532.0000 - auc: 0.7177 - prc: 0.3453 - val_loss: 0.3863 - val_Sensitivity: 0.0368 - val_tn: 3903.0000 - val_auc: 0.7378 - val_prc: 0.3678\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4078 - Sensitivity: 0.0511 - tn: 11530.0000 - auc: 0.7204 - prc: 0.3519 - val_loss: 0.3862 - val_Sensitivity: 0.0368 - val_tn: 3904.0000 - val_auc: 0.7380 - val_prc: 0.3678\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4093 - Sensitivity: 0.0460 - tn: 11537.0000 - auc: 0.7187 - prc: 0.3403 - val_loss: 0.3861 - val_Sensitivity: 0.0355 - val_tn: 3905.0000 - val_auc: 0.7382 - val_prc: 0.3684\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4076 - Sensitivity: 0.0563 - tn: 11516.0000 - auc: 0.7206 - prc: 0.3502 - val_loss: 0.3860 - val_Sensitivity: 0.0368 - val_tn: 3904.0000 - val_auc: 0.7385 - val_prc: 0.3679\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4084 - Sensitivity: 0.0490 - tn: 11525.0000 - auc: 0.7195 - prc: 0.3509 - val_loss: 0.3860 - val_Sensitivity: 0.0327 - val_tn: 3904.0000 - val_auc: 0.7384 - val_prc: 0.3676\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4082 - Sensitivity: 0.0464 - tn: 11529.0000 - auc: 0.7216 - prc: 0.3466 - val_loss: 0.3858 - val_Sensitivity: 0.0327 - val_tn: 3902.0000 - val_auc: 0.7387 - val_prc: 0.3679\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4082 - Sensitivity: 0.0464 - tn: 11525.0000 - auc: 0.7213 - prc: 0.3447 - val_loss: 0.3858 - val_Sensitivity: 0.0327 - val_tn: 3903.0000 - val_auc: 0.7388 - val_prc: 0.3684\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4080 - Sensitivity: 0.0442 - tn: 11538.0000 - auc: 0.7211 - prc: 0.3502 - val_loss: 0.3858 - val_Sensitivity: 0.0327 - val_tn: 3904.0000 - val_auc: 0.7390 - val_prc: 0.3689\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4048 - Sensitivity: 0.0455 - tn: 11544.0000 - auc: 0.7306 - prc: 0.3570 - val_loss: 0.3857 - val_Sensitivity: 0.0341 - val_tn: 3903.0000 - val_auc: 0.7389 - val_prc: 0.3685\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4050 - Sensitivity: 0.0520 - tn: 11538.0000 - auc: 0.7271 - prc: 0.3641 - val_loss: 0.3855 - val_Sensitivity: 0.0341 - val_tn: 3901.0000 - val_auc: 0.7391 - val_prc: 0.3686\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4063 - Sensitivity: 0.0563 - tn: 11518.0000 - auc: 0.7266 - prc: 0.3519 - val_loss: 0.3853 - val_Sensitivity: 0.0368 - val_tn: 3901.0000 - val_auc: 0.7397 - val_prc: 0.3694\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4085 - Sensitivity: 0.0511 - tn: 11539.0000 - auc: 0.7213 - prc: 0.3552 - val_loss: 0.3855 - val_Sensitivity: 0.0341 - val_tn: 3902.0000 - val_auc: 0.7396 - val_prc: 0.3697\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4065 - Sensitivity: 0.0524 - tn: 11536.0000 - auc: 0.7245 - prc: 0.3573 - val_loss: 0.3854 - val_Sensitivity: 0.0355 - val_tn: 3901.0000 - val_auc: 0.7397 - val_prc: 0.3690\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4049 - Sensitivity: 0.0515 - tn: 11546.0000 - auc: 0.7269 - prc: 0.3572 - val_loss: 0.3853 - val_Sensitivity: 0.0341 - val_tn: 3903.0000 - val_auc: 0.7394 - val_prc: 0.3689\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4051 - Sensitivity: 0.0494 - tn: 11524.0000 - auc: 0.7266 - prc: 0.3601 - val_loss: 0.3852 - val_Sensitivity: 0.0341 - val_tn: 3901.0000 - val_auc: 0.7401 - val_prc: 0.3695\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4058 - Sensitivity: 0.0464 - tn: 11537.0000 - auc: 0.7257 - prc: 0.3585 - val_loss: 0.3853 - val_Sensitivity: 0.0341 - val_tn: 3901.0000 - val_auc: 0.7400 - val_prc: 0.3695\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4069 - Sensitivity: 0.0515 - tn: 11547.0000 - auc: 0.7234 - prc: 0.3606 - val_loss: 0.3852 - val_Sensitivity: 0.0355 - val_tn: 3901.0000 - val_auc: 0.7401 - val_prc: 0.3693\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4044 - Sensitivity: 0.0515 - tn: 11530.0000 - auc: 0.7279 - prc: 0.3600 - val_loss: 0.3851 - val_Sensitivity: 0.0382 - val_tn: 3899.0000 - val_auc: 0.7401 - val_prc: 0.3693\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4032 - Sensitivity: 0.0520 - tn: 11536.0000 - auc: 0.7305 - prc: 0.3672 - val_loss: 0.3849 - val_Sensitivity: 0.0382 - val_tn: 3899.0000 - val_auc: 0.7405 - val_prc: 0.3700\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4039 - Sensitivity: 0.0537 - tn: 11523.0000 - auc: 0.7301 - prc: 0.3566 - val_loss: 0.3847 - val_Sensitivity: 0.0382 - val_tn: 3898.0000 - val_auc: 0.7406 - val_prc: 0.3699\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4037 - Sensitivity: 0.0546 - tn: 11536.0000 - auc: 0.7284 - prc: 0.3635 - val_loss: 0.3847 - val_Sensitivity: 0.0382 - val_tn: 3898.0000 - val_auc: 0.7405 - val_prc: 0.3697\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4034 - Sensitivity: 0.0503 - tn: 11524.0000 - auc: 0.7301 - prc: 0.3658 - val_loss: 0.3847 - val_Sensitivity: 0.0396 - val_tn: 3899.0000 - val_auc: 0.7403 - val_prc: 0.3694\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4026 - Sensitivity: 0.0550 - tn: 11526.0000 - auc: 0.7338 - prc: 0.3625 - val_loss: 0.3846 - val_Sensitivity: 0.0396 - val_tn: 3899.0000 - val_auc: 0.7407 - val_prc: 0.3694\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4022 - Sensitivity: 0.0485 - tn: 11538.0000 - auc: 0.7324 - prc: 0.3655 - val_loss: 0.3844 - val_Sensitivity: 0.0409 - val_tn: 3899.0000 - val_auc: 0.7407 - val_prc: 0.3697\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3995 - Sensitivity: 0.0576 - tn: 11508.0000 - auc: 0.7399 - prc: 0.3751 - val_loss: 0.3843 - val_Sensitivity: 0.0423 - val_tn: 3896.0000 - val_auc: 0.7411 - val_prc: 0.3700\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4010 - Sensitivity: 0.0571 - tn: 11531.0000 - auc: 0.7356 - prc: 0.3715 - val_loss: 0.3842 - val_Sensitivity: 0.0437 - val_tn: 3896.0000 - val_auc: 0.7413 - val_prc: 0.3702\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4025 - Sensitivity: 0.0601 - tn: 11506.0000 - auc: 0.7317 - prc: 0.3678 - val_loss: 0.3842 - val_Sensitivity: 0.0464 - val_tn: 3895.0000 - val_auc: 0.7413 - val_prc: 0.3702\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4072 - Sensitivity: 0.0627 - tn: 11503.0000 - auc: 0.7238 - prc: 0.3508 - val_loss: 0.3841 - val_Sensitivity: 0.0437 - val_tn: 3896.0000 - val_auc: 0.7413 - val_prc: 0.3707\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4039 - Sensitivity: 0.0674 - tn: 11512.0000 - auc: 0.7295 - prc: 0.3643 - val_loss: 0.3842 - val_Sensitivity: 0.0423 - val_tn: 3897.0000 - val_auc: 0.7416 - val_prc: 0.3708\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4032 - Sensitivity: 0.0584 - tn: 11506.0000 - auc: 0.7303 - prc: 0.3636 - val_loss: 0.3842 - val_Sensitivity: 0.0396 - val_tn: 3897.0000 - val_auc: 0.7413 - val_prc: 0.3704\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4045 - Sensitivity: 0.0541 - tn: 11533.0000 - auc: 0.7294 - prc: 0.3583 - val_loss: 0.3845 - val_Sensitivity: 0.0382 - val_tn: 3900.0000 - val_auc: 0.7409 - val_prc: 0.3695\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4026 - Sensitivity: 0.0558 - tn: 11524.0000 - auc: 0.7342 - prc: 0.3659 - val_loss: 0.3843 - val_Sensitivity: 0.0409 - val_tn: 3898.0000 - val_auc: 0.7412 - val_prc: 0.3694\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3991 - Sensitivity: 0.0580 - tn: 11528.0000 - auc: 0.7394 - prc: 0.3718 - val_loss: 0.3841 - val_Sensitivity: 0.0396 - val_tn: 3898.0000 - val_auc: 0.7417 - val_prc: 0.3699\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4032 - Sensitivity: 0.0546 - tn: 11525.0000 - auc: 0.7318 - prc: 0.3666 - val_loss: 0.3842 - val_Sensitivity: 0.0396 - val_tn: 3898.0000 - val_auc: 0.7415 - val_prc: 0.3692\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4018 - Sensitivity: 0.0614 - tn: 11522.0000 - auc: 0.7352 - prc: 0.3653 - val_loss: 0.3841 - val_Sensitivity: 0.0423 - val_tn: 3899.0000 - val_auc: 0.7417 - val_prc: 0.3695\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4002 - Sensitivity: 0.0554 - tn: 11541.0000 - auc: 0.7357 - prc: 0.3696 - val_loss: 0.3842 - val_Sensitivity: 0.0396 - val_tn: 3900.0000 - val_auc: 0.7417 - val_prc: 0.3699\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4010 - Sensitivity: 0.0601 - tn: 11523.0000 - auc: 0.7342 - prc: 0.3692 - val_loss: 0.3841 - val_Sensitivity: 0.0396 - val_tn: 3900.0000 - val_auc: 0.7416 - val_prc: 0.3694\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4025 - Sensitivity: 0.0597 - tn: 11508.0000 - auc: 0.7347 - prc: 0.3680 - val_loss: 0.3841 - val_Sensitivity: 0.0396 - val_tn: 3900.0000 - val_auc: 0.7420 - val_prc: 0.3697\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4025 - Sensitivity: 0.0490 - tn: 11525.0000 - auc: 0.7339 - prc: 0.3674 - val_loss: 0.3841 - val_Sensitivity: 0.0396 - val_tn: 3898.0000 - val_auc: 0.7418 - val_prc: 0.3703\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4014 - Sensitivity: 0.0576 - tn: 11517.0000 - auc: 0.7354 - prc: 0.3667 - val_loss: 0.3840 - val_Sensitivity: 0.0437 - val_tn: 3897.0000 - val_auc: 0.7417 - val_prc: 0.3697\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4022 - Sensitivity: 0.0550 - tn: 11525.0000 - auc: 0.7339 - prc: 0.3636 - val_loss: 0.3841 - val_Sensitivity: 0.0423 - val_tn: 3900.0000 - val_auc: 0.7416 - val_prc: 0.3687\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4009 - Sensitivity: 0.0541 - tn: 11531.0000 - auc: 0.7356 - prc: 0.3635 - val_loss: 0.3840 - val_Sensitivity: 0.0409 - val_tn: 3900.0000 - val_auc: 0.7416 - val_prc: 0.3697\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3984 - Sensitivity: 0.0554 - tn: 11526.0000 - auc: 0.7429 - prc: 0.3750 - val_loss: 0.3839 - val_Sensitivity: 0.0409 - val_tn: 3900.0000 - val_auc: 0.7421 - val_prc: 0.3701\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4013 - Sensitivity: 0.0563 - tn: 11520.0000 - auc: 0.7343 - prc: 0.3642 - val_loss: 0.3840 - val_Sensitivity: 0.0409 - val_tn: 3900.0000 - val_auc: 0.7420 - val_prc: 0.3703\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3996 - Sensitivity: 0.0576 - tn: 11539.0000 - auc: 0.7377 - prc: 0.3748 - val_loss: 0.3840 - val_Sensitivity: 0.0423 - val_tn: 3898.0000 - val_auc: 0.7416 - val_prc: 0.3702\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3982 - Sensitivity: 0.0610 - tn: 11525.0000 - auc: 0.7405 - prc: 0.3759 - val_loss: 0.3839 - val_Sensitivity: 0.0423 - val_tn: 3899.0000 - val_auc: 0.7418 - val_prc: 0.3692\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4026 - Sensitivity: 0.0580 - tn: 11516.0000 - auc: 0.7324 - prc: 0.3652 - val_loss: 0.3839 - val_Sensitivity: 0.0437 - val_tn: 3899.0000 - val_auc: 0.7416 - val_prc: 0.3686\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3994 - Sensitivity: 0.0584 - tn: 11510.0000 - auc: 0.7411 - prc: 0.3649 - val_loss: 0.3839 - val_Sensitivity: 0.0423 - val_tn: 3899.0000 - val_auc: 0.7417 - val_prc: 0.3683\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3966 - Sensitivity: 0.0584 - tn: 11523.0000 - auc: 0.7446 - prc: 0.3779 - val_loss: 0.3839 - val_Sensitivity: 0.0437 - val_tn: 3896.0000 - val_auc: 0.7418 - val_prc: 0.3681\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4012 - Sensitivity: 0.0580 - tn: 11517.0000 - auc: 0.7374 - prc: 0.3712 - val_loss: 0.3838 - val_Sensitivity: 0.0450 - val_tn: 3895.0000 - val_auc: 0.7419 - val_prc: 0.3691\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3976 - Sensitivity: 0.0537 - tn: 11528.0000 - auc: 0.7418 - prc: 0.3758 - val_loss: 0.3840 - val_Sensitivity: 0.0423 - val_tn: 3895.0000 - val_auc: 0.7416 - val_prc: 0.3688\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3997 - Sensitivity: 0.0597 - tn: 11533.0000 - auc: 0.7372 - prc: 0.3718 - val_loss: 0.3839 - val_Sensitivity: 0.0450 - val_tn: 3896.0000 - val_auc: 0.7416 - val_prc: 0.3678\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4008 - Sensitivity: 0.0537 - tn: 11510.0000 - auc: 0.7380 - prc: 0.3665 - val_loss: 0.3839 - val_Sensitivity: 0.0450 - val_tn: 3896.0000 - val_auc: 0.7418 - val_prc: 0.3683\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3986 - Sensitivity: 0.0588 - tn: 11531.0000 - auc: 0.7402 - prc: 0.3766 - val_loss: 0.3837 - val_Sensitivity: 0.0437 - val_tn: 3895.0000 - val_auc: 0.7424 - val_prc: 0.3695\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3997 - Sensitivity: 0.0601 - tn: 11519.0000 - auc: 0.7382 - prc: 0.3738 - val_loss: 0.3836 - val_Sensitivity: 0.0437 - val_tn: 3895.0000 - val_auc: 0.7422 - val_prc: 0.3691\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4025 - Sensitivity: 0.0533 - tn: 11520.0000 - auc: 0.7331 - prc: 0.3639 - val_loss: 0.3835 - val_Sensitivity: 0.0437 - val_tn: 3894.0000 - val_auc: 0.7425 - val_prc: 0.3700\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4008 - Sensitivity: 0.0597 - tn: 11513.0000 - auc: 0.7366 - prc: 0.3746 - val_loss: 0.3836 - val_Sensitivity: 0.0450 - val_tn: 3894.0000 - val_auc: 0.7423 - val_prc: 0.3696\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3975 - Sensitivity: 0.0588 - tn: 11525.0000 - auc: 0.7435 - prc: 0.3770 - val_loss: 0.3834 - val_Sensitivity: 0.0423 - val_tn: 3894.0000 - val_auc: 0.7429 - val_prc: 0.3712\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3966 - Sensitivity: 0.0511 - tn: 11531.0000 - auc: 0.7459 - prc: 0.3774 - val_loss: 0.3833 - val_Sensitivity: 0.0409 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3710\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3975 - Sensitivity: 0.0588 - tn: 11523.0000 - auc: 0.7427 - prc: 0.3755 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3704\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3987 - Sensitivity: 0.0606 - tn: 11537.0000 - auc: 0.7405 - prc: 0.3743 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7427 - val_prc: 0.3696\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3987 - Sensitivity: 0.0597 - tn: 11513.0000 - auc: 0.7409 - prc: 0.3742 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7431 - val_prc: 0.3708\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4003 - Sensitivity: 0.0567 - tn: 11507.0000 - auc: 0.7368 - prc: 0.3727 - val_loss: 0.3834 - val_Sensitivity: 0.0450 - val_tn: 3896.0000 - val_auc: 0.7425 - val_prc: 0.3697\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3989 - Sensitivity: 0.0550 - tn: 11531.0000 - auc: 0.7414 - prc: 0.3770 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7429 - val_prc: 0.3701\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3974 - Sensitivity: 0.0640 - tn: 11526.0000 - auc: 0.7436 - prc: 0.3849 - val_loss: 0.3833 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3704\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3982 - Sensitivity: 0.0593 - tn: 11527.0000 - auc: 0.7415 - prc: 0.3804 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7422 - val_prc: 0.3702\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4000 - Sensitivity: 0.0627 - tn: 11531.0000 - auc: 0.7368 - prc: 0.3769 - val_loss: 0.3835 - val_Sensitivity: 0.0464 - val_tn: 3895.0000 - val_auc: 0.7423 - val_prc: 0.3699\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3957 - Sensitivity: 0.0571 - tn: 11511.0000 - auc: 0.7470 - prc: 0.3765 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3895.0000 - val_auc: 0.7424 - val_prc: 0.3693\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3977 - Sensitivity: 0.0597 - tn: 11521.0000 - auc: 0.7423 - prc: 0.3786 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3893.0000 - val_auc: 0.7423 - val_prc: 0.3697\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3949 - Sensitivity: 0.0567 - tn: 11521.0000 - auc: 0.7466 - prc: 0.3845 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7424 - val_prc: 0.3694\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3973 - Sensitivity: 0.0623 - tn: 11515.0000 - auc: 0.7450 - prc: 0.3781 - val_loss: 0.3833 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7422 - val_prc: 0.3695\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3960 - Sensitivity: 0.0619 - tn: 11500.0000 - auc: 0.7460 - prc: 0.3786 - val_loss: 0.3833 - val_Sensitivity: 0.0477 - val_tn: 3892.0000 - val_auc: 0.7426 - val_prc: 0.3698\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3944 - Sensitivity: 0.0653 - tn: 11520.0000 - auc: 0.7490 - prc: 0.3869 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3893.0000 - val_auc: 0.7427 - val_prc: 0.3694\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3973 - Sensitivity: 0.0627 - tn: 11504.0000 - auc: 0.7434 - prc: 0.3757 - val_loss: 0.3832 - val_Sensitivity: 0.0464 - val_tn: 3892.0000 - val_auc: 0.7426 - val_prc: 0.3697\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3987 - Sensitivity: 0.0644 - tn: 11512.0000 - auc: 0.7409 - prc: 0.3725 - val_loss: 0.3833 - val_Sensitivity: 0.0477 - val_tn: 3892.0000 - val_auc: 0.7425 - val_prc: 0.3699\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3972 - Sensitivity: 0.0619 - tn: 11520.0000 - auc: 0.7435 - prc: 0.3784 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7426 - val_prc: 0.3703\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3980 - Sensitivity: 0.0649 - tn: 11521.0000 - auc: 0.7409 - prc: 0.3812 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7425 - val_prc: 0.3700\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3942 - Sensitivity: 0.0674 - tn: 11529.0000 - auc: 0.7470 - prc: 0.3942 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7425 - val_prc: 0.3696\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3967 - Sensitivity: 0.0713 - tn: 11511.0000 - auc: 0.7466 - prc: 0.3823 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3893.0000 - val_auc: 0.7426 - val_prc: 0.3703\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3980 - Sensitivity: 0.0644 - tn: 11518.0000 - auc: 0.7429 - prc: 0.3806 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3893.0000 - val_auc: 0.7426 - val_prc: 0.3705\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3966 - Sensitivity: 0.0662 - tn: 11503.0000 - auc: 0.7447 - prc: 0.3792 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3893.0000 - val_auc: 0.7425 - val_prc: 0.3706\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3955 - Sensitivity: 0.0704 - tn: 11516.0000 - auc: 0.7461 - prc: 0.3810 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7423 - val_prc: 0.3706\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3940 - Sensitivity: 0.0614 - tn: 11517.0000 - auc: 0.7499 - prc: 0.3822 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7425 - val_prc: 0.3691\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3966 - Sensitivity: 0.0627 - tn: 11522.0000 - auc: 0.7451 - prc: 0.3836 - val_loss: 0.3834 - val_Sensitivity: 0.0450 - val_tn: 3894.0000 - val_auc: 0.7425 - val_prc: 0.3690\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3934 - Sensitivity: 0.0597 - tn: 11510.0000 - auc: 0.7520 - prc: 0.3869 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7422 - val_prc: 0.3684\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3954 - Sensitivity: 0.0657 - tn: 11506.0000 - auc: 0.7481 - prc: 0.3798 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7423 - val_prc: 0.3688\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3927 - Sensitivity: 0.0657 - tn: 11512.0000 - auc: 0.7518 - prc: 0.3899 - val_loss: 0.3833 - val_Sensitivity: 0.0450 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3697\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3929 - Sensitivity: 0.0674 - tn: 11517.0000 - auc: 0.7510 - prc: 0.3955 - val_loss: 0.3833 - val_Sensitivity: 0.0450 - val_tn: 3894.0000 - val_auc: 0.7425 - val_prc: 0.3682\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3937 - Sensitivity: 0.0829 - tn: 11502.0000 - auc: 0.7495 - prc: 0.3920 - val_loss: 0.3833 - val_Sensitivity: 0.0505 - val_tn: 3891.0000 - val_auc: 0.7423 - val_prc: 0.3678\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3950 - Sensitivity: 0.0786 - tn: 11505.0000 - auc: 0.7488 - prc: 0.3867 - val_loss: 0.3832 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7427 - val_prc: 0.3681\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0623 - tn: 11526.0000 - auc: 0.7514 - prc: 0.3947 - val_loss: 0.3832 - val_Sensitivity: 0.0450 - val_tn: 3894.0000 - val_auc: 0.7426 - val_prc: 0.3692\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3963 - Sensitivity: 0.0696 - tn: 11528.0000 - auc: 0.7436 - prc: 0.3859 - val_loss: 0.3831 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7432 - val_prc: 0.3694\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3940 - Sensitivity: 0.0674 - tn: 11506.0000 - auc: 0.7492 - prc: 0.3889 - val_loss: 0.3831 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7430 - val_prc: 0.3693\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3958 - Sensitivity: 0.0597 - tn: 11509.0000 - auc: 0.7464 - prc: 0.3800 - val_loss: 0.3832 - val_Sensitivity: 0.0450 - val_tn: 3894.0000 - val_auc: 0.7431 - val_prc: 0.3686\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3912 - Sensitivity: 0.0700 - tn: 11512.0000 - auc: 0.7550 - prc: 0.3939 - val_loss: 0.3832 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7429 - val_prc: 0.3687\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3917 - Sensitivity: 0.0619 - tn: 11525.0000 - auc: 0.7565 - prc: 0.3930 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3688\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3934 - Sensitivity: 0.0662 - tn: 11518.0000 - auc: 0.7500 - prc: 0.3891 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7429 - val_prc: 0.3683\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.3944 - Sensitivity: 0.0631 - tn: 11516.0000 - auc: 0.7477 - prc: 0.3884 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3681\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3910 - Sensitivity: 0.0601 - tn: 11535.0000 - auc: 0.7552 - prc: 0.3992 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3669\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3922 - Sensitivity: 0.0730 - tn: 11519.0000 - auc: 0.7519 - prc: 0.3966 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7427 - val_prc: 0.3676\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3954 - Sensitivity: 0.0627 - tn: 11500.0000 - auc: 0.7463 - prc: 0.3796 - val_loss: 0.3834 - val_Sensitivity: 0.0477 - val_tn: 3894.0000 - val_auc: 0.7428 - val_prc: 0.3676\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3929 - Sensitivity: 0.0657 - tn: 11504.0000 - auc: 0.7527 - prc: 0.3840 - val_loss: 0.3834 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7426 - val_prc: 0.3674\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3925 - Sensitivity: 0.0666 - tn: 11517.0000 - auc: 0.7521 - prc: 0.3947 - val_loss: 0.3833 - val_Sensitivity: 0.0464 - val_tn: 3894.0000 - val_auc: 0.7430 - val_prc: 0.3672\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3936 - Sensitivity: 0.0743 - tn: 11518.0000 - auc: 0.7510 - prc: 0.3964 - val_loss: 0.3833 - val_Sensitivity: 0.0491 - val_tn: 3893.0000 - val_auc: 0.7428 - val_prc: 0.3671\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3934 - Sensitivity: 0.0752 - tn: 11501.0000 - auc: 0.7515 - prc: 0.3871 - val_loss: 0.3833 - val_Sensitivity: 0.0477 - val_tn: 3893.0000 - val_auc: 0.7429 - val_prc: 0.3671\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0752 - tn: 11490.0000 - auc: 0.7541 - prc: 0.3927 - val_loss: 0.3833 - val_Sensitivity: 0.0491 - val_tn: 3893.0000 - val_auc: 0.7428 - val_prc: 0.3668\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3924 - Sensitivity: 0.0722 - tn: 11519.0000 - auc: 0.7530 - prc: 0.3968 - val_loss: 0.3834 - val_Sensitivity: 0.0491 - val_tn: 3893.0000 - val_auc: 0.7424 - val_prc: 0.3668\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3927 - Sensitivity: 0.0752 - tn: 11507.0000 - auc: 0.7528 - prc: 0.3984 - val_loss: 0.3834 - val_Sensitivity: 0.0505 - val_tn: 3892.0000 - val_auc: 0.7425 - val_prc: 0.3661\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3900 - Sensitivity: 0.0743 - tn: 11498.0000 - auc: 0.7568 - prc: 0.3949 - val_loss: 0.3833 - val_Sensitivity: 0.0491 - val_tn: 3892.0000 - val_auc: 0.7422 - val_prc: 0.3673\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3942 - Sensitivity: 0.0709 - tn: 11501.0000 - auc: 0.7502 - prc: 0.3928 - val_loss: 0.3833 - val_Sensitivity: 0.0505 - val_tn: 3892.0000 - val_auc: 0.7427 - val_prc: 0.3673\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3899 - Sensitivity: 0.0743 - tn: 11499.0000 - auc: 0.7567 - prc: 0.3983 - val_loss: 0.3832 - val_Sensitivity: 0.0505 - val_tn: 3892.0000 - val_auc: 0.7426 - val_prc: 0.3669\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.3920 - Sensitivity: 0.0795 - tn: 11493.0000 - auc: 0.7532 - prc: 0.3954 - val_loss: 0.3832 - val_Sensitivity: 0.0505 - val_tn: 3892.0000 - val_auc: 0.7427 - val_prc: 0.3673\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3938 - Sensitivity: 0.0726 - tn: 11510.0000 - auc: 0.7496 - prc: 0.3899 - val_loss: 0.3834 - val_Sensitivity: 0.0491 - val_tn: 3892.0000 - val_auc: 0.7426 - val_prc: 0.3669\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3940 - Sensitivity: 0.0696 - tn: 11502.0000 - auc: 0.7504 - prc: 0.3914 - val_loss: 0.3835 - val_Sensitivity: 0.0505 - val_tn: 3892.0000 - val_auc: 0.7425 - val_prc: 0.3665\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3925 - Sensitivity: 0.0704 - tn: 11503.0000 - auc: 0.7527 - prc: 0.3906 - val_loss: 0.3836 - val_Sensitivity: 0.0505 - val_tn: 3892.0000 - val_auc: 0.7423 - val_prc: 0.3665\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3905 - Sensitivity: 0.0790 - tn: 11504.0000 - auc: 0.7574 - prc: 0.3996 - val_loss: 0.3835 - val_Sensitivity: 0.0477 - val_tn: 3892.0000 - val_auc: 0.7422 - val_prc: 0.3671\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3942 - Sensitivity: 0.0704 - tn: 11494.0000 - auc: 0.7482 - prc: 0.3836 - val_loss: 0.3835 - val_Sensitivity: 0.0477 - val_tn: 3891.0000 - val_auc: 0.7423 - val_prc: 0.3660\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3917 - Sensitivity: 0.0782 - tn: 11504.0000 - auc: 0.7545 - prc: 0.3926 - val_loss: 0.3835 - val_Sensitivity: 0.0477 - val_tn: 3892.0000 - val_auc: 0.7424 - val_prc: 0.3666\n",
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 27ms/step - loss: 1.0074 - Sensitivity: 0.4926 - tn: 6094.0000 - auc: 0.5120 - prc: 0.1702 - val_loss: 0.4662 - val_Sensitivity: 0.0065 - val_tn: 3862.0000 - val_auc: 0.5438 - val_prc: 0.1873\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8068 - Sensitivity: 0.3912 - tn: 7697.0000 - auc: 0.5355 - prc: 0.1833 - val_loss: 0.4480 - val_Sensitivity: 0.0026 - val_tn: 3869.0000 - val_auc: 0.5961 - val_prc: 0.2174\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.6966 - Sensitivity: 0.3064 - tn: 8873.0000 - auc: 0.5534 - prc: 0.1932 - val_loss: 0.4399 - val_Sensitivity: 0.0052 - val_tn: 3870.0000 - val_auc: 0.6245 - val_prc: 0.2397\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6472 - Sensitivity: 0.2386 - tn: 9455.0000 - auc: 0.5557 - prc: 0.1971 - val_loss: 0.4367 - val_Sensitivity: 0.0026 - val_tn: 3871.0000 - val_auc: 0.6399 - val_prc: 0.2522\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6175 - Sensitivity: 0.2146 - tn: 9985.0000 - auc: 0.5639 - prc: 0.2044 - val_loss: 0.4333 - val_Sensitivity: 0.0026 - val_tn: 3872.0000 - val_auc: 0.6559 - val_prc: 0.2667\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5986 - Sensitivity: 0.1705 - tn: 10187.0000 - auc: 0.5724 - prc: 0.1983 - val_loss: 0.4305 - val_Sensitivity: 0.0039 - val_tn: 3870.0000 - val_auc: 0.6688 - val_prc: 0.2789\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5830 - Sensitivity: 0.1997 - tn: 10323.0000 - auc: 0.5856 - prc: 0.2165 - val_loss: 0.4277 - val_Sensitivity: 0.0103 - val_tn: 3869.0000 - val_auc: 0.6774 - val_prc: 0.2883\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5690 - Sensitivity: 0.1770 - tn: 10476.0000 - auc: 0.5928 - prc: 0.2197 - val_loss: 0.4263 - val_Sensitivity: 0.0116 - val_tn: 3868.0000 - val_auc: 0.6832 - val_prc: 0.2964\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5619 - Sensitivity: 0.1735 - tn: 10467.0000 - auc: 0.5979 - prc: 0.2262 - val_loss: 0.4253 - val_Sensitivity: 0.0155 - val_tn: 3863.0000 - val_auc: 0.6873 - val_prc: 0.3032\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5479 - Sensitivity: 0.1783 - tn: 10594.0000 - auc: 0.6086 - prc: 0.2278 - val_loss: 0.4250 - val_Sensitivity: 0.0142 - val_tn: 3862.0000 - val_auc: 0.6902 - val_prc: 0.3080\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5291 - Sensitivity: 0.1809 - tn: 10646.0000 - auc: 0.6230 - prc: 0.2434 - val_loss: 0.4242 - val_Sensitivity: 0.0194 - val_tn: 3863.0000 - val_auc: 0.6929 - val_prc: 0.3127\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5341 - Sensitivity: 0.1713 - tn: 10604.0000 - auc: 0.6179 - prc: 0.2354 - val_loss: 0.4239 - val_Sensitivity: 0.0207 - val_tn: 3862.0000 - val_auc: 0.6952 - val_prc: 0.3159\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5100 - Sensitivity: 0.1674 - tn: 10733.0000 - auc: 0.6342 - prc: 0.2490 - val_loss: 0.4233 - val_Sensitivity: 0.0246 - val_tn: 3861.0000 - val_auc: 0.6973 - val_prc: 0.3196\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5165 - Sensitivity: 0.1595 - tn: 10756.0000 - auc: 0.6313 - prc: 0.2445 - val_loss: 0.4225 - val_Sensitivity: 0.0323 - val_tn: 3859.0000 - val_auc: 0.6986 - val_prc: 0.3219\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5095 - Sensitivity: 0.1613 - tn: 10779.0000 - auc: 0.6320 - prc: 0.2448 - val_loss: 0.4221 - val_Sensitivity: 0.0336 - val_tn: 3857.0000 - val_auc: 0.7004 - val_prc: 0.3241\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5034 - Sensitivity: 0.1490 - tn: 10856.0000 - auc: 0.6348 - prc: 0.2457 - val_loss: 0.4211 - val_Sensitivity: 0.0349 - val_tn: 3856.0000 - val_auc: 0.7012 - val_prc: 0.3265\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4883 - Sensitivity: 0.1543 - tn: 10856.0000 - auc: 0.6463 - prc: 0.2602 - val_loss: 0.4206 - val_Sensitivity: 0.0336 - val_tn: 3858.0000 - val_auc: 0.7010 - val_prc: 0.3271\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4940 - Sensitivity: 0.1464 - tn: 10879.0000 - auc: 0.6411 - prc: 0.2474 - val_loss: 0.4202 - val_Sensitivity: 0.0336 - val_tn: 3857.0000 - val_auc: 0.7018 - val_prc: 0.3291\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4817 - Sensitivity: 0.1512 - tn: 10950.0000 - auc: 0.6487 - prc: 0.2637 - val_loss: 0.4198 - val_Sensitivity: 0.0362 - val_tn: 3857.0000 - val_auc: 0.7024 - val_prc: 0.3298\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4744 - Sensitivity: 0.1447 - tn: 10984.0000 - auc: 0.6539 - prc: 0.2676 - val_loss: 0.4190 - val_Sensitivity: 0.0388 - val_tn: 3857.0000 - val_auc: 0.7028 - val_prc: 0.3302\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4694 - Sensitivity: 0.1639 - tn: 11013.0000 - auc: 0.6574 - prc: 0.2842 - val_loss: 0.4186 - val_Sensitivity: 0.0375 - val_tn: 3857.0000 - val_auc: 0.7034 - val_prc: 0.3305\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4690 - Sensitivity: 0.1482 - tn: 11048.0000 - auc: 0.6577 - prc: 0.2741 - val_loss: 0.4182 - val_Sensitivity: 0.0401 - val_tn: 3856.0000 - val_auc: 0.7041 - val_prc: 0.3316\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4697 - Sensitivity: 0.1206 - tn: 11067.0000 - auc: 0.6550 - prc: 0.2591 - val_loss: 0.4177 - val_Sensitivity: 0.0375 - val_tn: 3857.0000 - val_auc: 0.7049 - val_prc: 0.3320\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4649 - Sensitivity: 0.1289 - tn: 11075.0000 - auc: 0.6585 - prc: 0.2689 - val_loss: 0.4172 - val_Sensitivity: 0.0388 - val_tn: 3857.0000 - val_auc: 0.7054 - val_prc: 0.3326\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4643 - Sensitivity: 0.1267 - tn: 11099.0000 - auc: 0.6522 - prc: 0.2669 - val_loss: 0.4164 - val_Sensitivity: 0.0401 - val_tn: 3857.0000 - val_auc: 0.7064 - val_prc: 0.3340\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4555 - Sensitivity: 0.1184 - tn: 11120.0000 - auc: 0.6650 - prc: 0.2773 - val_loss: 0.4163 - val_Sensitivity: 0.0375 - val_tn: 3857.0000 - val_auc: 0.7064 - val_prc: 0.3343\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4511 - Sensitivity: 0.1136 - tn: 11167.0000 - auc: 0.6681 - prc: 0.2760 - val_loss: 0.4162 - val_Sensitivity: 0.0349 - val_tn: 3858.0000 - val_auc: 0.7069 - val_prc: 0.3347\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4487 - Sensitivity: 0.1106 - tn: 11212.0000 - auc: 0.6664 - prc: 0.2799 - val_loss: 0.4161 - val_Sensitivity: 0.0336 - val_tn: 3859.0000 - val_auc: 0.7073 - val_prc: 0.3353\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4517 - Sensitivity: 0.1163 - tn: 11245.0000 - auc: 0.6610 - prc: 0.2787 - val_loss: 0.4153 - val_Sensitivity: 0.0336 - val_tn: 3859.0000 - val_auc: 0.7086 - val_prc: 0.3370\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4476 - Sensitivity: 0.1106 - tn: 11237.0000 - auc: 0.6670 - prc: 0.2814 - val_loss: 0.4151 - val_Sensitivity: 0.0336 - val_tn: 3859.0000 - val_auc: 0.7082 - val_prc: 0.3364\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4441 - Sensitivity: 0.1036 - tn: 11270.0000 - auc: 0.6675 - prc: 0.2818 - val_loss: 0.4144 - val_Sensitivity: 0.0336 - val_tn: 3861.0000 - val_auc: 0.7089 - val_prc: 0.3371\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4372 - Sensitivity: 0.0979 - tn: 11304.0000 - auc: 0.6786 - prc: 0.2866 - val_loss: 0.4142 - val_Sensitivity: 0.0310 - val_tn: 3862.0000 - val_auc: 0.7089 - val_prc: 0.3374\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4339 - Sensitivity: 0.1014 - tn: 11330.0000 - auc: 0.6815 - prc: 0.2972 - val_loss: 0.4135 - val_Sensitivity: 0.0310 - val_tn: 3862.0000 - val_auc: 0.7098 - val_prc: 0.3381\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4362 - Sensitivity: 0.0839 - tn: 11373.0000 - auc: 0.6720 - prc: 0.2894 - val_loss: 0.4126 - val_Sensitivity: 0.0310 - val_tn: 3861.0000 - val_auc: 0.7103 - val_prc: 0.3385\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4323 - Sensitivity: 0.0839 - tn: 11380.0000 - auc: 0.6794 - prc: 0.2856 - val_loss: 0.4123 - val_Sensitivity: 0.0298 - val_tn: 3862.0000 - val_auc: 0.7108 - val_prc: 0.3391\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4288 - Sensitivity: 0.0800 - tn: 11376.0000 - auc: 0.6849 - prc: 0.2979 - val_loss: 0.4121 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7111 - val_prc: 0.3392\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4276 - Sensitivity: 0.0883 - tn: 11393.0000 - auc: 0.6871 - prc: 0.3015 - val_loss: 0.4119 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7115 - val_prc: 0.3395\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4333 - Sensitivity: 0.0787 - tn: 11387.0000 - auc: 0.6771 - prc: 0.2914 - val_loss: 0.4112 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7121 - val_prc: 0.3398\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4235 - Sensitivity: 0.0817 - tn: 11437.0000 - auc: 0.6920 - prc: 0.3065 - val_loss: 0.4110 - val_Sensitivity: 0.0272 - val_tn: 3864.0000 - val_auc: 0.7124 - val_prc: 0.3405\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4283 - Sensitivity: 0.0712 - tn: 11421.0000 - auc: 0.6817 - prc: 0.2959 - val_loss: 0.4106 - val_Sensitivity: 0.0285 - val_tn: 3864.0000 - val_auc: 0.7133 - val_prc: 0.3420\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4233 - Sensitivity: 0.0721 - tn: 11463.0000 - auc: 0.6918 - prc: 0.3094 - val_loss: 0.4111 - val_Sensitivity: 0.0246 - val_tn: 3865.0000 - val_auc: 0.7127 - val_prc: 0.3407\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4203 - Sensitivity: 0.0708 - tn: 11498.0000 - auc: 0.6940 - prc: 0.3116 - val_loss: 0.4102 - val_Sensitivity: 0.0272 - val_tn: 3864.0000 - val_auc: 0.7134 - val_prc: 0.3414\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4207 - Sensitivity: 0.0642 - tn: 11465.0000 - auc: 0.6937 - prc: 0.3020 - val_loss: 0.4100 - val_Sensitivity: 0.0272 - val_tn: 3864.0000 - val_auc: 0.7140 - val_prc: 0.3412\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4235 - Sensitivity: 0.0734 - tn: 11482.0000 - auc: 0.6891 - prc: 0.3087 - val_loss: 0.4096 - val_Sensitivity: 0.0285 - val_tn: 3863.0000 - val_auc: 0.7145 - val_prc: 0.3420\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4194 - Sensitivity: 0.0625 - tn: 11508.0000 - auc: 0.6968 - prc: 0.3126 - val_loss: 0.4093 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7149 - val_prc: 0.3429\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4156 - Sensitivity: 0.0664 - tn: 11498.0000 - auc: 0.7022 - prc: 0.3193 - val_loss: 0.4096 - val_Sensitivity: 0.0246 - val_tn: 3866.0000 - val_auc: 0.7150 - val_prc: 0.3424\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4173 - Sensitivity: 0.0590 - tn: 11524.0000 - auc: 0.6968 - prc: 0.3170 - val_loss: 0.4095 - val_Sensitivity: 0.0259 - val_tn: 3866.0000 - val_auc: 0.7153 - val_prc: 0.3434\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4183 - Sensitivity: 0.0555 - tn: 11518.0000 - auc: 0.6989 - prc: 0.3144 - val_loss: 0.4091 - val_Sensitivity: 0.0259 - val_tn: 3866.0000 - val_auc: 0.7157 - val_prc: 0.3435\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4121 - Sensitivity: 0.0629 - tn: 11530.0000 - auc: 0.7075 - prc: 0.3253 - val_loss: 0.4090 - val_Sensitivity: 0.0272 - val_tn: 3866.0000 - val_auc: 0.7157 - val_prc: 0.3431\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4198 - Sensitivity: 0.0573 - tn: 11536.0000 - auc: 0.6933 - prc: 0.3189 - val_loss: 0.4088 - val_Sensitivity: 0.0298 - val_tn: 3866.0000 - val_auc: 0.7161 - val_prc: 0.3438\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4141 - Sensitivity: 0.0594 - tn: 11537.0000 - auc: 0.7026 - prc: 0.3240 - val_loss: 0.4085 - val_Sensitivity: 0.0298 - val_tn: 3864.0000 - val_auc: 0.7166 - val_prc: 0.3437\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4139 - Sensitivity: 0.0568 - tn: 11525.0000 - auc: 0.7035 - prc: 0.3269 - val_loss: 0.4084 - val_Sensitivity: 0.0272 - val_tn: 3866.0000 - val_auc: 0.7169 - val_prc: 0.3441\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4126 - Sensitivity: 0.0603 - tn: 11536.0000 - auc: 0.7061 - prc: 0.3265 - val_loss: 0.4081 - val_Sensitivity: 0.0285 - val_tn: 3864.0000 - val_auc: 0.7171 - val_prc: 0.3446\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4114 - Sensitivity: 0.0590 - tn: 11532.0000 - auc: 0.7089 - prc: 0.3290 - val_loss: 0.4082 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7171 - val_prc: 0.3442\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4125 - Sensitivity: 0.0516 - tn: 11553.0000 - auc: 0.7045 - prc: 0.3255 - val_loss: 0.4081 - val_Sensitivity: 0.0272 - val_tn: 3866.0000 - val_auc: 0.7173 - val_prc: 0.3438\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4111 - Sensitivity: 0.0476 - tn: 11551.0000 - auc: 0.7071 - prc: 0.3273 - val_loss: 0.4079 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7178 - val_prc: 0.3441\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4090 - Sensitivity: 0.0538 - tn: 11543.0000 - auc: 0.7126 - prc: 0.3287 - val_loss: 0.4077 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7180 - val_prc: 0.3444\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4109 - Sensitivity: 0.0490 - tn: 11561.0000 - auc: 0.7075 - prc: 0.3314 - val_loss: 0.4077 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7184 - val_prc: 0.3451\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4130 - Sensitivity: 0.0459 - tn: 11560.0000 - auc: 0.7052 - prc: 0.3220 - val_loss: 0.4071 - val_Sensitivity: 0.0285 - val_tn: 3866.0000 - val_auc: 0.7191 - val_prc: 0.3456\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4113 - Sensitivity: 0.0424 - tn: 11547.0000 - auc: 0.7072 - prc: 0.3173 - val_loss: 0.4072 - val_Sensitivity: 0.0259 - val_tn: 3867.0000 - val_auc: 0.7192 - val_prc: 0.3458\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4105 - Sensitivity: 0.0459 - tn: 11569.0000 - auc: 0.7085 - prc: 0.3274 - val_loss: 0.4068 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7196 - val_prc: 0.3464\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.4056 - Sensitivity: 0.0463 - tn: 11564.0000 - auc: 0.7184 - prc: 0.3332 - val_loss: 0.4068 - val_Sensitivity: 0.0246 - val_tn: 3867.0000 - val_auc: 0.7200 - val_prc: 0.3467\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4102 - Sensitivity: 0.0437 - tn: 11570.0000 - auc: 0.7118 - prc: 0.3378 - val_loss: 0.4065 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7202 - val_prc: 0.3474\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4067 - Sensitivity: 0.0459 - tn: 11553.0000 - auc: 0.7169 - prc: 0.3351 - val_loss: 0.4066 - val_Sensitivity: 0.0246 - val_tn: 3867.0000 - val_auc: 0.7204 - val_prc: 0.3477\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4073 - Sensitivity: 0.0428 - tn: 11567.0000 - auc: 0.7161 - prc: 0.3410 - val_loss: 0.4068 - val_Sensitivity: 0.0246 - val_tn: 3867.0000 - val_auc: 0.7204 - val_prc: 0.3471\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4059 - Sensitivity: 0.0494 - tn: 11572.0000 - auc: 0.7184 - prc: 0.3400 - val_loss: 0.4064 - val_Sensitivity: 0.0259 - val_tn: 3867.0000 - val_auc: 0.7207 - val_prc: 0.3472\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4054 - Sensitivity: 0.0533 - tn: 11579.0000 - auc: 0.7199 - prc: 0.3494 - val_loss: 0.4063 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7209 - val_prc: 0.3471\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4028 - Sensitivity: 0.0455 - tn: 11572.0000 - auc: 0.7249 - prc: 0.3421 - val_loss: 0.4061 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7212 - val_prc: 0.3477\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4044 - Sensitivity: 0.0520 - tn: 11578.0000 - auc: 0.7210 - prc: 0.3486 - val_loss: 0.4058 - val_Sensitivity: 0.0285 - val_tn: 3866.0000 - val_auc: 0.7217 - val_prc: 0.3483\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4046 - Sensitivity: 0.0476 - tn: 11570.0000 - auc: 0.7199 - prc: 0.3428 - val_loss: 0.4057 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7219 - val_prc: 0.3487\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4042 - Sensitivity: 0.0463 - tn: 11574.0000 - auc: 0.7217 - prc: 0.3435 - val_loss: 0.4059 - val_Sensitivity: 0.0259 - val_tn: 3867.0000 - val_auc: 0.7223 - val_prc: 0.3490\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4018 - Sensitivity: 0.0524 - tn: 11575.0000 - auc: 0.7260 - prc: 0.3508 - val_loss: 0.4055 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7226 - val_prc: 0.3491\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4067 - Sensitivity: 0.0459 - tn: 11573.0000 - auc: 0.7160 - prc: 0.3388 - val_loss: 0.4056 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7228 - val_prc: 0.3490\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4030 - Sensitivity: 0.0542 - tn: 11569.0000 - auc: 0.7215 - prc: 0.3510 - val_loss: 0.4053 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7233 - val_prc: 0.3493\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4064 - Sensitivity: 0.0485 - tn: 11568.0000 - auc: 0.7167 - prc: 0.3356 - val_loss: 0.4051 - val_Sensitivity: 0.0272 - val_tn: 3867.0000 - val_auc: 0.7237 - val_prc: 0.3501\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4012 - Sensitivity: 0.0503 - tn: 11577.0000 - auc: 0.7266 - prc: 0.3497 - val_loss: 0.4049 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7237 - val_prc: 0.3496\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4029 - Sensitivity: 0.0494 - tn: 11585.0000 - auc: 0.7234 - prc: 0.3518 - val_loss: 0.4048 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7239 - val_prc: 0.3497\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.4029 - Sensitivity: 0.0503 - tn: 11575.0000 - auc: 0.7237 - prc: 0.3467 - val_loss: 0.4048 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7243 - val_prc: 0.3500\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4056 - Sensitivity: 0.0529 - tn: 11562.0000 - auc: 0.7176 - prc: 0.3501 - val_loss: 0.4048 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7242 - val_prc: 0.3502\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4026 - Sensitivity: 0.0524 - tn: 11570.0000 - auc: 0.7239 - prc: 0.3438 - val_loss: 0.4048 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7246 - val_prc: 0.3505\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4032 - Sensitivity: 0.0485 - tn: 11569.0000 - auc: 0.7237 - prc: 0.3412 - val_loss: 0.4046 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7248 - val_prc: 0.3504\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4011 - Sensitivity: 0.0507 - tn: 11551.0000 - auc: 0.7275 - prc: 0.3470 - val_loss: 0.4044 - val_Sensitivity: 0.0285 - val_tn: 3866.0000 - val_auc: 0.7251 - val_prc: 0.3508\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4009 - Sensitivity: 0.0529 - tn: 11570.0000 - auc: 0.7264 - prc: 0.3557 - val_loss: 0.4044 - val_Sensitivity: 0.0285 - val_tn: 3867.0000 - val_auc: 0.7250 - val_prc: 0.3512\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4003 - Sensitivity: 0.0538 - tn: 11574.0000 - auc: 0.7294 - prc: 0.3584 - val_loss: 0.4041 - val_Sensitivity: 0.0285 - val_tn: 3864.0000 - val_auc: 0.7256 - val_prc: 0.3515\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4018 - Sensitivity: 0.0463 - tn: 11567.0000 - auc: 0.7264 - prc: 0.3492 - val_loss: 0.4042 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7255 - val_prc: 0.3518\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4006 - Sensitivity: 0.0490 - tn: 11576.0000 - auc: 0.7268 - prc: 0.3599 - val_loss: 0.4040 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7256 - val_prc: 0.3524\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4001 - Sensitivity: 0.0555 - tn: 11577.0000 - auc: 0.7278 - prc: 0.3615 - val_loss: 0.4042 - val_Sensitivity: 0.0272 - val_tn: 3865.0000 - val_auc: 0.7253 - val_prc: 0.3523\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3993 - Sensitivity: 0.0472 - tn: 11567.0000 - auc: 0.7314 - prc: 0.3554 - val_loss: 0.4043 - val_Sensitivity: 0.0272 - val_tn: 3866.0000 - val_auc: 0.7254 - val_prc: 0.3522\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4001 - Sensitivity: 0.0446 - tn: 11577.0000 - auc: 0.7293 - prc: 0.3510 - val_loss: 0.4043 - val_Sensitivity: 0.0272 - val_tn: 3866.0000 - val_auc: 0.7254 - val_prc: 0.3523\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4000 - Sensitivity: 0.0533 - tn: 11582.0000 - auc: 0.7302 - prc: 0.3612 - val_loss: 0.4039 - val_Sensitivity: 0.0285 - val_tn: 3864.0000 - val_auc: 0.7257 - val_prc: 0.3521\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3980 - Sensitivity: 0.0516 - tn: 11558.0000 - auc: 0.7338 - prc: 0.3540 - val_loss: 0.4039 - val_Sensitivity: 0.0285 - val_tn: 3863.0000 - val_auc: 0.7260 - val_prc: 0.3528\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3993 - Sensitivity: 0.0490 - tn: 11574.0000 - auc: 0.7317 - prc: 0.3550 - val_loss: 0.4039 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7264 - val_prc: 0.3534\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3997 - Sensitivity: 0.0555 - tn: 11571.0000 - auc: 0.7294 - prc: 0.3581 - val_loss: 0.4037 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7268 - val_prc: 0.3540\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3985 - Sensitivity: 0.0498 - tn: 11579.0000 - auc: 0.7340 - prc: 0.3568 - val_loss: 0.4036 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7268 - val_prc: 0.3543\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3992 - Sensitivity: 0.0520 - tn: 11580.0000 - auc: 0.7302 - prc: 0.3583 - val_loss: 0.4036 - val_Sensitivity: 0.0285 - val_tn: 3865.0000 - val_auc: 0.7267 - val_prc: 0.3538\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4007 - Sensitivity: 0.0507 - tn: 11569.0000 - auc: 0.7298 - prc: 0.3544 - val_loss: 0.4037 - val_Sensitivity: 0.0285 - val_tn: 3866.0000 - val_auc: 0.7270 - val_prc: 0.3542\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3985 - Sensitivity: 0.0481 - tn: 11584.0000 - auc: 0.7329 - prc: 0.3575 - val_loss: 0.4036 - val_Sensitivity: 0.0285 - val_tn: 3866.0000 - val_auc: 0.7271 - val_prc: 0.3544\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3973 - Sensitivity: 0.0498 - tn: 11579.0000 - auc: 0.7347 - prc: 0.3668 - val_loss: 0.4036 - val_Sensitivity: 0.0285 - val_tn: 3866.0000 - val_auc: 0.7268 - val_prc: 0.3548\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3995 - Sensitivity: 0.0476 - tn: 11558.0000 - auc: 0.7314 - prc: 0.3581 - val_loss: 0.4035 - val_Sensitivity: 0.0285 - val_tn: 3864.0000 - val_auc: 0.7270 - val_prc: 0.3544\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3975 - Sensitivity: 0.0564 - tn: 11571.0000 - auc: 0.7350 - prc: 0.3639 - val_loss: 0.4032 - val_Sensitivity: 0.0310 - val_tn: 3863.0000 - val_auc: 0.7271 - val_prc: 0.3552\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3997 - Sensitivity: 0.0629 - tn: 11570.0000 - auc: 0.7309 - prc: 0.3626 - val_loss: 0.4034 - val_Sensitivity: 0.0285 - val_tn: 3863.0000 - val_auc: 0.7268 - val_prc: 0.3549\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3967 - Sensitivity: 0.0546 - tn: 11574.0000 - auc: 0.7354 - prc: 0.3757 - val_loss: 0.4032 - val_Sensitivity: 0.0310 - val_tn: 3863.0000 - val_auc: 0.7273 - val_prc: 0.3556\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3997 - Sensitivity: 0.0568 - tn: 11550.0000 - auc: 0.7295 - prc: 0.3615 - val_loss: 0.4031 - val_Sensitivity: 0.0310 - val_tn: 3862.0000 - val_auc: 0.7272 - val_prc: 0.3556\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3995 - Sensitivity: 0.0612 - tn: 11561.0000 - auc: 0.7298 - prc: 0.3622 - val_loss: 0.4029 - val_Sensitivity: 0.0310 - val_tn: 3862.0000 - val_auc: 0.7278 - val_prc: 0.3565\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3960 - Sensitivity: 0.0573 - tn: 11566.0000 - auc: 0.7369 - prc: 0.3603 - val_loss: 0.4031 - val_Sensitivity: 0.0298 - val_tn: 3862.0000 - val_auc: 0.7275 - val_prc: 0.3564\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3972 - Sensitivity: 0.0533 - tn: 11578.0000 - auc: 0.7352 - prc: 0.3722 - val_loss: 0.4029 - val_Sensitivity: 0.0298 - val_tn: 3862.0000 - val_auc: 0.7276 - val_prc: 0.3563\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3973 - Sensitivity: 0.0546 - tn: 11573.0000 - auc: 0.7369 - prc: 0.3634 - val_loss: 0.4032 - val_Sensitivity: 0.0298 - val_tn: 3864.0000 - val_auc: 0.7277 - val_prc: 0.3563\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3972 - Sensitivity: 0.0559 - tn: 11564.0000 - auc: 0.7357 - prc: 0.3647 - val_loss: 0.4029 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7280 - val_prc: 0.3569\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3979 - Sensitivity: 0.0516 - tn: 11572.0000 - auc: 0.7336 - prc: 0.3562 - val_loss: 0.4030 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7279 - val_prc: 0.3564\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3953 - Sensitivity: 0.0564 - tn: 11578.0000 - auc: 0.7390 - prc: 0.3685 - val_loss: 0.4030 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7282 - val_prc: 0.3576\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3948 - Sensitivity: 0.0551 - tn: 11569.0000 - auc: 0.7408 - prc: 0.3707 - val_loss: 0.4027 - val_Sensitivity: 0.0298 - val_tn: 3862.0000 - val_auc: 0.7283 - val_prc: 0.3579\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3965 - Sensitivity: 0.0529 - tn: 11562.0000 - auc: 0.7378 - prc: 0.3694 - val_loss: 0.4026 - val_Sensitivity: 0.0298 - val_tn: 3862.0000 - val_auc: 0.7286 - val_prc: 0.3579\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3981 - Sensitivity: 0.0533 - tn: 11562.0000 - auc: 0.7352 - prc: 0.3700 - val_loss: 0.4026 - val_Sensitivity: 0.0298 - val_tn: 3862.0000 - val_auc: 0.7285 - val_prc: 0.3567\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3981 - Sensitivity: 0.0529 - tn: 11565.0000 - auc: 0.7342 - prc: 0.3642 - val_loss: 0.4024 - val_Sensitivity: 0.0323 - val_tn: 3861.0000 - val_auc: 0.7286 - val_prc: 0.3568\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3955 - Sensitivity: 0.0616 - tn: 11569.0000 - auc: 0.7372 - prc: 0.3703 - val_loss: 0.4025 - val_Sensitivity: 0.0298 - val_tn: 3861.0000 - val_auc: 0.7286 - val_prc: 0.3573\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3977 - Sensitivity: 0.0546 - tn: 11565.0000 - auc: 0.7353 - prc: 0.3593 - val_loss: 0.4025 - val_Sensitivity: 0.0298 - val_tn: 3863.0000 - val_auc: 0.7287 - val_prc: 0.3579\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3950 - Sensitivity: 0.0616 - tn: 11566.0000 - auc: 0.7390 - prc: 0.3705 - val_loss: 0.4022 - val_Sensitivity: 0.0323 - val_tn: 3861.0000 - val_auc: 0.7292 - val_prc: 0.3587\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3956 - Sensitivity: 0.0612 - tn: 11561.0000 - auc: 0.7396 - prc: 0.3656 - val_loss: 0.4024 - val_Sensitivity: 0.0310 - val_tn: 3863.0000 - val_auc: 0.7290 - val_prc: 0.3586\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3931 - Sensitivity: 0.0612 - tn: 11568.0000 - auc: 0.7440 - prc: 0.3719 - val_loss: 0.4023 - val_Sensitivity: 0.0362 - val_tn: 3861.0000 - val_auc: 0.7289 - val_prc: 0.3582\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3948 - Sensitivity: 0.0656 - tn: 11570.0000 - auc: 0.7394 - prc: 0.3757 - val_loss: 0.4023 - val_Sensitivity: 0.0362 - val_tn: 3861.0000 - val_auc: 0.7291 - val_prc: 0.3583\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3955 - Sensitivity: 0.0669 - tn: 11566.0000 - auc: 0.7380 - prc: 0.3769 - val_loss: 0.4023 - val_Sensitivity: 0.0375 - val_tn: 3861.0000 - val_auc: 0.7291 - val_prc: 0.3581\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3936 - Sensitivity: 0.0568 - tn: 11564.0000 - auc: 0.7418 - prc: 0.3717 - val_loss: 0.4025 - val_Sensitivity: 0.0336 - val_tn: 3861.0000 - val_auc: 0.7288 - val_prc: 0.3577\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3953 - Sensitivity: 0.0533 - tn: 11573.0000 - auc: 0.7389 - prc: 0.3647 - val_loss: 0.4024 - val_Sensitivity: 0.0349 - val_tn: 3861.0000 - val_auc: 0.7292 - val_prc: 0.3578\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3943 - Sensitivity: 0.0577 - tn: 11560.0000 - auc: 0.7404 - prc: 0.3710 - val_loss: 0.4026 - val_Sensitivity: 0.0336 - val_tn: 3861.0000 - val_auc: 0.7292 - val_prc: 0.3577\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3962 - Sensitivity: 0.0629 - tn: 11561.0000 - auc: 0.7377 - prc: 0.3740 - val_loss: 0.4021 - val_Sensitivity: 0.0388 - val_tn: 3859.0000 - val_auc: 0.7294 - val_prc: 0.3576\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3965 - Sensitivity: 0.0608 - tn: 11565.0000 - auc: 0.7374 - prc: 0.3704 - val_loss: 0.4022 - val_Sensitivity: 0.0336 - val_tn: 3861.0000 - val_auc: 0.7295 - val_prc: 0.3574\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3940 - Sensitivity: 0.0542 - tn: 11562.0000 - auc: 0.7420 - prc: 0.3668 - val_loss: 0.4022 - val_Sensitivity: 0.0336 - val_tn: 3861.0000 - val_auc: 0.7295 - val_prc: 0.3569\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3958 - Sensitivity: 0.0603 - tn: 11560.0000 - auc: 0.7375 - prc: 0.3710 - val_loss: 0.4022 - val_Sensitivity: 0.0349 - val_tn: 3861.0000 - val_auc: 0.7295 - val_prc: 0.3570\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3944 - Sensitivity: 0.0599 - tn: 11556.0000 - auc: 0.7399 - prc: 0.3777 - val_loss: 0.4024 - val_Sensitivity: 0.0349 - val_tn: 3862.0000 - val_auc: 0.7293 - val_prc: 0.3571\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3928 - Sensitivity: 0.0625 - tn: 11563.0000 - auc: 0.7446 - prc: 0.3774 - val_loss: 0.4020 - val_Sensitivity: 0.0362 - val_tn: 3861.0000 - val_auc: 0.7297 - val_prc: 0.3575\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3937 - Sensitivity: 0.0590 - tn: 11564.0000 - auc: 0.7419 - prc: 0.3739 - val_loss: 0.4023 - val_Sensitivity: 0.0336 - val_tn: 3863.0000 - val_auc: 0.7296 - val_prc: 0.3576\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3946 - Sensitivity: 0.0520 - tn: 11573.0000 - auc: 0.7388 - prc: 0.3737 - val_loss: 0.4021 - val_Sensitivity: 0.0336 - val_tn: 3862.0000 - val_auc: 0.7297 - val_prc: 0.3578\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3928 - Sensitivity: 0.0616 - tn: 11570.0000 - auc: 0.7439 - prc: 0.3769 - val_loss: 0.4021 - val_Sensitivity: 0.0336 - val_tn: 3862.0000 - val_auc: 0.7299 - val_prc: 0.3580\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3945 - Sensitivity: 0.0642 - tn: 11575.0000 - auc: 0.7400 - prc: 0.3788 - val_loss: 0.4020 - val_Sensitivity: 0.0349 - val_tn: 3860.0000 - val_auc: 0.7298 - val_prc: 0.3577\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3936 - Sensitivity: 0.0682 - tn: 11564.0000 - auc: 0.7430 - prc: 0.3755 - val_loss: 0.4021 - val_Sensitivity: 0.0349 - val_tn: 3860.0000 - val_auc: 0.7298 - val_prc: 0.3576\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3917 - Sensitivity: 0.0651 - tn: 11579.0000 - auc: 0.7446 - prc: 0.3891 - val_loss: 0.4021 - val_Sensitivity: 0.0362 - val_tn: 3860.0000 - val_auc: 0.7295 - val_prc: 0.3577\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3956 - Sensitivity: 0.0642 - tn: 11559.0000 - auc: 0.7382 - prc: 0.3730 - val_loss: 0.4019 - val_Sensitivity: 0.0362 - val_tn: 3860.0000 - val_auc: 0.7300 - val_prc: 0.3583\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3936 - Sensitivity: 0.0673 - tn: 11554.0000 - auc: 0.7424 - prc: 0.3794 - val_loss: 0.4020 - val_Sensitivity: 0.0362 - val_tn: 3860.0000 - val_auc: 0.7300 - val_prc: 0.3587\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3922 - Sensitivity: 0.0638 - tn: 11559.0000 - auc: 0.7441 - prc: 0.3795 - val_loss: 0.4021 - val_Sensitivity: 0.0362 - val_tn: 3862.0000 - val_auc: 0.7300 - val_prc: 0.3591\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3928 - Sensitivity: 0.0629 - tn: 11562.0000 - auc: 0.7444 - prc: 0.3770 - val_loss: 0.4022 - val_Sensitivity: 0.0362 - val_tn: 3862.0000 - val_auc: 0.7300 - val_prc: 0.3584\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3930 - Sensitivity: 0.0673 - tn: 11565.0000 - auc: 0.7428 - prc: 0.3773 - val_loss: 0.4020 - val_Sensitivity: 0.0375 - val_tn: 3860.0000 - val_auc: 0.7300 - val_prc: 0.3589\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3927 - Sensitivity: 0.0612 - tn: 11546.0000 - auc: 0.7453 - prc: 0.3727 - val_loss: 0.4020 - val_Sensitivity: 0.0362 - val_tn: 3861.0000 - val_auc: 0.7301 - val_prc: 0.3586\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3934 - Sensitivity: 0.0594 - tn: 11557.0000 - auc: 0.7432 - prc: 0.3747 - val_loss: 0.4019 - val_Sensitivity: 0.0349 - val_tn: 3861.0000 - val_auc: 0.7303 - val_prc: 0.3589\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0708 - tn: 11573.0000 - auc: 0.7428 - prc: 0.3786 - val_loss: 0.4019 - val_Sensitivity: 0.0362 - val_tn: 3861.0000 - val_auc: 0.7302 - val_prc: 0.3585\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3914 - Sensitivity: 0.0603 - tn: 11564.0000 - auc: 0.7460 - prc: 0.3749 - val_loss: 0.4021 - val_Sensitivity: 0.0362 - val_tn: 3863.0000 - val_auc: 0.7300 - val_prc: 0.3586\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3913 - Sensitivity: 0.0625 - tn: 11564.0000 - auc: 0.7452 - prc: 0.3807 - val_loss: 0.4020 - val_Sensitivity: 0.0362 - val_tn: 3862.0000 - val_auc: 0.7305 - val_prc: 0.3586\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3934 - Sensitivity: 0.0568 - tn: 11558.0000 - auc: 0.7440 - prc: 0.3769 - val_loss: 0.4021 - val_Sensitivity: 0.0362 - val_tn: 3863.0000 - val_auc: 0.7303 - val_prc: 0.3589\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3915 - Sensitivity: 0.0634 - tn: 11571.0000 - auc: 0.7483 - prc: 0.3837 - val_loss: 0.4022 - val_Sensitivity: 0.0362 - val_tn: 3860.0000 - val_auc: 0.7303 - val_prc: 0.3589\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3917 - Sensitivity: 0.0638 - tn: 11559.0000 - auc: 0.7464 - prc: 0.3832 - val_loss: 0.4019 - val_Sensitivity: 0.0362 - val_tn: 3860.0000 - val_auc: 0.7306 - val_prc: 0.3590\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3927 - Sensitivity: 0.0629 - tn: 11556.0000 - auc: 0.7446 - prc: 0.3763 - val_loss: 0.4017 - val_Sensitivity: 0.0375 - val_tn: 3858.0000 - val_auc: 0.7304 - val_prc: 0.3586\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3900 - Sensitivity: 0.0634 - tn: 11546.0000 - auc: 0.7501 - prc: 0.3766 - val_loss: 0.4016 - val_Sensitivity: 0.0375 - val_tn: 3859.0000 - val_auc: 0.7305 - val_prc: 0.3593\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3929 - Sensitivity: 0.0704 - tn: 11568.0000 - auc: 0.7418 - prc: 0.3862 - val_loss: 0.4016 - val_Sensitivity: 0.0375 - val_tn: 3859.0000 - val_auc: 0.7301 - val_prc: 0.3589\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3910 - Sensitivity: 0.0730 - tn: 11557.0000 - auc: 0.7465 - prc: 0.3851 - val_loss: 0.4014 - val_Sensitivity: 0.0375 - val_tn: 3857.0000 - val_auc: 0.7309 - val_prc: 0.3597\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3928 - Sensitivity: 0.0660 - tn: 11559.0000 - auc: 0.7430 - prc: 0.3794 - val_loss: 0.4017 - val_Sensitivity: 0.0375 - val_tn: 3859.0000 - val_auc: 0.7311 - val_prc: 0.3594\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3911 - Sensitivity: 0.0673 - tn: 11558.0000 - auc: 0.7478 - prc: 0.3825 - val_loss: 0.4015 - val_Sensitivity: 0.0375 - val_tn: 3858.0000 - val_auc: 0.7314 - val_prc: 0.3598\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3932 - Sensitivity: 0.0642 - tn: 11554.0000 - auc: 0.7421 - prc: 0.3815 - val_loss: 0.4015 - val_Sensitivity: 0.0375 - val_tn: 3858.0000 - val_auc: 0.7310 - val_prc: 0.3593\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3925 - Sensitivity: 0.0660 - tn: 11556.0000 - auc: 0.7447 - prc: 0.3791 - val_loss: 0.4014 - val_Sensitivity: 0.0401 - val_tn: 3856.0000 - val_auc: 0.7312 - val_prc: 0.3598\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0677 - tn: 11544.0000 - auc: 0.7473 - prc: 0.3801 - val_loss: 0.4017 - val_Sensitivity: 0.0375 - val_tn: 3860.0000 - val_auc: 0.7309 - val_prc: 0.3592\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3898 - Sensitivity: 0.0651 - tn: 11566.0000 - auc: 0.7504 - prc: 0.3835 - val_loss: 0.4017 - val_Sensitivity: 0.0375 - val_tn: 3859.0000 - val_auc: 0.7312 - val_prc: 0.3594\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3917 - Sensitivity: 0.0599 - tn: 11555.0000 - auc: 0.7478 - prc: 0.3773 - val_loss: 0.4017 - val_Sensitivity: 0.0375 - val_tn: 3861.0000 - val_auc: 0.7314 - val_prc: 0.3601\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3920 - Sensitivity: 0.0599 - tn: 11549.0000 - auc: 0.7459 - prc: 0.3738 - val_loss: 0.4014 - val_Sensitivity: 0.0375 - val_tn: 3862.0000 - val_auc: 0.7313 - val_prc: 0.3603\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3931 - Sensitivity: 0.0656 - tn: 11555.0000 - auc: 0.7431 - prc: 0.3746 - val_loss: 0.4014 - val_Sensitivity: 0.0375 - val_tn: 3861.0000 - val_auc: 0.7316 - val_prc: 0.3613\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3893 - Sensitivity: 0.0695 - tn: 11551.0000 - auc: 0.7519 - prc: 0.3886 - val_loss: 0.4013 - val_Sensitivity: 0.0401 - val_tn: 3856.0000 - val_auc: 0.7315 - val_prc: 0.3605\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3898 - Sensitivity: 0.0638 - tn: 11565.0000 - auc: 0.7508 - prc: 0.3839 - val_loss: 0.4015 - val_Sensitivity: 0.0414 - val_tn: 3859.0000 - val_auc: 0.7314 - val_prc: 0.3607\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3889 - Sensitivity: 0.0642 - tn: 11580.0000 - auc: 0.7523 - prc: 0.3928 - val_loss: 0.4017 - val_Sensitivity: 0.0362 - val_tn: 3861.0000 - val_auc: 0.7311 - val_prc: 0.3603\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3901 - Sensitivity: 0.0612 - tn: 11569.0000 - auc: 0.7507 - prc: 0.3843 - val_loss: 0.4017 - val_Sensitivity: 0.0375 - val_tn: 3859.0000 - val_auc: 0.7310 - val_prc: 0.3603\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3874 - Sensitivity: 0.0734 - tn: 11549.0000 - auc: 0.7559 - prc: 0.3958 - val_loss: 0.4014 - val_Sensitivity: 0.0401 - val_tn: 3858.0000 - val_auc: 0.7314 - val_prc: 0.3602\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3939 - Sensitivity: 0.0660 - tn: 11555.0000 - auc: 0.7436 - prc: 0.3774 - val_loss: 0.4013 - val_Sensitivity: 0.0388 - val_tn: 3859.0000 - val_auc: 0.7314 - val_prc: 0.3606\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3902 - Sensitivity: 0.0708 - tn: 11560.0000 - auc: 0.7494 - prc: 0.3853 - val_loss: 0.4014 - val_Sensitivity: 0.0375 - val_tn: 3859.0000 - val_auc: 0.7317 - val_prc: 0.3610\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3871 - Sensitivity: 0.0669 - tn: 11565.0000 - auc: 0.7545 - prc: 0.3913 - val_loss: 0.4015 - val_Sensitivity: 0.0375 - val_tn: 3860.0000 - val_auc: 0.7319 - val_prc: 0.3607\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3898 - Sensitivity: 0.0616 - tn: 11559.0000 - auc: 0.7498 - prc: 0.3835 - val_loss: 0.4013 - val_Sensitivity: 0.0375 - val_tn: 3857.0000 - val_auc: 0.7319 - val_prc: 0.3606\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3898 - Sensitivity: 0.0682 - tn: 11554.0000 - auc: 0.7507 - prc: 0.3883 - val_loss: 0.4015 - val_Sensitivity: 0.0375 - val_tn: 3856.0000 - val_auc: 0.7315 - val_prc: 0.3604\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3902 - Sensitivity: 0.0677 - tn: 11568.0000 - auc: 0.7498 - prc: 0.3874 - val_loss: 0.4013 - val_Sensitivity: 0.0401 - val_tn: 3856.0000 - val_auc: 0.7319 - val_prc: 0.3602\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3905 - Sensitivity: 0.0686 - tn: 11556.0000 - auc: 0.7484 - prc: 0.3858 - val_loss: 0.4011 - val_Sensitivity: 0.0427 - val_tn: 3855.0000 - val_auc: 0.7318 - val_prc: 0.3599\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3908 - Sensitivity: 0.0664 - tn: 11550.0000 - auc: 0.7494 - prc: 0.3817 - val_loss: 0.4012 - val_Sensitivity: 0.0401 - val_tn: 3855.0000 - val_auc: 0.7319 - val_prc: 0.3601\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3870 - Sensitivity: 0.0791 - tn: 11535.0000 - auc: 0.7550 - prc: 0.3926 - val_loss: 0.4010 - val_Sensitivity: 0.0453 - val_tn: 3850.0000 - val_auc: 0.7321 - val_prc: 0.3606\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3908 - Sensitivity: 0.0712 - tn: 11558.0000 - auc: 0.7477 - prc: 0.3889 - val_loss: 0.4014 - val_Sensitivity: 0.0427 - val_tn: 3856.0000 - val_auc: 0.7320 - val_prc: 0.3608\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3891 - Sensitivity: 0.0734 - tn: 11539.0000 - auc: 0.7511 - prc: 0.3852 - val_loss: 0.4011 - val_Sensitivity: 0.0427 - val_tn: 3854.0000 - val_auc: 0.7322 - val_prc: 0.3613\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3883 - Sensitivity: 0.0642 - tn: 11556.0000 - auc: 0.7524 - prc: 0.3923 - val_loss: 0.4009 - val_Sensitivity: 0.0401 - val_tn: 3858.0000 - val_auc: 0.7327 - val_prc: 0.3625\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3861 - Sensitivity: 0.0708 - tn: 11556.0000 - auc: 0.7576 - prc: 0.3940 - val_loss: 0.4009 - val_Sensitivity: 0.0414 - val_tn: 3856.0000 - val_auc: 0.7329 - val_prc: 0.3637\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3909 - Sensitivity: 0.0752 - tn: 11554.0000 - auc: 0.7465 - prc: 0.3837 - val_loss: 0.4007 - val_Sensitivity: 0.0427 - val_tn: 3854.0000 - val_auc: 0.7328 - val_prc: 0.3632\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3895 - Sensitivity: 0.0686 - tn: 11543.0000 - auc: 0.7492 - prc: 0.3853 - val_loss: 0.4009 - val_Sensitivity: 0.0427 - val_tn: 3854.0000 - val_auc: 0.7330 - val_prc: 0.3633\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3887 - Sensitivity: 0.0782 - tn: 11549.0000 - auc: 0.7529 - prc: 0.3930 - val_loss: 0.4009 - val_Sensitivity: 0.0427 - val_tn: 3853.0000 - val_auc: 0.7324 - val_prc: 0.3630\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3880 - Sensitivity: 0.0695 - tn: 11563.0000 - auc: 0.7542 - prc: 0.3880 - val_loss: 0.4011 - val_Sensitivity: 0.0414 - val_tn: 3857.0000 - val_auc: 0.7327 - val_prc: 0.3639\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3881 - Sensitivity: 0.0682 - tn: 11556.0000 - auc: 0.7548 - prc: 0.3917 - val_loss: 0.4011 - val_Sensitivity: 0.0414 - val_tn: 3856.0000 - val_auc: 0.7327 - val_prc: 0.3640\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3867 - Sensitivity: 0.0712 - tn: 11566.0000 - auc: 0.7566 - prc: 0.3944 - val_loss: 0.4010 - val_Sensitivity: 0.0414 - val_tn: 3853.0000 - val_auc: 0.7325 - val_prc: 0.3634\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3887 - Sensitivity: 0.0782 - tn: 11543.0000 - auc: 0.7524 - prc: 0.3969 - val_loss: 0.4010 - val_Sensitivity: 0.0427 - val_tn: 3852.0000 - val_auc: 0.7323 - val_prc: 0.3630\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3877 - Sensitivity: 0.0752 - tn: 11539.0000 - auc: 0.7555 - prc: 0.3880 - val_loss: 0.4014 - val_Sensitivity: 0.0414 - val_tn: 3855.0000 - val_auc: 0.7324 - val_prc: 0.3635\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3882 - Sensitivity: 0.0708 - tn: 11549.0000 - auc: 0.7545 - prc: 0.3905 - val_loss: 0.4014 - val_Sensitivity: 0.0414 - val_tn: 3856.0000 - val_auc: 0.7320 - val_prc: 0.3628\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3891 - Sensitivity: 0.0844 - tn: 11545.0000 - auc: 0.7504 - prc: 0.3901 - val_loss: 0.4014 - val_Sensitivity: 0.0414 - val_tn: 3856.0000 - val_auc: 0.7319 - val_prc: 0.3630\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3871 - Sensitivity: 0.0699 - tn: 11549.0000 - auc: 0.7562 - prc: 0.3940 - val_loss: 0.4016 - val_Sensitivity: 0.0388 - val_tn: 3860.0000 - val_auc: 0.7322 - val_prc: 0.3635\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3872 - Sensitivity: 0.0826 - tn: 11552.0000 - auc: 0.7534 - prc: 0.3969 - val_loss: 0.4014 - val_Sensitivity: 0.0427 - val_tn: 3856.0000 - val_auc: 0.7319 - val_prc: 0.3629\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3848 - Sensitivity: 0.0813 - tn: 11547.0000 - auc: 0.7602 - prc: 0.3980 - val_loss: 0.4015 - val_Sensitivity: 0.0414 - val_tn: 3855.0000 - val_auc: 0.7322 - val_prc: 0.3630\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3874 - Sensitivity: 0.0782 - tn: 11546.0000 - auc: 0.7544 - prc: 0.3920 - val_loss: 0.4013 - val_Sensitivity: 0.0440 - val_tn: 3854.0000 - val_auc: 0.7322 - val_prc: 0.3628\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3833 - Sensitivity: 0.0817 - tn: 11550.0000 - auc: 0.7625 - prc: 0.4006 - val_loss: 0.4016 - val_Sensitivity: 0.0414 - val_tn: 3855.0000 - val_auc: 0.7320 - val_prc: 0.3627\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3870 - Sensitivity: 0.0787 - tn: 11527.0000 - auc: 0.7553 - prc: 0.3914 - val_loss: 0.4016 - val_Sensitivity: 0.0414 - val_tn: 3855.0000 - val_auc: 0.7320 - val_prc: 0.3626\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3880 - Sensitivity: 0.0765 - tn: 11544.0000 - auc: 0.7528 - prc: 0.3895 - val_loss: 0.4016 - val_Sensitivity: 0.0414 - val_tn: 3855.0000 - val_auc: 0.7320 - val_prc: 0.3629\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3880 - Sensitivity: 0.0756 - tn: 11555.0000 - auc: 0.7545 - prc: 0.3875 - val_loss: 0.4017 - val_Sensitivity: 0.0414 - val_tn: 3856.0000 - val_auc: 0.7318 - val_prc: 0.3629\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3878 - Sensitivity: 0.0782 - tn: 11542.0000 - auc: 0.7536 - prc: 0.3936 - val_loss: 0.4015 - val_Sensitivity: 0.0427 - val_tn: 3855.0000 - val_auc: 0.7321 - val_prc: 0.3629\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3837 - Sensitivity: 0.0760 - tn: 11546.0000 - auc: 0.7613 - prc: 0.4002 - val_loss: 0.4015 - val_Sensitivity: 0.0427 - val_tn: 3855.0000 - val_auc: 0.7319 - val_prc: 0.3628\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3851 - Sensitivity: 0.0791 - tn: 11541.0000 - auc: 0.7600 - prc: 0.4016 - val_loss: 0.4017 - val_Sensitivity: 0.0427 - val_tn: 3855.0000 - val_auc: 0.7319 - val_prc: 0.3629\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3862 - Sensitivity: 0.0870 - tn: 11552.0000 - auc: 0.7561 - prc: 0.4039 - val_loss: 0.4014 - val_Sensitivity: 0.0440 - val_tn: 3850.0000 - val_auc: 0.7319 - val_prc: 0.3631\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3869 - Sensitivity: 0.0813 - tn: 11538.0000 - auc: 0.7548 - prc: 0.3962 - val_loss: 0.4016 - val_Sensitivity: 0.0453 - val_tn: 3851.0000 - val_auc: 0.7321 - val_prc: 0.3627\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3882 - Sensitivity: 0.0848 - tn: 11522.0000 - auc: 0.7521 - prc: 0.3887 - val_loss: 0.4012 - val_Sensitivity: 0.0440 - val_tn: 3852.0000 - val_auc: 0.7329 - val_prc: 0.3639\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3857 - Sensitivity: 0.0822 - tn: 11539.0000 - auc: 0.7579 - prc: 0.3987 - val_loss: 0.4013 - val_Sensitivity: 0.0440 - val_tn: 3852.0000 - val_auc: 0.7328 - val_prc: 0.3637\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3853 - Sensitivity: 0.0887 - tn: 11540.0000 - auc: 0.7572 - prc: 0.4061 - val_loss: 0.4012 - val_Sensitivity: 0.0440 - val_tn: 3852.0000 - val_auc: 0.7324 - val_prc: 0.3635\n",
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 27ms/step - loss: 1.0211 - Sensitivity: 0.4872 - tn: 6059.0000 - auc: 0.5043 - prc: 0.1652 - val_loss: 0.4619 - val_Sensitivity: 0.0000e+00 - val_tn: 3852.0000 - val_auc: 0.5694 - val_prc: 0.2029\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.8189 - Sensitivity: 0.3838 - tn: 7645.0000 - auc: 0.5269 - prc: 0.1807 - val_loss: 0.4450 - val_Sensitivity: 0.0000e+00 - val_tn: 3859.0000 - val_auc: 0.6235 - val_prc: 0.2385\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7039 - Sensitivity: 0.3037 - tn: 8857.0000 - auc: 0.5463 - prc: 0.1890 - val_loss: 0.4370 - val_Sensitivity: 0.0013 - val_tn: 3859.0000 - val_auc: 0.6515 - val_prc: 0.2606\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.6579 - Sensitivity: 0.2438 - tn: 9460.0000 - auc: 0.5536 - prc: 0.1913 - val_loss: 0.4321 - val_Sensitivity: 0.0013 - val_tn: 3859.0000 - val_auc: 0.6696 - val_prc: 0.2752\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.6090 - Sensitivity: 0.2201 - tn: 9898.0000 - auc: 0.5723 - prc: 0.2051 - val_loss: 0.4301 - val_Sensitivity: 0.0013 - val_tn: 3859.0000 - val_auc: 0.6816 - val_prc: 0.2868\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5983 - Sensitivity: 0.2069 - tn: 10210.0000 - auc: 0.5749 - prc: 0.2111 - val_loss: 0.4288 - val_Sensitivity: 0.0013 - val_tn: 3859.0000 - val_auc: 0.6902 - val_prc: 0.2967\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5812 - Sensitivity: 0.1796 - tn: 10362.0000 - auc: 0.5799 - prc: 0.2105 - val_loss: 0.4269 - val_Sensitivity: 0.0013 - val_tn: 3858.0000 - val_auc: 0.6963 - val_prc: 0.3067\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5708 - Sensitivity: 0.1765 - tn: 10490.0000 - auc: 0.5894 - prc: 0.2163 - val_loss: 0.4256 - val_Sensitivity: 0.0025 - val_tn: 3856.0000 - val_auc: 0.7006 - val_prc: 0.3154\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5588 - Sensitivity: 0.1739 - tn: 10532.0000 - auc: 0.5988 - prc: 0.2176 - val_loss: 0.4243 - val_Sensitivity: 0.0051 - val_tn: 3855.0000 - val_auc: 0.7043 - val_prc: 0.3220\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5527 - Sensitivity: 0.1783 - tn: 10575.0000 - auc: 0.6022 - prc: 0.2281 - val_loss: 0.4236 - val_Sensitivity: 0.0114 - val_tn: 3850.0000 - val_auc: 0.7074 - val_prc: 0.3273\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5293 - Sensitivity: 0.1849 - tn: 10679.0000 - auc: 0.6232 - prc: 0.2439 - val_loss: 0.4232 - val_Sensitivity: 0.0127 - val_tn: 3852.0000 - val_auc: 0.7096 - val_prc: 0.3327\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5397 - Sensitivity: 0.1580 - tn: 10690.0000 - auc: 0.6050 - prc: 0.2317 - val_loss: 0.4224 - val_Sensitivity: 0.0152 - val_tn: 3851.0000 - val_auc: 0.7110 - val_prc: 0.3358\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5184 - Sensitivity: 0.1708 - tn: 10774.0000 - auc: 0.6239 - prc: 0.2434 - val_loss: 0.4220 - val_Sensitivity: 0.0190 - val_tn: 3849.0000 - val_auc: 0.7126 - val_prc: 0.3382\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5184 - Sensitivity: 0.1549 - tn: 10806.0000 - auc: 0.6240 - prc: 0.2409 - val_loss: 0.4208 - val_Sensitivity: 0.0203 - val_tn: 3846.0000 - val_auc: 0.7144 - val_prc: 0.3397\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5035 - Sensitivity: 0.1607 - tn: 10854.0000 - auc: 0.6298 - prc: 0.2543 - val_loss: 0.4191 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7161 - val_prc: 0.3418\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4997 - Sensitivity: 0.1545 - tn: 10892.0000 - auc: 0.6306 - prc: 0.2550 - val_loss: 0.4181 - val_Sensitivity: 0.0266 - val_tn: 3844.0000 - val_auc: 0.7170 - val_prc: 0.3437\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5016 - Sensitivity: 0.1554 - tn: 10882.0000 - auc: 0.6300 - prc: 0.2494 - val_loss: 0.4184 - val_Sensitivity: 0.0241 - val_tn: 3843.0000 - val_auc: 0.7178 - val_prc: 0.3465\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4880 - Sensitivity: 0.1514 - tn: 10961.0000 - auc: 0.6446 - prc: 0.2566 - val_loss: 0.4176 - val_Sensitivity: 0.0228 - val_tn: 3842.0000 - val_auc: 0.7189 - val_prc: 0.3489\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4887 - Sensitivity: 0.1408 - tn: 10977.0000 - auc: 0.6381 - prc: 0.2472 - val_loss: 0.4165 - val_Sensitivity: 0.0266 - val_tn: 3841.0000 - val_auc: 0.7198 - val_prc: 0.3503\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4838 - Sensitivity: 0.1444 - tn: 11022.0000 - auc: 0.6357 - prc: 0.2593 - val_loss: 0.4159 - val_Sensitivity: 0.0292 - val_tn: 3841.0000 - val_auc: 0.7206 - val_prc: 0.3517\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4756 - Sensitivity: 0.1448 - tn: 11076.0000 - auc: 0.6429 - prc: 0.2688 - val_loss: 0.4155 - val_Sensitivity: 0.0279 - val_tn: 3841.0000 - val_auc: 0.7208 - val_prc: 0.3518\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4645 - Sensitivity: 0.1439 - tn: 11026.0000 - auc: 0.6572 - prc: 0.2750 - val_loss: 0.4155 - val_Sensitivity: 0.0266 - val_tn: 3842.0000 - val_auc: 0.7209 - val_prc: 0.3516\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4647 - Sensitivity: 0.1281 - tn: 11105.0000 - auc: 0.6595 - prc: 0.2652 - val_loss: 0.4152 - val_Sensitivity: 0.0241 - val_tn: 3842.0000 - val_auc: 0.7214 - val_prc: 0.3517\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4614 - Sensitivity: 0.1193 - tn: 11143.0000 - auc: 0.6567 - prc: 0.2694 - val_loss: 0.4139 - val_Sensitivity: 0.0305 - val_tn: 3841.0000 - val_auc: 0.7229 - val_prc: 0.3534\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4522 - Sensitivity: 0.1444 - tn: 11135.0000 - auc: 0.6670 - prc: 0.2872 - val_loss: 0.4138 - val_Sensitivity: 0.0279 - val_tn: 3841.0000 - val_auc: 0.7230 - val_prc: 0.3531\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4518 - Sensitivity: 0.1294 - tn: 11209.0000 - auc: 0.6658 - prc: 0.2866 - val_loss: 0.4132 - val_Sensitivity: 0.0279 - val_tn: 3841.0000 - val_auc: 0.7239 - val_prc: 0.3542\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4531 - Sensitivity: 0.1303 - tn: 11214.0000 - auc: 0.6621 - prc: 0.2814 - val_loss: 0.4123 - val_Sensitivity: 0.0279 - val_tn: 3840.0000 - val_auc: 0.7244 - val_prc: 0.3548\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4489 - Sensitivity: 0.1153 - tn: 11226.0000 - auc: 0.6615 - prc: 0.2820 - val_loss: 0.4113 - val_Sensitivity: 0.0266 - val_tn: 3840.0000 - val_auc: 0.7252 - val_prc: 0.3561\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4447 - Sensitivity: 0.1276 - tn: 11251.0000 - auc: 0.6736 - prc: 0.2927 - val_loss: 0.4114 - val_Sensitivity: 0.0266 - val_tn: 3840.0000 - val_auc: 0.7256 - val_prc: 0.3563\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4417 - Sensitivity: 0.1122 - tn: 11268.0000 - auc: 0.6720 - prc: 0.2859 - val_loss: 0.4113 - val_Sensitivity: 0.0241 - val_tn: 3841.0000 - val_auc: 0.7263 - val_prc: 0.3568\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4391 - Sensitivity: 0.0995 - tn: 11326.0000 - auc: 0.6720 - prc: 0.2861 - val_loss: 0.4110 - val_Sensitivity: 0.0254 - val_tn: 3842.0000 - val_auc: 0.7263 - val_prc: 0.3575\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4364 - Sensitivity: 0.1083 - tn: 11326.0000 - auc: 0.6768 - prc: 0.2983 - val_loss: 0.4110 - val_Sensitivity: 0.0254 - val_tn: 3844.0000 - val_auc: 0.7260 - val_prc: 0.3571\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4383 - Sensitivity: 0.0990 - tn: 11365.0000 - auc: 0.6704 - prc: 0.2901 - val_loss: 0.4102 - val_Sensitivity: 0.0266 - val_tn: 3841.0000 - val_auc: 0.7268 - val_prc: 0.3583\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4327 - Sensitivity: 0.1074 - tn: 11359.0000 - auc: 0.6764 - prc: 0.3010 - val_loss: 0.4096 - val_Sensitivity: 0.0266 - val_tn: 3839.0000 - val_auc: 0.7274 - val_prc: 0.3589\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4314 - Sensitivity: 0.0964 - tn: 11365.0000 - auc: 0.6810 - prc: 0.2997 - val_loss: 0.4098 - val_Sensitivity: 0.0254 - val_tn: 3841.0000 - val_auc: 0.7270 - val_prc: 0.3589\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4288 - Sensitivity: 0.0990 - tn: 11382.0000 - auc: 0.6846 - prc: 0.2995 - val_loss: 0.4097 - val_Sensitivity: 0.0254 - val_tn: 3842.0000 - val_auc: 0.7274 - val_prc: 0.3595\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4281 - Sensitivity: 0.0964 - tn: 11395.0000 - auc: 0.6822 - prc: 0.3009 - val_loss: 0.4094 - val_Sensitivity: 0.0254 - val_tn: 3842.0000 - val_auc: 0.7278 - val_prc: 0.3599\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4209 - Sensitivity: 0.0920 - tn: 11429.0000 - auc: 0.6928 - prc: 0.3163 - val_loss: 0.4088 - val_Sensitivity: 0.0241 - val_tn: 3841.0000 - val_auc: 0.7287 - val_prc: 0.3610\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.4258 - Sensitivity: 0.0893 - tn: 11434.0000 - auc: 0.6859 - prc: 0.3093 - val_loss: 0.4082 - val_Sensitivity: 0.0254 - val_tn: 3841.0000 - val_auc: 0.7293 - val_prc: 0.3613\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4247 - Sensitivity: 0.0832 - tn: 11446.0000 - auc: 0.6843 - prc: 0.3044 - val_loss: 0.4084 - val_Sensitivity: 0.0241 - val_tn: 3842.0000 - val_auc: 0.7297 - val_prc: 0.3609\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4221 - Sensitivity: 0.0876 - tn: 11453.0000 - auc: 0.6889 - prc: 0.3166 - val_loss: 0.4084 - val_Sensitivity: 0.0241 - val_tn: 3846.0000 - val_auc: 0.7295 - val_prc: 0.3606\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4204 - Sensitivity: 0.0832 - tn: 11478.0000 - auc: 0.6919 - prc: 0.3181 - val_loss: 0.4078 - val_Sensitivity: 0.0241 - val_tn: 3844.0000 - val_auc: 0.7304 - val_prc: 0.3611\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4223 - Sensitivity: 0.0775 - tn: 11504.0000 - auc: 0.6891 - prc: 0.3152 - val_loss: 0.4078 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7306 - val_prc: 0.3615\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4187 - Sensitivity: 0.0726 - tn: 11495.0000 - auc: 0.6921 - prc: 0.3144 - val_loss: 0.4071 - val_Sensitivity: 0.0228 - val_tn: 3844.0000 - val_auc: 0.7313 - val_prc: 0.3619\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4146 - Sensitivity: 0.0871 - tn: 11520.0000 - auc: 0.7006 - prc: 0.3401 - val_loss: 0.4069 - val_Sensitivity: 0.0241 - val_tn: 3843.0000 - val_auc: 0.7317 - val_prc: 0.3628\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4151 - Sensitivity: 0.0889 - tn: 11480.0000 - auc: 0.6984 - prc: 0.3266 - val_loss: 0.4069 - val_Sensitivity: 0.0241 - val_tn: 3845.0000 - val_auc: 0.7317 - val_prc: 0.3628\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4155 - Sensitivity: 0.0779 - tn: 11503.0000 - auc: 0.6975 - prc: 0.3251 - val_loss: 0.4072 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7319 - val_prc: 0.3628\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4154 - Sensitivity: 0.0757 - tn: 11519.0000 - auc: 0.6977 - prc: 0.3220 - val_loss: 0.4068 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7325 - val_prc: 0.3634\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4121 - Sensitivity: 0.0739 - tn: 11532.0000 - auc: 0.7036 - prc: 0.3350 - val_loss: 0.4066 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7326 - val_prc: 0.3640\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4116 - Sensitivity: 0.0735 - tn: 11540.0000 - auc: 0.7036 - prc: 0.3337 - val_loss: 0.4066 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7329 - val_prc: 0.3642\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4113 - Sensitivity: 0.0665 - tn: 11534.0000 - auc: 0.7040 - prc: 0.3302 - val_loss: 0.4061 - val_Sensitivity: 0.0254 - val_tn: 3844.0000 - val_auc: 0.7332 - val_prc: 0.3646\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4084 - Sensitivity: 0.0849 - tn: 11541.0000 - auc: 0.7104 - prc: 0.3441 - val_loss: 0.4063 - val_Sensitivity: 0.0228 - val_tn: 3845.0000 - val_auc: 0.7335 - val_prc: 0.3645\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4129 - Sensitivity: 0.0634 - tn: 11539.0000 - auc: 0.7016 - prc: 0.3218 - val_loss: 0.4065 - val_Sensitivity: 0.0216 - val_tn: 3846.0000 - val_auc: 0.7335 - val_prc: 0.3645\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4138 - Sensitivity: 0.0643 - tn: 11559.0000 - auc: 0.6959 - prc: 0.3272 - val_loss: 0.4062 - val_Sensitivity: 0.0228 - val_tn: 3846.0000 - val_auc: 0.7337 - val_prc: 0.3649\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4106 - Sensitivity: 0.0695 - tn: 11564.0000 - auc: 0.7041 - prc: 0.3389 - val_loss: 0.4057 - val_Sensitivity: 0.0254 - val_tn: 3843.0000 - val_auc: 0.7344 - val_prc: 0.3655\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4080 - Sensitivity: 0.0585 - tn: 11554.0000 - auc: 0.7112 - prc: 0.3320 - val_loss: 0.4060 - val_Sensitivity: 0.0228 - val_tn: 3845.0000 - val_auc: 0.7343 - val_prc: 0.3657\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4103 - Sensitivity: 0.0607 - tn: 11575.0000 - auc: 0.7062 - prc: 0.3317 - val_loss: 0.4058 - val_Sensitivity: 0.0241 - val_tn: 3845.0000 - val_auc: 0.7343 - val_prc: 0.3656\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4087 - Sensitivity: 0.0634 - tn: 11563.0000 - auc: 0.7075 - prc: 0.3376 - val_loss: 0.4058 - val_Sensitivity: 0.0241 - val_tn: 3844.0000 - val_auc: 0.7346 - val_prc: 0.3660\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4082 - Sensitivity: 0.0665 - tn: 11541.0000 - auc: 0.7106 - prc: 0.3360 - val_loss: 0.4054 - val_Sensitivity: 0.0266 - val_tn: 3842.0000 - val_auc: 0.7346 - val_prc: 0.3661\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4077 - Sensitivity: 0.0656 - tn: 11552.0000 - auc: 0.7085 - prc: 0.3368 - val_loss: 0.4052 - val_Sensitivity: 0.0279 - val_tn: 3842.0000 - val_auc: 0.7350 - val_prc: 0.3666\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4069 - Sensitivity: 0.0700 - tn: 11566.0000 - auc: 0.7108 - prc: 0.3468 - val_loss: 0.4051 - val_Sensitivity: 0.0305 - val_tn: 3842.0000 - val_auc: 0.7353 - val_prc: 0.3670\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4057 - Sensitivity: 0.0691 - tn: 11549.0000 - auc: 0.7125 - prc: 0.3455 - val_loss: 0.4049 - val_Sensitivity: 0.0279 - val_tn: 3841.0000 - val_auc: 0.7356 - val_prc: 0.3670\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4057 - Sensitivity: 0.0669 - tn: 11562.0000 - auc: 0.7134 - prc: 0.3431 - val_loss: 0.4050 - val_Sensitivity: 0.0279 - val_tn: 3842.0000 - val_auc: 0.7358 - val_prc: 0.3673\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4046 - Sensitivity: 0.0647 - tn: 11581.0000 - auc: 0.7149 - prc: 0.3459 - val_loss: 0.4051 - val_Sensitivity: 0.0266 - val_tn: 3842.0000 - val_auc: 0.7360 - val_prc: 0.3672\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.4061 - Sensitivity: 0.0647 - tn: 11577.0000 - auc: 0.7130 - prc: 0.3380 - val_loss: 0.4049 - val_Sensitivity: 0.0254 - val_tn: 3842.0000 - val_auc: 0.7358 - val_prc: 0.3672\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4033 - Sensitivity: 0.0581 - tn: 11584.0000 - auc: 0.7168 - prc: 0.3497 - val_loss: 0.4048 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7361 - val_prc: 0.3676\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4051 - Sensitivity: 0.0599 - tn: 11588.0000 - auc: 0.7148 - prc: 0.3492 - val_loss: 0.4047 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7361 - val_prc: 0.3678\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4025 - Sensitivity: 0.0691 - tn: 11580.0000 - auc: 0.7189 - prc: 0.3571 - val_loss: 0.4049 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7360 - val_prc: 0.3671\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4049 - Sensitivity: 0.0581 - tn: 11570.0000 - auc: 0.7153 - prc: 0.3453 - val_loss: 0.4048 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7365 - val_prc: 0.3680\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4034 - Sensitivity: 0.0669 - tn: 11576.0000 - auc: 0.7180 - prc: 0.3522 - val_loss: 0.4046 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7363 - val_prc: 0.3675\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4016 - Sensitivity: 0.0625 - tn: 11587.0000 - auc: 0.7214 - prc: 0.3557 - val_loss: 0.4044 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7369 - val_prc: 0.3683\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4054 - Sensitivity: 0.0660 - tn: 11570.0000 - auc: 0.7138 - prc: 0.3510 - val_loss: 0.4043 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7368 - val_prc: 0.3680\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4038 - Sensitivity: 0.0682 - tn: 11571.0000 - auc: 0.7162 - prc: 0.3487 - val_loss: 0.4044 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7371 - val_prc: 0.3681\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4065 - Sensitivity: 0.0660 - tn: 11573.0000 - auc: 0.7124 - prc: 0.3468 - val_loss: 0.4043 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7371 - val_prc: 0.3684\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4030 - Sensitivity: 0.0643 - tn: 11566.0000 - auc: 0.7211 - prc: 0.3539 - val_loss: 0.4043 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7369 - val_prc: 0.3684\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.4026 - Sensitivity: 0.0647 - tn: 11595.0000 - auc: 0.7177 - prc: 0.3567 - val_loss: 0.4042 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7370 - val_prc: 0.3686\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.4019 - Sensitivity: 0.0651 - tn: 11586.0000 - auc: 0.7199 - prc: 0.3601 - val_loss: 0.4042 - val_Sensitivity: 0.0305 - val_tn: 3842.0000 - val_auc: 0.7372 - val_prc: 0.3690\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 0.3993 - Sensitivity: 0.0634 - tn: 11576.0000 - auc: 0.7261 - prc: 0.3593 - val_loss: 0.4044 - val_Sensitivity: 0.0305 - val_tn: 3842.0000 - val_auc: 0.7371 - val_prc: 0.3688\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.3984 - Sensitivity: 0.0673 - tn: 11578.0000 - auc: 0.7271 - prc: 0.3618 - val_loss: 0.4043 - val_Sensitivity: 0.0317 - val_tn: 3842.0000 - val_auc: 0.7372 - val_prc: 0.3690\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.3996 - Sensitivity: 0.0665 - tn: 11566.0000 - auc: 0.7269 - prc: 0.3634 - val_loss: 0.4041 - val_Sensitivity: 0.0330 - val_tn: 3842.0000 - val_auc: 0.7372 - val_prc: 0.3693\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3991 - Sensitivity: 0.0673 - tn: 11584.0000 - auc: 0.7264 - prc: 0.3644 - val_loss: 0.4043 - val_Sensitivity: 0.0292 - val_tn: 3842.0000 - val_auc: 0.7370 - val_prc: 0.3698\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4031 - Sensitivity: 0.0656 - tn: 11564.0000 - auc: 0.7181 - prc: 0.3507 - val_loss: 0.4038 - val_Sensitivity: 0.0330 - val_tn: 3840.0000 - val_auc: 0.7375 - val_prc: 0.3702\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4003 - Sensitivity: 0.0665 - tn: 11562.0000 - auc: 0.7265 - prc: 0.3554 - val_loss: 0.4039 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7374 - val_prc: 0.3704\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.4019 - Sensitivity: 0.0704 - tn: 11567.0000 - auc: 0.7215 - prc: 0.3583 - val_loss: 0.4039 - val_Sensitivity: 0.0330 - val_tn: 3842.0000 - val_auc: 0.7375 - val_prc: 0.3713\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3992 - Sensitivity: 0.0682 - tn: 11572.0000 - auc: 0.7270 - prc: 0.3593 - val_loss: 0.4038 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7373 - val_prc: 0.3707\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.3961 - Sensitivity: 0.0629 - tn: 11561.0000 - auc: 0.7357 - prc: 0.3640 - val_loss: 0.4036 - val_Sensitivity: 0.0355 - val_tn: 3839.0000 - val_auc: 0.7382 - val_prc: 0.3709\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3968 - Sensitivity: 0.0744 - tn: 11562.0000 - auc: 0.7324 - prc: 0.3675 - val_loss: 0.4038 - val_Sensitivity: 0.0317 - val_tn: 3842.0000 - val_auc: 0.7378 - val_prc: 0.3711\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.4010 - Sensitivity: 0.0691 - tn: 11581.0000 - auc: 0.7240 - prc: 0.3642 - val_loss: 0.4037 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7376 - val_prc: 0.3706\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4001 - Sensitivity: 0.0603 - tn: 11577.0000 - auc: 0.7245 - prc: 0.3576 - val_loss: 0.4035 - val_Sensitivity: 0.0368 - val_tn: 3841.0000 - val_auc: 0.7380 - val_prc: 0.3710\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3972 - Sensitivity: 0.0673 - tn: 11580.0000 - auc: 0.7320 - prc: 0.3713 - val_loss: 0.4036 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7377 - val_prc: 0.3703\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3966 - Sensitivity: 0.0673 - tn: 11565.0000 - auc: 0.7326 - prc: 0.3699 - val_loss: 0.4037 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7375 - val_prc: 0.3700\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3969 - Sensitivity: 0.0726 - tn: 11573.0000 - auc: 0.7305 - prc: 0.3686 - val_loss: 0.4037 - val_Sensitivity: 0.0330 - val_tn: 3842.0000 - val_auc: 0.7381 - val_prc: 0.3704\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3985 - Sensitivity: 0.0660 - tn: 11573.0000 - auc: 0.7272 - prc: 0.3659 - val_loss: 0.4038 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7377 - val_prc: 0.3699\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3988 - Sensitivity: 0.0678 - tn: 11575.0000 - auc: 0.7266 - prc: 0.3603 - val_loss: 0.4036 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7378 - val_prc: 0.3702\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3966 - Sensitivity: 0.0647 - tn: 11569.0000 - auc: 0.7323 - prc: 0.3671 - val_loss: 0.4036 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7382 - val_prc: 0.3701\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3957 - Sensitivity: 0.0678 - tn: 11582.0000 - auc: 0.7351 - prc: 0.3799 - val_loss: 0.4036 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7385 - val_prc: 0.3702\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3993 - Sensitivity: 0.0731 - tn: 11574.0000 - auc: 0.7256 - prc: 0.3622 - val_loss: 0.4036 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7383 - val_prc: 0.3705\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3985 - Sensitivity: 0.0656 - tn: 11568.0000 - auc: 0.7298 - prc: 0.3613 - val_loss: 0.4036 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7384 - val_prc: 0.3709\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3958 - Sensitivity: 0.0678 - tn: 11583.0000 - auc: 0.7324 - prc: 0.3744 - val_loss: 0.4036 - val_Sensitivity: 0.0317 - val_tn: 3841.0000 - val_auc: 0.7383 - val_prc: 0.3707\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3948 - Sensitivity: 0.0616 - tn: 11579.0000 - auc: 0.7373 - prc: 0.3730 - val_loss: 0.4035 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7385 - val_prc: 0.3706\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3958 - Sensitivity: 0.0669 - tn: 11590.0000 - auc: 0.7347 - prc: 0.3680 - val_loss: 0.4036 - val_Sensitivity: 0.0317 - val_tn: 3841.0000 - val_auc: 0.7387 - val_prc: 0.3707\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3968 - Sensitivity: 0.0656 - tn: 11581.0000 - auc: 0.7310 - prc: 0.3730 - val_loss: 0.4035 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7390 - val_prc: 0.3710\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3966 - Sensitivity: 0.0691 - tn: 11573.0000 - auc: 0.7318 - prc: 0.3658 - val_loss: 0.4036 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7389 - val_prc: 0.3710\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3962 - Sensitivity: 0.0643 - tn: 11564.0000 - auc: 0.7331 - prc: 0.3635 - val_loss: 0.4036 - val_Sensitivity: 0.0355 - val_tn: 3841.0000 - val_auc: 0.7384 - val_prc: 0.3701\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3962 - Sensitivity: 0.0779 - tn: 11566.0000 - auc: 0.7313 - prc: 0.3683 - val_loss: 0.4033 - val_Sensitivity: 0.0355 - val_tn: 3840.0000 - val_auc: 0.7388 - val_prc: 0.3709\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3925 - Sensitivity: 0.0687 - tn: 11569.0000 - auc: 0.7419 - prc: 0.3833 - val_loss: 0.4036 - val_Sensitivity: 0.0343 - val_tn: 3841.0000 - val_auc: 0.7388 - val_prc: 0.3710\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3956 - Sensitivity: 0.0647 - tn: 11585.0000 - auc: 0.7344 - prc: 0.3722 - val_loss: 0.4035 - val_Sensitivity: 0.0343 - val_tn: 3840.0000 - val_auc: 0.7388 - val_prc: 0.3717\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3978 - Sensitivity: 0.0687 - tn: 11557.0000 - auc: 0.7303 - prc: 0.3623 - val_loss: 0.4032 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7388 - val_prc: 0.3708\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3993 - Sensitivity: 0.0673 - tn: 11570.0000 - auc: 0.7264 - prc: 0.3685 - val_loss: 0.4032 - val_Sensitivity: 0.0330 - val_tn: 3841.0000 - val_auc: 0.7392 - val_prc: 0.3716\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3973 - Sensitivity: 0.0643 - tn: 11584.0000 - auc: 0.7310 - prc: 0.3744 - val_loss: 0.4031 - val_Sensitivity: 0.0343 - val_tn: 3840.0000 - val_auc: 0.7390 - val_prc: 0.3717\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3994 - Sensitivity: 0.0678 - tn: 11575.0000 - auc: 0.7270 - prc: 0.3624 - val_loss: 0.4033 - val_Sensitivity: 0.0317 - val_tn: 3841.0000 - val_auc: 0.7392 - val_prc: 0.3715\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3947 - Sensitivity: 0.0643 - tn: 11592.0000 - auc: 0.7348 - prc: 0.3789 - val_loss: 0.4032 - val_Sensitivity: 0.0317 - val_tn: 3841.0000 - val_auc: 0.7394 - val_prc: 0.3720\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3936 - Sensitivity: 0.0700 - tn: 11581.0000 - auc: 0.7376 - prc: 0.3767 - val_loss: 0.4032 - val_Sensitivity: 0.0355 - val_tn: 3840.0000 - val_auc: 0.7393 - val_prc: 0.3716\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3981 - Sensitivity: 0.0643 - tn: 11579.0000 - auc: 0.7281 - prc: 0.3680 - val_loss: 0.4032 - val_Sensitivity: 0.0368 - val_tn: 3840.0000 - val_auc: 0.7390 - val_prc: 0.3717\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3918 - Sensitivity: 0.0656 - tn: 11578.0000 - auc: 0.7425 - prc: 0.3854 - val_loss: 0.4032 - val_Sensitivity: 0.0381 - val_tn: 3840.0000 - val_auc: 0.7395 - val_prc: 0.3725\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3928 - Sensitivity: 0.0713 - tn: 11571.0000 - auc: 0.7394 - prc: 0.3817 - val_loss: 0.4031 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7392 - val_prc: 0.3722\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0709 - tn: 11565.0000 - auc: 0.7393 - prc: 0.3788 - val_loss: 0.4034 - val_Sensitivity: 0.0368 - val_tn: 3840.0000 - val_auc: 0.7387 - val_prc: 0.3725\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3938 - Sensitivity: 0.0757 - tn: 11574.0000 - auc: 0.7365 - prc: 0.3774 - val_loss: 0.4032 - val_Sensitivity: 0.0355 - val_tn: 3840.0000 - val_auc: 0.7390 - val_prc: 0.3723\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3915 - Sensitivity: 0.0744 - tn: 11575.0000 - auc: 0.7412 - prc: 0.3844 - val_loss: 0.4032 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7387 - val_prc: 0.3719\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3937 - Sensitivity: 0.0735 - tn: 11565.0000 - auc: 0.7391 - prc: 0.3751 - val_loss: 0.4032 - val_Sensitivity: 0.0393 - val_tn: 3839.0000 - val_auc: 0.7391 - val_prc: 0.3721\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3926 - Sensitivity: 0.0788 - tn: 11563.0000 - auc: 0.7410 - prc: 0.3828 - val_loss: 0.4033 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7388 - val_prc: 0.3716\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3925 - Sensitivity: 0.0775 - tn: 11570.0000 - auc: 0.7400 - prc: 0.3801 - val_loss: 0.4036 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7385 - val_prc: 0.3711\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3924 - Sensitivity: 0.0673 - tn: 11575.0000 - auc: 0.7405 - prc: 0.3785 - val_loss: 0.4034 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7387 - val_prc: 0.3713\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3903 - Sensitivity: 0.0748 - tn: 11570.0000 - auc: 0.7443 - prc: 0.3839 - val_loss: 0.4032 - val_Sensitivity: 0.0393 - val_tn: 3840.0000 - val_auc: 0.7389 - val_prc: 0.3711\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3953 - Sensitivity: 0.0753 - tn: 11561.0000 - auc: 0.7349 - prc: 0.3765 - val_loss: 0.4031 - val_Sensitivity: 0.0431 - val_tn: 3837.0000 - val_auc: 0.7392 - val_prc: 0.3718\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3916 - Sensitivity: 0.0717 - tn: 11566.0000 - auc: 0.7410 - prc: 0.3794 - val_loss: 0.4034 - val_Sensitivity: 0.0381 - val_tn: 3840.0000 - val_auc: 0.7390 - val_prc: 0.3721\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3941 - Sensitivity: 0.0673 - tn: 11579.0000 - auc: 0.7373 - prc: 0.3767 - val_loss: 0.4033 - val_Sensitivity: 0.0406 - val_tn: 3840.0000 - val_auc: 0.7388 - val_prc: 0.3722\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3911 - Sensitivity: 0.0753 - tn: 11571.0000 - auc: 0.7435 - prc: 0.3860 - val_loss: 0.4034 - val_Sensitivity: 0.0368 - val_tn: 3840.0000 - val_auc: 0.7390 - val_prc: 0.3723\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3910 - Sensitivity: 0.0713 - tn: 11568.0000 - auc: 0.7444 - prc: 0.3834 - val_loss: 0.4030 - val_Sensitivity: 0.0419 - val_tn: 3838.0000 - val_auc: 0.7392 - val_prc: 0.3722\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3908 - Sensitivity: 0.0691 - tn: 11559.0000 - auc: 0.7462 - prc: 0.3813 - val_loss: 0.4032 - val_Sensitivity: 0.0419 - val_tn: 3838.0000 - val_auc: 0.7392 - val_prc: 0.3719\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3910 - Sensitivity: 0.0814 - tn: 11547.0000 - auc: 0.7437 - prc: 0.3847 - val_loss: 0.4030 - val_Sensitivity: 0.0431 - val_tn: 3837.0000 - val_auc: 0.7394 - val_prc: 0.3721\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3911 - Sensitivity: 0.0792 - tn: 11564.0000 - auc: 0.7435 - prc: 0.3858 - val_loss: 0.4031 - val_Sensitivity: 0.0431 - val_tn: 3836.0000 - val_auc: 0.7394 - val_prc: 0.3721\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3901 - Sensitivity: 0.0779 - tn: 11562.0000 - auc: 0.7449 - prc: 0.3886 - val_loss: 0.4031 - val_Sensitivity: 0.0419 - val_tn: 3836.0000 - val_auc: 0.7392 - val_prc: 0.3721\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3894 - Sensitivity: 0.0748 - tn: 11555.0000 - auc: 0.7478 - prc: 0.3842 - val_loss: 0.4031 - val_Sensitivity: 0.0431 - val_tn: 3836.0000 - val_auc: 0.7390 - val_prc: 0.3717\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3937 - Sensitivity: 0.0739 - tn: 11570.0000 - auc: 0.7389 - prc: 0.3808 - val_loss: 0.4032 - val_Sensitivity: 0.0381 - val_tn: 3838.0000 - val_auc: 0.7392 - val_prc: 0.3724\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3920 - Sensitivity: 0.0739 - tn: 11562.0000 - auc: 0.7405 - prc: 0.3792 - val_loss: 0.4031 - val_Sensitivity: 0.0419 - val_tn: 3837.0000 - val_auc: 0.7390 - val_prc: 0.3722\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3908 - Sensitivity: 0.0827 - tn: 11547.0000 - auc: 0.7443 - prc: 0.3838 - val_loss: 0.4033 - val_Sensitivity: 0.0368 - val_tn: 3839.0000 - val_auc: 0.7389 - val_prc: 0.3723\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3900 - Sensitivity: 0.0717 - tn: 11574.0000 - auc: 0.7443 - prc: 0.3866 - val_loss: 0.4032 - val_Sensitivity: 0.0406 - val_tn: 3836.0000 - val_auc: 0.7388 - val_prc: 0.3723\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3908 - Sensitivity: 0.0761 - tn: 11560.0000 - auc: 0.7445 - prc: 0.3872 - val_loss: 0.4031 - val_Sensitivity: 0.0470 - val_tn: 3834.0000 - val_auc: 0.7387 - val_prc: 0.3714\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3897 - Sensitivity: 0.0744 - tn: 11553.0000 - auc: 0.7468 - prc: 0.3865 - val_loss: 0.4034 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7382 - val_prc: 0.3712\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3912 - Sensitivity: 0.0788 - tn: 11579.0000 - auc: 0.7435 - prc: 0.3889 - val_loss: 0.4033 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7385 - val_prc: 0.3721\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3890 - Sensitivity: 0.0722 - tn: 11575.0000 - auc: 0.7460 - prc: 0.3865 - val_loss: 0.4034 - val_Sensitivity: 0.0419 - val_tn: 3835.0000 - val_auc: 0.7383 - val_prc: 0.3721\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3900 - Sensitivity: 0.0805 - tn: 11561.0000 - auc: 0.7443 - prc: 0.3812 - val_loss: 0.4032 - val_Sensitivity: 0.0457 - val_tn: 3834.0000 - val_auc: 0.7385 - val_prc: 0.3718\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3878 - Sensitivity: 0.0801 - tn: 11562.0000 - auc: 0.7495 - prc: 0.3908 - val_loss: 0.4034 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7385 - val_prc: 0.3719\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3889 - Sensitivity: 0.0761 - tn: 11562.0000 - auc: 0.7474 - prc: 0.3865 - val_loss: 0.4035 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7386 - val_prc: 0.3724\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3922 - Sensitivity: 0.0757 - tn: 11561.0000 - auc: 0.7397 - prc: 0.3808 - val_loss: 0.4032 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7389 - val_prc: 0.3725\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3884 - Sensitivity: 0.0823 - tn: 11552.0000 - auc: 0.7469 - prc: 0.3881 - val_loss: 0.4032 - val_Sensitivity: 0.0457 - val_tn: 3835.0000 - val_auc: 0.7386 - val_prc: 0.3723\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3892 - Sensitivity: 0.0775 - tn: 11552.0000 - auc: 0.7472 - prc: 0.3857 - val_loss: 0.4031 - val_Sensitivity: 0.0444 - val_tn: 3835.0000 - val_auc: 0.7388 - val_prc: 0.3726\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3903 - Sensitivity: 0.0779 - tn: 11567.0000 - auc: 0.7462 - prc: 0.3856 - val_loss: 0.4031 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7388 - val_prc: 0.3727\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3893 - Sensitivity: 0.0849 - tn: 11569.0000 - auc: 0.7458 - prc: 0.3908 - val_loss: 0.4031 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7389 - val_prc: 0.3727\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3922 - Sensitivity: 0.0827 - tn: 11555.0000 - auc: 0.7409 - prc: 0.3850 - val_loss: 0.4030 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7388 - val_prc: 0.3726\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3896 - Sensitivity: 0.0797 - tn: 11580.0000 - auc: 0.7437 - prc: 0.3901 - val_loss: 0.4032 - val_Sensitivity: 0.0431 - val_tn: 3835.0000 - val_auc: 0.7388 - val_prc: 0.3722\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3892 - Sensitivity: 0.0867 - tn: 11560.0000 - auc: 0.7478 - prc: 0.3925 - val_loss: 0.4030 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7392 - val_prc: 0.3722\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3904 - Sensitivity: 0.0810 - tn: 11564.0000 - auc: 0.7425 - prc: 0.3899 - val_loss: 0.4032 - val_Sensitivity: 0.0419 - val_tn: 3836.0000 - val_auc: 0.7393 - val_prc: 0.3727\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3902 - Sensitivity: 0.0805 - tn: 11567.0000 - auc: 0.7436 - prc: 0.3898 - val_loss: 0.4030 - val_Sensitivity: 0.0444 - val_tn: 3834.0000 - val_auc: 0.7391 - val_prc: 0.3718\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3885 - Sensitivity: 0.0845 - tn: 11558.0000 - auc: 0.7470 - prc: 0.3930 - val_loss: 0.4032 - val_Sensitivity: 0.0431 - val_tn: 3834.0000 - val_auc: 0.7390 - val_prc: 0.3715\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3887 - Sensitivity: 0.0858 - tn: 11555.0000 - auc: 0.7487 - prc: 0.3898 - val_loss: 0.4032 - val_Sensitivity: 0.0482 - val_tn: 3833.0000 - val_auc: 0.7389 - val_prc: 0.3710\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3883 - Sensitivity: 0.0880 - tn: 11560.0000 - auc: 0.7481 - prc: 0.3984 - val_loss: 0.4033 - val_Sensitivity: 0.0419 - val_tn: 3835.0000 - val_auc: 0.7391 - val_prc: 0.3714\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3874 - Sensitivity: 0.0819 - tn: 11544.0000 - auc: 0.7504 - prc: 0.3921 - val_loss: 0.4031 - val_Sensitivity: 0.0457 - val_tn: 3834.0000 - val_auc: 0.7391 - val_prc: 0.3714\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3875 - Sensitivity: 0.0823 - tn: 11553.0000 - auc: 0.7498 - prc: 0.3939 - val_loss: 0.4032 - val_Sensitivity: 0.0457 - val_tn: 3833.0000 - val_auc: 0.7390 - val_prc: 0.3716\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3881 - Sensitivity: 0.0854 - tn: 11544.0000 - auc: 0.7495 - prc: 0.3936 - val_loss: 0.4030 - val_Sensitivity: 0.0457 - val_tn: 3833.0000 - val_auc: 0.7391 - val_prc: 0.3720\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3863 - Sensitivity: 0.0854 - tn: 11568.0000 - auc: 0.7518 - prc: 0.4004 - val_loss: 0.4032 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7388 - val_prc: 0.3722\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3894 - Sensitivity: 0.0845 - tn: 11556.0000 - auc: 0.7469 - prc: 0.3858 - val_loss: 0.4034 - val_Sensitivity: 0.0444 - val_tn: 3834.0000 - val_auc: 0.7387 - val_prc: 0.3718\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3861 - Sensitivity: 0.0827 - tn: 11552.0000 - auc: 0.7516 - prc: 0.3921 - val_loss: 0.4033 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7389 - val_prc: 0.3718\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3868 - Sensitivity: 0.0858 - tn: 11560.0000 - auc: 0.7512 - prc: 0.3960 - val_loss: 0.4034 - val_Sensitivity: 0.0444 - val_tn: 3832.0000 - val_auc: 0.7384 - val_prc: 0.3718\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3883 - Sensitivity: 0.0854 - tn: 11561.0000 - auc: 0.7487 - prc: 0.3934 - val_loss: 0.4033 - val_Sensitivity: 0.0444 - val_tn: 3831.0000 - val_auc: 0.7389 - val_prc: 0.3717\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3877 - Sensitivity: 0.0841 - tn: 11549.0000 - auc: 0.7492 - prc: 0.3940 - val_loss: 0.4032 - val_Sensitivity: 0.0457 - val_tn: 3827.0000 - val_auc: 0.7390 - val_prc: 0.3718\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3857 - Sensitivity: 0.0841 - tn: 11553.0000 - auc: 0.7538 - prc: 0.3974 - val_loss: 0.4032 - val_Sensitivity: 0.0457 - val_tn: 3830.0000 - val_auc: 0.7393 - val_prc: 0.3724\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3864 - Sensitivity: 0.0832 - tn: 11544.0000 - auc: 0.7539 - prc: 0.3898 - val_loss: 0.4033 - val_Sensitivity: 0.0457 - val_tn: 3832.0000 - val_auc: 0.7388 - val_prc: 0.3724\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3886 - Sensitivity: 0.0827 - tn: 11551.0000 - auc: 0.7484 - prc: 0.3882 - val_loss: 0.4033 - val_Sensitivity: 0.0457 - val_tn: 3834.0000 - val_auc: 0.7387 - val_prc: 0.3720\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3859 - Sensitivity: 0.0814 - tn: 11570.0000 - auc: 0.7544 - prc: 0.4036 - val_loss: 0.4034 - val_Sensitivity: 0.0444 - val_tn: 3834.0000 - val_auc: 0.7387 - val_prc: 0.3726\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3870 - Sensitivity: 0.0810 - tn: 11556.0000 - auc: 0.7495 - prc: 0.3934 - val_loss: 0.4033 - val_Sensitivity: 0.0457 - val_tn: 3832.0000 - val_auc: 0.7384 - val_prc: 0.3718\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3879 - Sensitivity: 0.0797 - tn: 11554.0000 - auc: 0.7512 - prc: 0.3926 - val_loss: 0.4033 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7388 - val_prc: 0.3720\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3878 - Sensitivity: 0.0889 - tn: 11569.0000 - auc: 0.7473 - prc: 0.3969 - val_loss: 0.4034 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7386 - val_prc: 0.3724\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3881 - Sensitivity: 0.0797 - tn: 11555.0000 - auc: 0.7493 - prc: 0.3902 - val_loss: 0.4036 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7383 - val_prc: 0.3724\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3842 - Sensitivity: 0.0775 - tn: 11566.0000 - auc: 0.7578 - prc: 0.4005 - val_loss: 0.4035 - val_Sensitivity: 0.0444 - val_tn: 3833.0000 - val_auc: 0.7384 - val_prc: 0.3724\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3873 - Sensitivity: 0.0788 - tn: 11569.0000 - auc: 0.7517 - prc: 0.3930 - val_loss: 0.4035 - val_Sensitivity: 0.0431 - val_tn: 3832.0000 - val_auc: 0.7387 - val_prc: 0.3725\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3882 - Sensitivity: 0.0836 - tn: 11551.0000 - auc: 0.7488 - prc: 0.3851 - val_loss: 0.4033 - val_Sensitivity: 0.0431 - val_tn: 3832.0000 - val_auc: 0.7387 - val_prc: 0.3730\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3863 - Sensitivity: 0.0827 - tn: 11548.0000 - auc: 0.7542 - prc: 0.3947 - val_loss: 0.4033 - val_Sensitivity: 0.0444 - val_tn: 3831.0000 - val_auc: 0.7388 - val_prc: 0.3724\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3869 - Sensitivity: 0.0841 - tn: 11565.0000 - auc: 0.7517 - prc: 0.3965 - val_loss: 0.4034 - val_Sensitivity: 0.0444 - val_tn: 3831.0000 - val_auc: 0.7384 - val_prc: 0.3720\n",
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 26ms/step - loss: 1.0247 - Sensitivity: 0.4987 - tn: 6050.0000 - auc: 0.5110 - prc: 0.1692 - val_loss: 0.4668 - val_Sensitivity: 0.0013 - val_tn: 3850.0000 - val_auc: 0.5556 - val_prc: 0.1976\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8108 - Sensitivity: 0.3791 - tn: 7673.0000 - auc: 0.5280 - prc: 0.1752 - val_loss: 0.4489 - val_Sensitivity: 0.0013 - val_tn: 3859.0000 - val_auc: 0.6137 - val_prc: 0.2322\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.7100 - Sensitivity: 0.3087 - tn: 8754.0000 - auc: 0.5402 - prc: 0.1859 - val_loss: 0.4414 - val_Sensitivity: 0.0013 - val_tn: 3860.0000 - val_auc: 0.6407 - val_prc: 0.2520\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6414 - Sensitivity: 0.2467 - tn: 9551.0000 - auc: 0.5580 - prc: 0.1955 - val_loss: 0.4356 - val_Sensitivity: 0.0000e+00 - val_tn: 3860.0000 - val_auc: 0.6605 - val_prc: 0.2676\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6213 - Sensitivity: 0.2040 - tn: 9907.0000 - auc: 0.5564 - prc: 0.1964 - val_loss: 0.4313 - val_Sensitivity: 0.0000e+00 - val_tn: 3859.0000 - val_auc: 0.6763 - val_prc: 0.2832\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5994 - Sensitivity: 0.1944 - tn: 10217.0000 - auc: 0.5718 - prc: 0.2004 - val_loss: 0.4290 - val_Sensitivity: 0.0000e+00 - val_tn: 3858.0000 - val_auc: 0.6876 - val_prc: 0.2941\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5826 - Sensitivity: 0.1834 - tn: 10362.0000 - auc: 0.5866 - prc: 0.2143 - val_loss: 0.4262 - val_Sensitivity: 0.0000e+00 - val_tn: 3857.0000 - val_auc: 0.6971 - val_prc: 0.3049\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5708 - Sensitivity: 0.1693 - tn: 10492.0000 - auc: 0.5839 - prc: 0.2117 - val_loss: 0.4243 - val_Sensitivity: 0.0013 - val_tn: 3856.0000 - val_auc: 0.7041 - val_prc: 0.3144\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5588 - Sensitivity: 0.1741 - tn: 10478.0000 - auc: 0.5990 - prc: 0.2176 - val_loss: 0.4232 - val_Sensitivity: 0.0025 - val_tn: 3856.0000 - val_auc: 0.7087 - val_prc: 0.3210\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5563 - Sensitivity: 0.1570 - tn: 10627.0000 - auc: 0.5988 - prc: 0.2194 - val_loss: 0.4215 - val_Sensitivity: 0.0102 - val_tn: 3850.0000 - val_auc: 0.7125 - val_prc: 0.3278\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5349 - Sensitivity: 0.1755 - tn: 10706.0000 - auc: 0.6193 - prc: 0.2425 - val_loss: 0.4199 - val_Sensitivity: 0.0191 - val_tn: 3844.0000 - val_auc: 0.7153 - val_prc: 0.3311\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5353 - Sensitivity: 0.1609 - tn: 10720.0000 - auc: 0.6082 - prc: 0.2350 - val_loss: 0.4176 - val_Sensitivity: 0.0305 - val_tn: 3841.0000 - val_auc: 0.7184 - val_prc: 0.3352\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5233 - Sensitivity: 0.1636 - tn: 10746.0000 - auc: 0.6221 - prc: 0.2370 - val_loss: 0.4170 - val_Sensitivity: 0.0356 - val_tn: 3836.0000 - val_auc: 0.7204 - val_prc: 0.3392\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5134 - Sensitivity: 0.1684 - tn: 10785.0000 - auc: 0.6260 - prc: 0.2430 - val_loss: 0.4164 - val_Sensitivity: 0.0381 - val_tn: 3837.0000 - val_auc: 0.7217 - val_prc: 0.3412\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5140 - Sensitivity: 0.1522 - tn: 10834.0000 - auc: 0.6236 - prc: 0.2429 - val_loss: 0.4154 - val_Sensitivity: 0.0356 - val_tn: 3835.0000 - val_auc: 0.7229 - val_prc: 0.3434\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5066 - Sensitivity: 0.1539 - tn: 10889.0000 - auc: 0.6267 - prc: 0.2451 - val_loss: 0.4150 - val_Sensitivity: 0.0407 - val_tn: 3834.0000 - val_auc: 0.7243 - val_prc: 0.3448\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4966 - Sensitivity: 0.1469 - tn: 10930.0000 - auc: 0.6302 - prc: 0.2521 - val_loss: 0.4140 - val_Sensitivity: 0.0432 - val_tn: 3832.0000 - val_auc: 0.7255 - val_prc: 0.3461\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4846 - Sensitivity: 0.1500 - tn: 10910.0000 - auc: 0.6434 - prc: 0.2639 - val_loss: 0.4129 - val_Sensitivity: 0.0457 - val_tn: 3832.0000 - val_auc: 0.7262 - val_prc: 0.3481\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4898 - Sensitivity: 0.1407 - tn: 10951.0000 - auc: 0.6367 - prc: 0.2560 - val_loss: 0.4128 - val_Sensitivity: 0.0445 - val_tn: 3832.0000 - val_auc: 0.7264 - val_prc: 0.3475\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4831 - Sensitivity: 0.1385 - tn: 11002.0000 - auc: 0.6377 - prc: 0.2571 - val_loss: 0.4123 - val_Sensitivity: 0.0445 - val_tn: 3831.0000 - val_auc: 0.7270 - val_prc: 0.3486\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4730 - Sensitivity: 0.1407 - tn: 11015.0000 - auc: 0.6486 - prc: 0.2699 - val_loss: 0.4118 - val_Sensitivity: 0.0432 - val_tn: 3831.0000 - val_auc: 0.7275 - val_prc: 0.3497\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4761 - Sensitivity: 0.1372 - tn: 11025.0000 - auc: 0.6488 - prc: 0.2617 - val_loss: 0.4115 - val_Sensitivity: 0.0432 - val_tn: 3832.0000 - val_auc: 0.7285 - val_prc: 0.3503\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4707 - Sensitivity: 0.1315 - tn: 11093.0000 - auc: 0.6487 - prc: 0.2605 - val_loss: 0.4109 - val_Sensitivity: 0.0445 - val_tn: 3832.0000 - val_auc: 0.7287 - val_prc: 0.3506\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4640 - Sensitivity: 0.1284 - tn: 11101.0000 - auc: 0.6545 - prc: 0.2726 - val_loss: 0.4102 - val_Sensitivity: 0.0445 - val_tn: 3832.0000 - val_auc: 0.7292 - val_prc: 0.3509\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4701 - Sensitivity: 0.1258 - tn: 11130.0000 - auc: 0.6437 - prc: 0.2634 - val_loss: 0.4097 - val_Sensitivity: 0.0432 - val_tn: 3831.0000 - val_auc: 0.7295 - val_prc: 0.3512\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4564 - Sensitivity: 0.1302 - tn: 11161.0000 - auc: 0.6588 - prc: 0.2717 - val_loss: 0.4096 - val_Sensitivity: 0.0394 - val_tn: 3834.0000 - val_auc: 0.7300 - val_prc: 0.3510\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4564 - Sensitivity: 0.1060 - tn: 11206.0000 - auc: 0.6555 - prc: 0.2664 - val_loss: 0.4088 - val_Sensitivity: 0.0432 - val_tn: 3830.0000 - val_auc: 0.7301 - val_prc: 0.3510\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4535 - Sensitivity: 0.1165 - tn: 11225.0000 - auc: 0.6563 - prc: 0.2759 - val_loss: 0.4085 - val_Sensitivity: 0.0407 - val_tn: 3833.0000 - val_auc: 0.7308 - val_prc: 0.3519\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4489 - Sensitivity: 0.1174 - tn: 11260.0000 - auc: 0.6583 - prc: 0.2821 - val_loss: 0.4081 - val_Sensitivity: 0.0407 - val_tn: 3831.0000 - val_auc: 0.7314 - val_prc: 0.3523\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4431 - Sensitivity: 0.1139 - tn: 11300.0000 - auc: 0.6674 - prc: 0.2826 - val_loss: 0.4077 - val_Sensitivity: 0.0407 - val_tn: 3832.0000 - val_auc: 0.7322 - val_prc: 0.3528\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4427 - Sensitivity: 0.1091 - tn: 11310.0000 - auc: 0.6671 - prc: 0.2885 - val_loss: 0.4073 - val_Sensitivity: 0.0394 - val_tn: 3832.0000 - val_auc: 0.7319 - val_prc: 0.3525\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4419 - Sensitivity: 0.1038 - tn: 11257.0000 - auc: 0.6677 - prc: 0.2792 - val_loss: 0.4072 - val_Sensitivity: 0.0368 - val_tn: 3832.0000 - val_auc: 0.7326 - val_prc: 0.3526\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4370 - Sensitivity: 0.0959 - tn: 11319.0000 - auc: 0.6731 - prc: 0.2847 - val_loss: 0.4074 - val_Sensitivity: 0.0330 - val_tn: 3834.0000 - val_auc: 0.7324 - val_prc: 0.3523\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4373 - Sensitivity: 0.0998 - tn: 11384.0000 - auc: 0.6687 - prc: 0.2925 - val_loss: 0.4074 - val_Sensitivity: 0.0254 - val_tn: 3834.0000 - val_auc: 0.7327 - val_prc: 0.3525\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4302 - Sensitivity: 0.0932 - tn: 11431.0000 - auc: 0.6774 - prc: 0.3024 - val_loss: 0.4069 - val_Sensitivity: 0.0267 - val_tn: 3833.0000 - val_auc: 0.7330 - val_prc: 0.3526\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4343 - Sensitivity: 0.0910 - tn: 11405.0000 - auc: 0.6699 - prc: 0.2870 - val_loss: 0.4066 - val_Sensitivity: 0.0292 - val_tn: 3833.0000 - val_auc: 0.7338 - val_prc: 0.3531\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4376 - Sensitivity: 0.0879 - tn: 10463.0000 - auc: 0.6669 - prc: 0.28 - 0s 11ms/step - loss: 0.4362 - Sensitivity: 0.0893 - tn: 11403.0000 - auc: 0.6682 - prc: 0.2879 - val_loss: 0.4064 - val_Sensitivity: 0.0280 - val_tn: 3833.0000 - val_auc: 0.7343 - val_prc: 0.3535\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4299 - Sensitivity: 0.0805 - tn: 11435.0000 - auc: 0.6802 - prc: 0.2936 - val_loss: 0.4061 - val_Sensitivity: 0.0305 - val_tn: 3831.0000 - val_auc: 0.7347 - val_prc: 0.3540\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4263 - Sensitivity: 0.0831 - tn: 11451.0000 - auc: 0.6810 - prc: 0.3040 - val_loss: 0.4059 - val_Sensitivity: 0.0292 - val_tn: 3832.0000 - val_auc: 0.7348 - val_prc: 0.3541\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4229 - Sensitivity: 0.0814 - tn: 11448.0000 - auc: 0.6892 - prc: 0.3057 - val_loss: 0.4055 - val_Sensitivity: 0.0305 - val_tn: 3832.0000 - val_auc: 0.7359 - val_prc: 0.3548\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4212 - Sensitivity: 0.0739 - tn: 11478.0000 - auc: 0.6902 - prc: 0.3042 - val_loss: 0.4053 - val_Sensitivity: 0.0292 - val_tn: 3833.0000 - val_auc: 0.7360 - val_prc: 0.3550\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4257 - Sensitivity: 0.0752 - tn: 11480.0000 - auc: 0.6790 - prc: 0.3023 - val_loss: 0.4055 - val_Sensitivity: 0.0229 - val_tn: 3835.0000 - val_auc: 0.7357 - val_prc: 0.3543\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4203 - Sensitivity: 0.0770 - tn: 11509.0000 - auc: 0.6917 - prc: 0.3155 - val_loss: 0.4053 - val_Sensitivity: 0.0229 - val_tn: 3836.0000 - val_auc: 0.7361 - val_prc: 0.3543\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4204 - Sensitivity: 0.0783 - tn: 11521.0000 - auc: 0.6882 - prc: 0.3132 - val_loss: 0.4049 - val_Sensitivity: 0.0280 - val_tn: 3835.0000 - val_auc: 0.7369 - val_prc: 0.3552\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4194 - Sensitivity: 0.0787 - tn: 11491.0000 - auc: 0.6906 - prc: 0.3146 - val_loss: 0.4048 - val_Sensitivity: 0.0254 - val_tn: 3836.0000 - val_auc: 0.7369 - val_prc: 0.3551\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4178 - Sensitivity: 0.0699 - tn: 11523.0000 - auc: 0.6931 - prc: 0.3179 - val_loss: 0.4046 - val_Sensitivity: 0.0267 - val_tn: 3836.0000 - val_auc: 0.7374 - val_prc: 0.3558\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4183 - Sensitivity: 0.0712 - tn: 11503.0000 - auc: 0.6926 - prc: 0.3108 - val_loss: 0.4045 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7375 - val_prc: 0.3561\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4203 - Sensitivity: 0.0651 - tn: 11519.0000 - auc: 0.6901 - prc: 0.3039 - val_loss: 0.4045 - val_Sensitivity: 0.0229 - val_tn: 3838.0000 - val_auc: 0.7374 - val_prc: 0.3562\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4163 - Sensitivity: 0.0646 - tn: 11530.0000 - auc: 0.6946 - prc: 0.3184 - val_loss: 0.4044 - val_Sensitivity: 0.0229 - val_tn: 3838.0000 - val_auc: 0.7378 - val_prc: 0.3563\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4153 - Sensitivity: 0.0660 - tn: 11547.0000 - auc: 0.6940 - prc: 0.3208 - val_loss: 0.4044 - val_Sensitivity: 0.0216 - val_tn: 3839.0000 - val_auc: 0.7380 - val_prc: 0.3563\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4182 - Sensitivity: 0.0668 - tn: 11537.0000 - auc: 0.6908 - prc: 0.3160 - val_loss: 0.4040 - val_Sensitivity: 0.0267 - val_tn: 3837.0000 - val_auc: 0.7386 - val_prc: 0.3573\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4129 - Sensitivity: 0.0642 - tn: 11528.0000 - auc: 0.7017 - prc: 0.3251 - val_loss: 0.4041 - val_Sensitivity: 0.0216 - val_tn: 3838.0000 - val_auc: 0.7384 - val_prc: 0.3570\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4158 - Sensitivity: 0.0576 - tn: 11564.0000 - auc: 0.6936 - prc: 0.3232 - val_loss: 0.4040 - val_Sensitivity: 0.0216 - val_tn: 3839.0000 - val_auc: 0.7392 - val_prc: 0.3578\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4135 - Sensitivity: 0.0607 - tn: 11553.0000 - auc: 0.6998 - prc: 0.3264 - val_loss: 0.4037 - val_Sensitivity: 0.0229 - val_tn: 3838.0000 - val_auc: 0.7396 - val_prc: 0.3577\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4161 - Sensitivity: 0.0567 - tn: 11564.0000 - auc: 0.6922 - prc: 0.3217 - val_loss: 0.4038 - val_Sensitivity: 0.0229 - val_tn: 3837.0000 - val_auc: 0.7394 - val_prc: 0.3576\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4101 - Sensitivity: 0.0541 - tn: 11566.0000 - auc: 0.7049 - prc: 0.3289 - val_loss: 0.4037 - val_Sensitivity: 0.0229 - val_tn: 3838.0000 - val_auc: 0.7397 - val_prc: 0.3582\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4115 - Sensitivity: 0.0515 - tn: 11569.0000 - auc: 0.7035 - prc: 0.3242 - val_loss: 0.4036 - val_Sensitivity: 0.0229 - val_tn: 3837.0000 - val_auc: 0.7396 - val_prc: 0.3578\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4101 - Sensitivity: 0.0541 - tn: 11582.0000 - auc: 0.7039 - prc: 0.3362 - val_loss: 0.4036 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7396 - val_prc: 0.3580\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4118 - Sensitivity: 0.0519 - tn: 11575.0000 - auc: 0.7014 - prc: 0.3298 - val_loss: 0.4035 - val_Sensitivity: 0.0254 - val_tn: 3837.0000 - val_auc: 0.7397 - val_prc: 0.3580\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4095 - Sensitivity: 0.0594 - tn: 11582.0000 - auc: 0.7057 - prc: 0.3374 - val_loss: 0.4034 - val_Sensitivity: 0.0254 - val_tn: 3837.0000 - val_auc: 0.7397 - val_prc: 0.3581\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4124 - Sensitivity: 0.0541 - tn: 11570.0000 - auc: 0.6993 - prc: 0.3306 - val_loss: 0.4035 - val_Sensitivity: 0.0254 - val_tn: 3837.0000 - val_auc: 0.7399 - val_prc: 0.3577\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4119 - Sensitivity: 0.0523 - tn: 11573.0000 - auc: 0.6985 - prc: 0.3260 - val_loss: 0.4033 - val_Sensitivity: 0.0254 - val_tn: 3837.0000 - val_auc: 0.7402 - val_prc: 0.3582\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4083 - Sensitivity: 0.0550 - tn: 11588.0000 - auc: 0.7071 - prc: 0.3390 - val_loss: 0.4033 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7402 - val_prc: 0.3582\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4095 - Sensitivity: 0.0523 - tn: 11577.0000 - auc: 0.7051 - prc: 0.3344 - val_loss: 0.4033 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7404 - val_prc: 0.3586\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4084 - Sensitivity: 0.0572 - tn: 11568.0000 - auc: 0.7065 - prc: 0.3368 - val_loss: 0.4030 - val_Sensitivity: 0.0254 - val_tn: 3837.0000 - val_auc: 0.7408 - val_prc: 0.3589\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4108 - Sensitivity: 0.0563 - tn: 11588.0000 - auc: 0.7022 - prc: 0.3318 - val_loss: 0.4030 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7407 - val_prc: 0.3590\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4069 - Sensitivity: 0.0515 - tn: 11590.0000 - auc: 0.7104 - prc: 0.3392 - val_loss: 0.4029 - val_Sensitivity: 0.0267 - val_tn: 3836.0000 - val_auc: 0.7411 - val_prc: 0.3594\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4082 - Sensitivity: 0.0572 - tn: 11574.0000 - auc: 0.7075 - prc: 0.3388 - val_loss: 0.4027 - val_Sensitivity: 0.0280 - val_tn: 3836.0000 - val_auc: 0.7412 - val_prc: 0.3596\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4058 - Sensitivity: 0.0594 - tn: 11570.0000 - auc: 0.7120 - prc: 0.3363 - val_loss: 0.4028 - val_Sensitivity: 0.0254 - val_tn: 3836.0000 - val_auc: 0.7414 - val_prc: 0.3598\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4054 - Sensitivity: 0.0567 - tn: 11594.0000 - auc: 0.7135 - prc: 0.3473 - val_loss: 0.4028 - val_Sensitivity: 0.0254 - val_tn: 3836.0000 - val_auc: 0.7412 - val_prc: 0.3598\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4058 - Sensitivity: 0.0550 - tn: 11592.0000 - auc: 0.7100 - prc: 0.3448 - val_loss: 0.4028 - val_Sensitivity: 0.0254 - val_tn: 3836.0000 - val_auc: 0.7413 - val_prc: 0.3598\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4071 - Sensitivity: 0.0510 - tn: 11590.0000 - auc: 0.7095 - prc: 0.3350 - val_loss: 0.4028 - val_Sensitivity: 0.0280 - val_tn: 3836.0000 - val_auc: 0.7413 - val_prc: 0.3603\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4040 - Sensitivity: 0.0554 - tn: 11590.0000 - auc: 0.7156 - prc: 0.3456 - val_loss: 0.4028 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7413 - val_prc: 0.3600\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4021 - Sensitivity: 0.0594 - tn: 11579.0000 - auc: 0.7199 - prc: 0.3496 - val_loss: 0.4026 - val_Sensitivity: 0.0292 - val_tn: 3836.0000 - val_auc: 0.7418 - val_prc: 0.3602\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4038 - Sensitivity: 0.0523 - tn: 11595.0000 - auc: 0.7162 - prc: 0.3501 - val_loss: 0.4028 - val_Sensitivity: 0.0267 - val_tn: 3837.0000 - val_auc: 0.7414 - val_prc: 0.3594\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4008 - Sensitivity: 0.0580 - tn: 11584.0000 - auc: 0.7229 - prc: 0.3515 - val_loss: 0.4027 - val_Sensitivity: 0.0254 - val_tn: 3838.0000 - val_auc: 0.7415 - val_prc: 0.3594\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4013 - Sensitivity: 0.0554 - tn: 11583.0000 - auc: 0.7245 - prc: 0.3463 - val_loss: 0.4027 - val_Sensitivity: 0.0292 - val_tn: 3835.0000 - val_auc: 0.7419 - val_prc: 0.3592\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4024 - Sensitivity: 0.0528 - tn: 11589.0000 - auc: 0.7180 - prc: 0.3454 - val_loss: 0.4027 - val_Sensitivity: 0.0267 - val_tn: 3837.0000 - val_auc: 0.7418 - val_prc: 0.3589\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4063 - Sensitivity: 0.0585 - tn: 11598.0000 - auc: 0.7105 - prc: 0.3502 - val_loss: 0.4024 - val_Sensitivity: 0.0343 - val_tn: 3834.0000 - val_auc: 0.7422 - val_prc: 0.3600\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4027 - Sensitivity: 0.0602 - tn: 11585.0000 - auc: 0.7180 - prc: 0.3550 - val_loss: 0.4023 - val_Sensitivity: 0.0343 - val_tn: 3833.0000 - val_auc: 0.7422 - val_prc: 0.3601\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4029 - Sensitivity: 0.0594 - tn: 11588.0000 - auc: 0.7180 - prc: 0.3548 - val_loss: 0.4023 - val_Sensitivity: 0.0343 - val_tn: 3834.0000 - val_auc: 0.7425 - val_prc: 0.3599\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4044 - Sensitivity: 0.0589 - tn: 11587.0000 - auc: 0.7163 - prc: 0.3515 - val_loss: 0.4024 - val_Sensitivity: 0.0280 - val_tn: 3835.0000 - val_auc: 0.7424 - val_prc: 0.3597\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4029 - Sensitivity: 0.0523 - tn: 11593.0000 - auc: 0.7198 - prc: 0.3542 - val_loss: 0.4022 - val_Sensitivity: 0.0330 - val_tn: 3834.0000 - val_auc: 0.7419 - val_prc: 0.3599\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4017 - Sensitivity: 0.0611 - tn: 11592.0000 - auc: 0.7219 - prc: 0.3564 - val_loss: 0.4024 - val_Sensitivity: 0.0318 - val_tn: 3833.0000 - val_auc: 0.7419 - val_prc: 0.3599\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4020 - Sensitivity: 0.0523 - tn: 11577.0000 - auc: 0.7215 - prc: 0.3496 - val_loss: 0.4023 - val_Sensitivity: 0.0343 - val_tn: 3833.0000 - val_auc: 0.7420 - val_prc: 0.3599\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4029 - Sensitivity: 0.0580 - tn: 11590.0000 - auc: 0.7193 - prc: 0.3544 - val_loss: 0.4023 - val_Sensitivity: 0.0356 - val_tn: 3833.0000 - val_auc: 0.7421 - val_prc: 0.3597\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3998 - Sensitivity: 0.0633 - tn: 11588.0000 - auc: 0.7257 - prc: 0.3617 - val_loss: 0.4023 - val_Sensitivity: 0.0368 - val_tn: 3832.0000 - val_auc: 0.7424 - val_prc: 0.3598\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4025 - Sensitivity: 0.0642 - tn: 11588.0000 - auc: 0.7189 - prc: 0.3576 - val_loss: 0.4021 - val_Sensitivity: 0.0394 - val_tn: 3829.0000 - val_auc: 0.7427 - val_prc: 0.3603\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3990 - Sensitivity: 0.0633 - tn: 11585.0000 - auc: 0.7261 - prc: 0.3653 - val_loss: 0.4022 - val_Sensitivity: 0.0381 - val_tn: 3830.0000 - val_auc: 0.7422 - val_prc: 0.3597\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3990 - Sensitivity: 0.0660 - tn: 11578.0000 - auc: 0.7268 - prc: 0.3627 - val_loss: 0.4023 - val_Sensitivity: 0.0356 - val_tn: 3830.0000 - val_auc: 0.7425 - val_prc: 0.3602\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3996 - Sensitivity: 0.0576 - tn: 11577.0000 - auc: 0.7259 - prc: 0.3600 - val_loss: 0.4022 - val_Sensitivity: 0.0381 - val_tn: 3830.0000 - val_auc: 0.7424 - val_prc: 0.3596\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3999 - Sensitivity: 0.0642 - tn: 11576.0000 - auc: 0.7235 - prc: 0.3581 - val_loss: 0.4023 - val_Sensitivity: 0.0381 - val_tn: 3830.0000 - val_auc: 0.7423 - val_prc: 0.3597\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3996 - Sensitivity: 0.0655 - tn: 11568.0000 - auc: 0.7253 - prc: 0.3607 - val_loss: 0.4023 - val_Sensitivity: 0.0356 - val_tn: 3830.0000 - val_auc: 0.7420 - val_prc: 0.3593\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4022 - Sensitivity: 0.0699 - tn: 11587.0000 - auc: 0.7186 - prc: 0.3629 - val_loss: 0.4021 - val_Sensitivity: 0.0419 - val_tn: 3830.0000 - val_auc: 0.7425 - val_prc: 0.3598\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4008 - Sensitivity: 0.0704 - tn: 11575.0000 - auc: 0.7230 - prc: 0.3598 - val_loss: 0.4022 - val_Sensitivity: 0.0407 - val_tn: 3830.0000 - val_auc: 0.7423 - val_prc: 0.3594\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3996 - Sensitivity: 0.0620 - tn: 11592.0000 - auc: 0.7247 - prc: 0.3643 - val_loss: 0.4025 - val_Sensitivity: 0.0330 - val_tn: 3833.0000 - val_auc: 0.7419 - val_prc: 0.3591\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3987 - Sensitivity: 0.0607 - tn: 11576.0000 - auc: 0.7265 - prc: 0.3651 - val_loss: 0.4023 - val_Sensitivity: 0.0368 - val_tn: 3830.0000 - val_auc: 0.7424 - val_prc: 0.3593\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3995 - Sensitivity: 0.0682 - tn: 11571.0000 - auc: 0.7247 - prc: 0.3620 - val_loss: 0.4023 - val_Sensitivity: 0.0368 - val_tn: 3830.0000 - val_auc: 0.7425 - val_prc: 0.3592\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3998 - Sensitivity: 0.0651 - tn: 11576.0000 - auc: 0.7250 - prc: 0.3648 - val_loss: 0.4023 - val_Sensitivity: 0.0368 - val_tn: 3830.0000 - val_auc: 0.7425 - val_prc: 0.3591\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3969 - Sensitivity: 0.0668 - tn: 11569.0000 - auc: 0.7319 - prc: 0.3682 - val_loss: 0.4023 - val_Sensitivity: 0.0419 - val_tn: 3830.0000 - val_auc: 0.7421 - val_prc: 0.3586\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3965 - Sensitivity: 0.0673 - tn: 11583.0000 - auc: 0.7317 - prc: 0.3684 - val_loss: 0.4023 - val_Sensitivity: 0.0419 - val_tn: 3829.0000 - val_auc: 0.7421 - val_prc: 0.3592\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4004 - Sensitivity: 0.0668 - tn: 11563.0000 - auc: 0.7254 - prc: 0.3554 - val_loss: 0.4023 - val_Sensitivity: 0.0381 - val_tn: 3830.0000 - val_auc: 0.7423 - val_prc: 0.3595\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3982 - Sensitivity: 0.0611 - tn: 11599.0000 - auc: 0.7289 - prc: 0.3655 - val_loss: 0.4024 - val_Sensitivity: 0.0356 - val_tn: 3831.0000 - val_auc: 0.7420 - val_prc: 0.3588\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3965 - Sensitivity: 0.0660 - tn: 11580.0000 - auc: 0.7312 - prc: 0.3679 - val_loss: 0.4022 - val_Sensitivity: 0.0368 - val_tn: 3831.0000 - val_auc: 0.7424 - val_prc: 0.3589\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3972 - Sensitivity: 0.0629 - tn: 11571.0000 - auc: 0.7298 - prc: 0.3678 - val_loss: 0.4020 - val_Sensitivity: 0.0432 - val_tn: 3828.0000 - val_auc: 0.7430 - val_prc: 0.3594\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3959 - Sensitivity: 0.0642 - tn: 11584.0000 - auc: 0.7331 - prc: 0.3715 - val_loss: 0.4020 - val_Sensitivity: 0.0432 - val_tn: 3828.0000 - val_auc: 0.7428 - val_prc: 0.3594\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3975 - Sensitivity: 0.0734 - tn: 11575.0000 - auc: 0.7323 - prc: 0.3701 - val_loss: 0.4020 - val_Sensitivity: 0.0419 - val_tn: 3828.0000 - val_auc: 0.7427 - val_prc: 0.3592\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3979 - Sensitivity: 0.0629 - tn: 11575.0000 - auc: 0.7289 - prc: 0.3617 - val_loss: 0.4021 - val_Sensitivity: 0.0394 - val_tn: 3829.0000 - val_auc: 0.7428 - val_prc: 0.3591\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3978 - Sensitivity: 0.0638 - tn: 11588.0000 - auc: 0.7310 - prc: 0.3661 - val_loss: 0.4021 - val_Sensitivity: 0.0394 - val_tn: 3829.0000 - val_auc: 0.7428 - val_prc: 0.3589\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3966 - Sensitivity: 0.0668 - tn: 11578.0000 - auc: 0.7288 - prc: 0.3716 - val_loss: 0.4019 - val_Sensitivity: 0.0419 - val_tn: 3828.0000 - val_auc: 0.7429 - val_prc: 0.3597\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3992 - Sensitivity: 0.0686 - tn: 11572.0000 - auc: 0.7278 - prc: 0.3644 - val_loss: 0.4020 - val_Sensitivity: 0.0407 - val_tn: 3828.0000 - val_auc: 0.7429 - val_prc: 0.3595\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3990 - Sensitivity: 0.0660 - tn: 11581.0000 - auc: 0.7262 - prc: 0.3686 - val_loss: 0.4017 - val_Sensitivity: 0.0419 - val_tn: 3828.0000 - val_auc: 0.7434 - val_prc: 0.3601\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3971 - Sensitivity: 0.0633 - tn: 11574.0000 - auc: 0.7306 - prc: 0.3646 - val_loss: 0.4018 - val_Sensitivity: 0.0432 - val_tn: 3828.0000 - val_auc: 0.7433 - val_prc: 0.3603\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3979 - Sensitivity: 0.0642 - tn: 11569.0000 - auc: 0.7306 - prc: 0.3571 - val_loss: 0.4018 - val_Sensitivity: 0.0407 - val_tn: 3830.0000 - val_auc: 0.7434 - val_prc: 0.3602\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3970 - Sensitivity: 0.0686 - tn: 11580.0000 - auc: 0.7316 - prc: 0.3688 - val_loss: 0.4016 - val_Sensitivity: 0.0407 - val_tn: 3830.0000 - val_auc: 0.7439 - val_prc: 0.3604\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3945 - Sensitivity: 0.0616 - tn: 11567.0000 - auc: 0.7358 - prc: 0.3775 - val_loss: 0.4017 - val_Sensitivity: 0.0407 - val_tn: 3829.0000 - val_auc: 0.7437 - val_prc: 0.3602\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3966 - Sensitivity: 0.0726 - tn: 11567.0000 - auc: 0.7324 - prc: 0.3680 - val_loss: 0.4016 - val_Sensitivity: 0.0457 - val_tn: 3825.0000 - val_auc: 0.7435 - val_prc: 0.3595\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3960 - Sensitivity: 0.0664 - tn: 11568.0000 - auc: 0.7350 - prc: 0.3714 - val_loss: 0.4015 - val_Sensitivity: 0.0445 - val_tn: 3827.0000 - val_auc: 0.7438 - val_prc: 0.3598\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3944 - Sensitivity: 0.0668 - tn: 11571.0000 - auc: 0.7369 - prc: 0.3737 - val_loss: 0.4017 - val_Sensitivity: 0.0394 - val_tn: 3829.0000 - val_auc: 0.7435 - val_prc: 0.3598\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3954 - Sensitivity: 0.0616 - tn: 11591.0000 - auc: 0.7346 - prc: 0.3687 - val_loss: 0.4016 - val_Sensitivity: 0.0419 - val_tn: 3828.0000 - val_auc: 0.7438 - val_prc: 0.3599\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3990 - Sensitivity: 0.0611 - tn: 11562.0000 - auc: 0.7300 - prc: 0.3595 - val_loss: 0.4016 - val_Sensitivity: 0.0419 - val_tn: 3826.0000 - val_auc: 0.7439 - val_prc: 0.3598\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3964 - Sensitivity: 0.0690 - tn: 11580.0000 - auc: 0.7321 - prc: 0.3737 - val_loss: 0.4015 - val_Sensitivity: 0.0432 - val_tn: 3827.0000 - val_auc: 0.7438 - val_prc: 0.3598\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3959 - Sensitivity: 0.0682 - tn: 11577.0000 - auc: 0.7336 - prc: 0.3705 - val_loss: 0.4015 - val_Sensitivity: 0.0407 - val_tn: 3827.0000 - val_auc: 0.7438 - val_prc: 0.3599\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0624 - tn: 11580.0000 - auc: 0.7391 - prc: 0.3754 - val_loss: 0.4017 - val_Sensitivity: 0.0394 - val_tn: 3830.0000 - val_auc: 0.7438 - val_prc: 0.3598\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3955 - Sensitivity: 0.0668 - tn: 11571.0000 - auc: 0.7347 - prc: 0.3732 - val_loss: 0.4017 - val_Sensitivity: 0.0419 - val_tn: 3828.0000 - val_auc: 0.7437 - val_prc: 0.3598\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3945 - Sensitivity: 0.0739 - tn: 11580.0000 - auc: 0.7357 - prc: 0.3768 - val_loss: 0.4016 - val_Sensitivity: 0.0432 - val_tn: 3826.0000 - val_auc: 0.7442 - val_prc: 0.3605\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3959 - Sensitivity: 0.0699 - tn: 11573.0000 - auc: 0.7329 - prc: 0.3664 - val_loss: 0.4015 - val_Sensitivity: 0.0445 - val_tn: 3827.0000 - val_auc: 0.7444 - val_prc: 0.3606\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3972 - Sensitivity: 0.0638 - tn: 11569.0000 - auc: 0.7314 - prc: 0.3676 - val_loss: 0.4015 - val_Sensitivity: 0.0432 - val_tn: 3827.0000 - val_auc: 0.7444 - val_prc: 0.3602\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3955 - Sensitivity: 0.0730 - tn: 11570.0000 - auc: 0.7339 - prc: 0.3675 - val_loss: 0.4016 - val_Sensitivity: 0.0407 - val_tn: 3830.0000 - val_auc: 0.7440 - val_prc: 0.3598\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3944 - Sensitivity: 0.0682 - tn: 11585.0000 - auc: 0.7361 - prc: 0.3782 - val_loss: 0.4015 - val_Sensitivity: 0.0407 - val_tn: 3829.0000 - val_auc: 0.7446 - val_prc: 0.3603\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0699 - tn: 11567.0000 - auc: 0.7387 - prc: 0.3767 - val_loss: 0.4014 - val_Sensitivity: 0.0407 - val_tn: 3829.0000 - val_auc: 0.7443 - val_prc: 0.3601\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3939 - Sensitivity: 0.0655 - tn: 11578.0000 - auc: 0.7386 - prc: 0.3714 - val_loss: 0.4014 - val_Sensitivity: 0.0419 - val_tn: 3829.0000 - val_auc: 0.7444 - val_prc: 0.3603\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3942 - Sensitivity: 0.0668 - tn: 11573.0000 - auc: 0.7354 - prc: 0.3744 - val_loss: 0.4013 - val_Sensitivity: 0.0457 - val_tn: 3828.0000 - val_auc: 0.7446 - val_prc: 0.3606\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3939 - Sensitivity: 0.0655 - tn: 11579.0000 - auc: 0.7391 - prc: 0.3748 - val_loss: 0.4012 - val_Sensitivity: 0.0470 - val_tn: 3827.0000 - val_auc: 0.7447 - val_prc: 0.3608\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3926 - Sensitivity: 0.0686 - tn: 11573.0000 - auc: 0.7402 - prc: 0.3795 - val_loss: 0.4013 - val_Sensitivity: 0.0508 - val_tn: 3825.0000 - val_auc: 0.7447 - val_prc: 0.3606\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3947 - Sensitivity: 0.0673 - tn: 11583.0000 - auc: 0.7367 - prc: 0.3790 - val_loss: 0.4012 - val_Sensitivity: 0.0534 - val_tn: 3824.0000 - val_auc: 0.7446 - val_prc: 0.3605\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3927 - Sensitivity: 0.0717 - tn: 11577.0000 - auc: 0.7404 - prc: 0.3817 - val_loss: 0.4013 - val_Sensitivity: 0.0508 - val_tn: 3824.0000 - val_auc: 0.7446 - val_prc: 0.3605\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0796 - tn: 11569.0000 - auc: 0.7423 - prc: 0.3806 - val_loss: 0.4013 - val_Sensitivity: 0.0521 - val_tn: 3825.0000 - val_auc: 0.7448 - val_prc: 0.3606\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3918 - Sensitivity: 0.0783 - tn: 11568.0000 - auc: 0.7412 - prc: 0.3804 - val_loss: 0.4011 - val_Sensitivity: 0.0534 - val_tn: 3822.0000 - val_auc: 0.7450 - val_prc: 0.3610\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3915 - Sensitivity: 0.0792 - tn: 11553.0000 - auc: 0.7437 - prc: 0.3849 - val_loss: 0.4014 - val_Sensitivity: 0.0521 - val_tn: 3825.0000 - val_auc: 0.7447 - val_prc: 0.3604\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0664 - tn: 11578.0000 - auc: 0.7413 - prc: 0.3786 - val_loss: 0.4012 - val_Sensitivity: 0.0534 - val_tn: 3825.0000 - val_auc: 0.7451 - val_prc: 0.3611\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3949 - Sensitivity: 0.0743 - tn: 11582.0000 - auc: 0.7357 - prc: 0.3818 - val_loss: 0.4013 - val_Sensitivity: 0.0483 - val_tn: 3827.0000 - val_auc: 0.7447 - val_prc: 0.3603\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3943 - Sensitivity: 0.0743 - tn: 11577.0000 - auc: 0.7356 - prc: 0.3749 - val_loss: 0.4014 - val_Sensitivity: 0.0483 - val_tn: 3824.0000 - val_auc: 0.7446 - val_prc: 0.3603\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3912 - Sensitivity: 0.0699 - tn: 11571.0000 - auc: 0.7430 - prc: 0.3826 - val_loss: 0.4013 - val_Sensitivity: 0.0508 - val_tn: 3825.0000 - val_auc: 0.7447 - val_prc: 0.3607\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0765 - tn: 11566.0000 - auc: 0.7422 - prc: 0.3809 - val_loss: 0.4014 - val_Sensitivity: 0.0496 - val_tn: 3826.0000 - val_auc: 0.7446 - val_prc: 0.3607\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3936 - Sensitivity: 0.0682 - tn: 11585.0000 - auc: 0.7379 - prc: 0.3795 - val_loss: 0.4015 - val_Sensitivity: 0.0483 - val_tn: 3828.0000 - val_auc: 0.7446 - val_prc: 0.3603\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3937 - Sensitivity: 0.0708 - tn: 11576.0000 - auc: 0.7374 - prc: 0.3790 - val_loss: 0.4015 - val_Sensitivity: 0.0496 - val_tn: 3825.0000 - val_auc: 0.7442 - val_prc: 0.3604\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3940 - Sensitivity: 0.0734 - tn: 11571.0000 - auc: 0.7374 - prc: 0.3794 - val_loss: 0.4015 - val_Sensitivity: 0.0496 - val_tn: 3826.0000 - val_auc: 0.7444 - val_prc: 0.3600\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3907 - Sensitivity: 0.0748 - tn: 11576.0000 - auc: 0.7436 - prc: 0.3892 - val_loss: 0.4015 - val_Sensitivity: 0.0534 - val_tn: 3823.0000 - val_auc: 0.7444 - val_prc: 0.3599\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3942 - Sensitivity: 0.0730 - tn: 11559.0000 - auc: 0.7365 - prc: 0.3756 - val_loss: 0.4016 - val_Sensitivity: 0.0521 - val_tn: 3824.0000 - val_auc: 0.7441 - val_prc: 0.3596\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3911 - Sensitivity: 0.0660 - tn: 11567.0000 - auc: 0.7431 - prc: 0.3833 - val_loss: 0.4017 - val_Sensitivity: 0.0508 - val_tn: 3824.0000 - val_auc: 0.7442 - val_prc: 0.3600\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3940 - Sensitivity: 0.0717 - tn: 11559.0000 - auc: 0.7375 - prc: 0.3761 - val_loss: 0.4014 - val_Sensitivity: 0.0521 - val_tn: 3824.0000 - val_auc: 0.7444 - val_prc: 0.3603\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3901 - Sensitivity: 0.0668 - tn: 11577.0000 - auc: 0.7465 - prc: 0.3806 - val_loss: 0.4013 - val_Sensitivity: 0.0572 - val_tn: 3822.0000 - val_auc: 0.7445 - val_prc: 0.3600\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3899 - Sensitivity: 0.0712 - tn: 11564.0000 - auc: 0.7488 - prc: 0.3853 - val_loss: 0.4014 - val_Sensitivity: 0.0534 - val_tn: 3822.0000 - val_auc: 0.7447 - val_prc: 0.3600\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3923 - Sensitivity: 0.0682 - tn: 11568.0000 - auc: 0.7421 - prc: 0.3813 - val_loss: 0.4014 - val_Sensitivity: 0.0508 - val_tn: 3826.0000 - val_auc: 0.7444 - val_prc: 0.3599\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3904 - Sensitivity: 0.0774 - tn: 11570.0000 - auc: 0.7425 - prc: 0.3860 - val_loss: 0.4011 - val_Sensitivity: 0.0534 - val_tn: 3823.0000 - val_auc: 0.7447 - val_prc: 0.3603\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3906 - Sensitivity: 0.0818 - tn: 11557.0000 - auc: 0.7443 - prc: 0.3883 - val_loss: 0.4011 - val_Sensitivity: 0.0546 - val_tn: 3825.0000 - val_auc: 0.7448 - val_prc: 0.3605\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0739 - tn: 11575.0000 - auc: 0.7419 - prc: 0.3812 - val_loss: 0.4013 - val_Sensitivity: 0.0559 - val_tn: 3825.0000 - val_auc: 0.7448 - val_prc: 0.3606\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3919 - Sensitivity: 0.0783 - tn: 11576.0000 - auc: 0.7396 - prc: 0.3849 - val_loss: 0.4010 - val_Sensitivity: 0.0559 - val_tn: 3823.0000 - val_auc: 0.7452 - val_prc: 0.3607\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3899 - Sensitivity: 0.0796 - tn: 11558.0000 - auc: 0.7450 - prc: 0.3916 - val_loss: 0.4011 - val_Sensitivity: 0.0559 - val_tn: 3822.0000 - val_auc: 0.7453 - val_prc: 0.3609\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3892 - Sensitivity: 0.0822 - tn: 11571.0000 - auc: 0.7462 - prc: 0.3921 - val_loss: 0.4009 - val_Sensitivity: 0.0559 - val_tn: 3821.0000 - val_auc: 0.7457 - val_prc: 0.3610\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3913 - Sensitivity: 0.0739 - tn: 11562.0000 - auc: 0.7437 - prc: 0.3821 - val_loss: 0.4011 - val_Sensitivity: 0.0546 - val_tn: 3823.0000 - val_auc: 0.7450 - val_prc: 0.3602\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3913 - Sensitivity: 0.0787 - tn: 11571.0000 - auc: 0.7428 - prc: 0.3829 - val_loss: 0.4009 - val_Sensitivity: 0.0559 - val_tn: 3822.0000 - val_auc: 0.7454 - val_prc: 0.3607\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3899 - Sensitivity: 0.0712 - tn: 11573.0000 - auc: 0.7454 - prc: 0.3845 - val_loss: 0.4012 - val_Sensitivity: 0.0521 - val_tn: 3826.0000 - val_auc: 0.7451 - val_prc: 0.3601\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3888 - Sensitivity: 0.0695 - tn: 11569.0000 - auc: 0.7490 - prc: 0.3797 - val_loss: 0.4009 - val_Sensitivity: 0.0521 - val_tn: 3823.0000 - val_auc: 0.7452 - val_prc: 0.3604\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3881 - Sensitivity: 0.0849 - tn: 11564.0000 - auc: 0.7470 - prc: 0.3928 - val_loss: 0.4009 - val_Sensitivity: 0.0584 - val_tn: 3820.0000 - val_auc: 0.7454 - val_prc: 0.3605\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3908 - Sensitivity: 0.0783 - tn: 11554.0000 - auc: 0.7444 - prc: 0.3827 - val_loss: 0.4009 - val_Sensitivity: 0.0584 - val_tn: 3820.0000 - val_auc: 0.7452 - val_prc: 0.3602\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3910 - Sensitivity: 0.0783 - tn: 11546.0000 - auc: 0.7437 - prc: 0.3858 - val_loss: 0.4011 - val_Sensitivity: 0.0559 - val_tn: 3822.0000 - val_auc: 0.7450 - val_prc: 0.3601\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3886 - Sensitivity: 0.0796 - tn: 11576.0000 - auc: 0.7477 - prc: 0.3901 - val_loss: 0.4011 - val_Sensitivity: 0.0559 - val_tn: 3821.0000 - val_auc: 0.7452 - val_prc: 0.3596\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3882 - Sensitivity: 0.0822 - tn: 11583.0000 - auc: 0.7463 - prc: 0.4011 - val_loss: 0.4012 - val_Sensitivity: 0.0610 - val_tn: 3820.0000 - val_auc: 0.7452 - val_prc: 0.3601\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3916 - Sensitivity: 0.0831 - tn: 11544.0000 - auc: 0.7438 - prc: 0.3857 - val_loss: 0.4011 - val_Sensitivity: 0.0635 - val_tn: 3820.0000 - val_auc: 0.7452 - val_prc: 0.3597\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3885 - Sensitivity: 0.0800 - tn: 11551.0000 - auc: 0.7502 - prc: 0.3918 - val_loss: 0.4011 - val_Sensitivity: 0.0584 - val_tn: 3820.0000 - val_auc: 0.7452 - val_prc: 0.3601\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0734 - tn: 11562.0000 - auc: 0.7450 - prc: 0.3830 - val_loss: 0.4013 - val_Sensitivity: 0.0534 - val_tn: 3821.0000 - val_auc: 0.7448 - val_prc: 0.3595\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3894 - Sensitivity: 0.0743 - tn: 11575.0000 - auc: 0.7479 - prc: 0.3861 - val_loss: 0.4013 - val_Sensitivity: 0.0534 - val_tn: 3822.0000 - val_auc: 0.7446 - val_prc: 0.3592\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3909 - Sensitivity: 0.0730 - tn: 11572.0000 - auc: 0.7437 - prc: 0.3864 - val_loss: 0.4013 - val_Sensitivity: 0.0546 - val_tn: 3820.0000 - val_auc: 0.7448 - val_prc: 0.3591\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3898 - Sensitivity: 0.0712 - tn: 11568.0000 - auc: 0.7447 - prc: 0.3896 - val_loss: 0.4012 - val_Sensitivity: 0.0597 - val_tn: 3820.0000 - val_auc: 0.7450 - val_prc: 0.3593\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3887 - Sensitivity: 0.0844 - tn: 11567.0000 - auc: 0.7473 - prc: 0.3911 - val_loss: 0.4010 - val_Sensitivity: 0.0597 - val_tn: 3819.0000 - val_auc: 0.7453 - val_prc: 0.3600\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3898 - Sensitivity: 0.0849 - tn: 11557.0000 - auc: 0.7460 - prc: 0.3884 - val_loss: 0.4010 - val_Sensitivity: 0.0610 - val_tn: 3820.0000 - val_auc: 0.7454 - val_prc: 0.3602\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3901 - Sensitivity: 0.0792 - tn: 11555.0000 - auc: 0.7465 - prc: 0.3828 - val_loss: 0.4010 - val_Sensitivity: 0.0521 - val_tn: 3824.0000 - val_auc: 0.7451 - val_prc: 0.3600\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3870 - Sensitivity: 0.0805 - tn: 11574.0000 - auc: 0.7503 - prc: 0.3965 - val_loss: 0.4010 - val_Sensitivity: 0.0559 - val_tn: 3823.0000 - val_auc: 0.7449 - val_prc: 0.3600\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3896 - Sensitivity: 0.0818 - tn: 11569.0000 - auc: 0.7442 - prc: 0.3888 - val_loss: 0.4009 - val_Sensitivity: 0.0572 - val_tn: 3822.0000 - val_auc: 0.7456 - val_prc: 0.3605\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3908 - Sensitivity: 0.0836 - tn: 11559.0000 - auc: 0.7441 - prc: 0.3869 - val_loss: 0.4009 - val_Sensitivity: 0.0559 - val_tn: 3821.0000 - val_auc: 0.7453 - val_prc: 0.3603\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3893 - Sensitivity: 0.0796 - tn: 11556.0000 - auc: 0.7480 - prc: 0.3846 - val_loss: 0.4008 - val_Sensitivity: 0.0559 - val_tn: 3822.0000 - val_auc: 0.7454 - val_prc: 0.3606\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3904 - Sensitivity: 0.0739 - tn: 11556.0000 - auc: 0.7466 - prc: 0.3791 - val_loss: 0.4008 - val_Sensitivity: 0.0546 - val_tn: 3824.0000 - val_auc: 0.7455 - val_prc: 0.3603\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3871 - Sensitivity: 0.0853 - tn: 11566.0000 - auc: 0.7505 - prc: 0.3994 - val_loss: 0.4010 - val_Sensitivity: 0.0559 - val_tn: 3821.0000 - val_auc: 0.7451 - val_prc: 0.3597\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3907 - Sensitivity: 0.0814 - tn: 11577.0000 - auc: 0.7430 - prc: 0.3933 - val_loss: 0.4010 - val_Sensitivity: 0.0546 - val_tn: 3822.0000 - val_auc: 0.7452 - val_prc: 0.3595\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3883 - Sensitivity: 0.0761 - tn: 11558.0000 - auc: 0.7499 - prc: 0.3932 - val_loss: 0.4010 - val_Sensitivity: 0.0559 - val_tn: 3821.0000 - val_auc: 0.7452 - val_prc: 0.3598\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3880 - Sensitivity: 0.0765 - tn: 11570.0000 - auc: 0.7492 - prc: 0.3940 - val_loss: 0.4011 - val_Sensitivity: 0.0534 - val_tn: 3823.0000 - val_auc: 0.7451 - val_prc: 0.3596\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3910 - Sensitivity: 0.0761 - tn: 11571.0000 - auc: 0.7413 - prc: 0.3886 - val_loss: 0.4010 - val_Sensitivity: 0.0559 - val_tn: 3821.0000 - val_auc: 0.7450 - val_prc: 0.3598\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3860 - Sensitivity: 0.0822 - tn: 11564.0000 - auc: 0.7538 - prc: 0.3936 - val_loss: 0.4011 - val_Sensitivity: 0.0572 - val_tn: 3822.0000 - val_auc: 0.7452 - val_prc: 0.3599\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3864 - Sensitivity: 0.0866 - tn: 11569.0000 - auc: 0.7522 - prc: 0.3955 - val_loss: 0.4010 - val_Sensitivity: 0.0597 - val_tn: 3821.0000 - val_auc: 0.7455 - val_prc: 0.3607\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3849 - Sensitivity: 0.0866 - tn: 11567.0000 - auc: 0.7553 - prc: 0.4025 - val_loss: 0.4010 - val_Sensitivity: 0.0559 - val_tn: 3822.0000 - val_auc: 0.7456 - val_prc: 0.3608\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3865 - Sensitivity: 0.0836 - tn: 11571.0000 - auc: 0.7510 - prc: 0.3983 - val_loss: 0.4011 - val_Sensitivity: 0.0610 - val_tn: 3821.0000 - val_auc: 0.7455 - val_prc: 0.3608\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3866 - Sensitivity: 0.0910 - tn: 11562.0000 - auc: 0.7501 - prc: 0.3996 - val_loss: 0.4011 - val_Sensitivity: 0.0623 - val_tn: 3820.0000 - val_auc: 0.7458 - val_prc: 0.3607\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3863 - Sensitivity: 0.0941 - tn: 11554.0000 - auc: 0.7507 - prc: 0.3964 - val_loss: 0.4010 - val_Sensitivity: 0.0648 - val_tn: 3819.0000 - val_auc: 0.7457 - val_prc: 0.3605\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3850 - Sensitivity: 0.0884 - tn: 11549.0000 - auc: 0.7555 - prc: 0.3954 - val_loss: 0.4011 - val_Sensitivity: 0.0673 - val_tn: 3818.0000 - val_auc: 0.7454 - val_prc: 0.3602\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3894 - Sensitivity: 0.0840 - tn: 11550.0000 - auc: 0.7466 - prc: 0.3908 - val_loss: 0.4009 - val_Sensitivity: 0.0648 - val_tn: 3820.0000 - val_auc: 0.7458 - val_prc: 0.3603\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3852 - Sensitivity: 0.0791 - tn: 11481.0000 - auc: 0.7543 - prc: 0.40 - 0s 11ms/step - loss: 0.3855 - Sensitivity: 0.0805 - tn: 11571.0000 - auc: 0.7547 - prc: 0.4019 - val_loss: 0.4009 - val_Sensitivity: 0.0661 - val_tn: 3819.0000 - val_auc: 0.7453 - val_prc: 0.3601\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3890 - Sensitivity: 0.0880 - tn: 11547.0000 - auc: 0.7475 - prc: 0.3947 - val_loss: 0.4009 - val_Sensitivity: 0.0661 - val_tn: 3818.0000 - val_auc: 0.7458 - val_prc: 0.3604\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3854 - Sensitivity: 0.0950 - tn: 11539.0000 - auc: 0.7544 - prc: 0.3978 - val_loss: 0.4007 - val_Sensitivity: 0.0661 - val_tn: 3818.0000 - val_auc: 0.7463 - val_prc: 0.3608\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3873 - Sensitivity: 0.0919 - tn: 11548.0000 - auc: 0.7516 - prc: 0.3976 - val_loss: 0.4007 - val_Sensitivity: 0.0648 - val_tn: 3819.0000 - val_auc: 0.7463 - val_prc: 0.3610\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3845 - Sensitivity: 0.0862 - tn: 11563.0000 - auc: 0.7550 - prc: 0.4051 - val_loss: 0.4009 - val_Sensitivity: 0.0661 - val_tn: 3820.0000 - val_auc: 0.7459 - val_prc: 0.3607\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3850 - Sensitivity: 0.0844 - tn: 11558.0000 - auc: 0.7566 - prc: 0.3973 - val_loss: 0.4009 - val_Sensitivity: 0.0610 - val_tn: 3821.0000 - val_auc: 0.7459 - val_prc: 0.3605\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3832 - Sensitivity: 0.0871 - tn: 11560.0000 - auc: 0.7581 - prc: 0.4021 - val_loss: 0.4011 - val_Sensitivity: 0.0635 - val_tn: 3820.0000 - val_auc: 0.7456 - val_prc: 0.3601\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3888 - Sensitivity: 0.0774 - tn: 11552.0000 - auc: 0.7472 - prc: 0.3896 - val_loss: 0.4012 - val_Sensitivity: 0.0648 - val_tn: 3819.0000 - val_auc: 0.7456 - val_prc: 0.3604\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3868 - Sensitivity: 0.0853 - tn: 11554.0000 - auc: 0.7510 - prc: 0.3954 - val_loss: 0.4012 - val_Sensitivity: 0.0648 - val_tn: 3819.0000 - val_auc: 0.7454 - val_prc: 0.3601\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3847 - Sensitivity: 0.0954 - tn: 11551.0000 - auc: 0.7549 - prc: 0.4038 - val_loss: 0.4011 - val_Sensitivity: 0.0686 - val_tn: 3817.0000 - val_auc: 0.7453 - val_prc: 0.3603\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3846 - Sensitivity: 0.0972 - tn: 11552.0000 - auc: 0.7539 - prc: 0.4019 - val_loss: 0.4012 - val_Sensitivity: 0.0673 - val_tn: 3817.0000 - val_auc: 0.7458 - val_prc: 0.3603\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3848 - Sensitivity: 0.1020 - tn: 11555.0000 - auc: 0.7543 - prc: 0.4054 - val_loss: 0.4013 - val_Sensitivity: 0.0661 - val_tn: 3818.0000 - val_auc: 0.7454 - val_prc: 0.3599\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3852 - Sensitivity: 0.0831 - tn: 11544.0000 - auc: 0.7557 - prc: 0.3916 - val_loss: 0.4011 - val_Sensitivity: 0.0661 - val_tn: 3818.0000 - val_auc: 0.7456 - val_prc: 0.3604\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3859 - Sensitivity: 0.0910 - tn: 11569.0000 - auc: 0.7536 - prc: 0.4041 - val_loss: 0.4011 - val_Sensitivity: 0.0623 - val_tn: 3821.0000 - val_auc: 0.7456 - val_prc: 0.3604\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3860 - Sensitivity: 0.0888 - tn: 11557.0000 - auc: 0.7526 - prc: 0.3971 - val_loss: 0.4009 - val_Sensitivity: 0.0623 - val_tn: 3820.0000 - val_auc: 0.7454 - val_prc: 0.3601\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3856 - Sensitivity: 0.0915 - tn: 11555.0000 - auc: 0.7525 - prc: 0.4040 - val_loss: 0.4010 - val_Sensitivity: 0.0648 - val_tn: 3820.0000 - val_auc: 0.7454 - val_prc: 0.3603\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3845 - Sensitivity: 0.0928 - tn: 11564.0000 - auc: 0.7553 - prc: 0.4003 - val_loss: 0.4012 - val_Sensitivity: 0.0623 - val_tn: 3821.0000 - val_auc: 0.7451 - val_prc: 0.3599\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3846 - Sensitivity: 0.0853 - tn: 11562.0000 - auc: 0.7554 - prc: 0.3991 - val_loss: 0.4010 - val_Sensitivity: 0.0648 - val_tn: 3819.0000 - val_auc: 0.7454 - val_prc: 0.3601\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3834 - Sensitivity: 0.0932 - tn: 11551.0000 - auc: 0.7570 - prc: 0.4028 - val_loss: 0.4010 - val_Sensitivity: 0.0673 - val_tn: 3818.0000 - val_auc: 0.7455 - val_prc: 0.3608\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3854 - Sensitivity: 0.0954 - tn: 11555.0000 - auc: 0.7534 - prc: 0.4006 - val_loss: 0.4009 - val_Sensitivity: 0.0686 - val_tn: 3819.0000 - val_auc: 0.7457 - val_prc: 0.3611\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3837 - Sensitivity: 0.0959 - tn: 11554.0000 - auc: 0.7563 - prc: 0.4032 - val_loss: 0.4009 - val_Sensitivity: 0.0673 - val_tn: 3818.0000 - val_auc: 0.7453 - val_prc: 0.3610\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3855 - Sensitivity: 0.0910 - tn: 11551.0000 - auc: 0.7540 - prc: 0.4021 - val_loss: 0.4008 - val_Sensitivity: 0.0673 - val_tn: 3818.0000 - val_auc: 0.7456 - val_prc: 0.3608\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3846 - Sensitivity: 0.0945 - tn: 11542.0000 - auc: 0.7557 - prc: 0.4024 - val_loss: 0.4008 - val_Sensitivity: 0.0699 - val_tn: 3815.0000 - val_auc: 0.7455 - val_prc: 0.3607\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3862 - Sensitivity: 0.0910 - tn: 11555.0000 - auc: 0.7523 - prc: 0.4019 - val_loss: 0.4010 - val_Sensitivity: 0.0686 - val_tn: 3816.0000 - val_auc: 0.7455 - val_prc: 0.3603\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3846 - Sensitivity: 0.0937 - tn: 11533.0000 - auc: 0.7562 - prc: 0.3956 - val_loss: 0.4009 - val_Sensitivity: 0.0661 - val_tn: 3816.0000 - val_auc: 0.7462 - val_prc: 0.3607\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3850 - Sensitivity: 0.0901 - tn: 11553.0000 - auc: 0.7565 - prc: 0.3978 - val_loss: 0.4009 - val_Sensitivity: 0.0661 - val_tn: 3819.0000 - val_auc: 0.7458 - val_prc: 0.3611\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3840 - Sensitivity: 0.0972 - tn: 11554.0000 - auc: 0.7569 - prc: 0.4031 - val_loss: 0.4007 - val_Sensitivity: 0.0635 - val_tn: 3821.0000 - val_auc: 0.7461 - val_prc: 0.3611\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3856 - Sensitivity: 0.1029 - tn: 11555.0000 - auc: 0.7526 - prc: 0.4037 - val_loss: 0.4006 - val_Sensitivity: 0.0673 - val_tn: 3816.0000 - val_auc: 0.7462 - val_prc: 0.3611\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3846 - Sensitivity: 0.0932 - tn: 11552.0000 - auc: 0.7572 - prc: 0.4015 - val_loss: 0.4008 - val_Sensitivity: 0.0623 - val_tn: 3820.0000 - val_auc: 0.7458 - val_prc: 0.3608\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3866 - Sensitivity: 0.0858 - tn: 11563.0000 - auc: 0.7515 - prc: 0.3972 - val_loss: 0.4010 - val_Sensitivity: 0.0610 - val_tn: 3820.0000 - val_auc: 0.7460 - val_prc: 0.3605\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3838 - Sensitivity: 0.0893 - tn: 11560.0000 - auc: 0.7567 - prc: 0.4038 - val_loss: 0.4008 - val_Sensitivity: 0.0648 - val_tn: 3819.0000 - val_auc: 0.7462 - val_prc: 0.3606\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3840 - Sensitivity: 0.0866 - tn: 11557.0000 - auc: 0.7568 - prc: 0.4032 - val_loss: 0.4008 - val_Sensitivity: 0.0610 - val_tn: 3820.0000 - val_auc: 0.7461 - val_prc: 0.3603\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3851 - Sensitivity: 0.0871 - tn: 11556.0000 - auc: 0.7546 - prc: 0.4002 - val_loss: 0.4007 - val_Sensitivity: 0.0686 - val_tn: 3819.0000 - val_auc: 0.7464 - val_prc: 0.3608\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3810 - Sensitivity: 0.0959 - tn: 11551.0000 - auc: 0.7633 - prc: 0.4109 - val_loss: 0.4008 - val_Sensitivity: 0.0699 - val_tn: 3814.0000 - val_auc: 0.7461 - val_prc: 0.3602\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3864 - Sensitivity: 0.0901 - tn: 11542.0000 - auc: 0.7530 - prc: 0.3981 - val_loss: 0.4009 - val_Sensitivity: 0.0699 - val_tn: 3817.0000 - val_auc: 0.7464 - val_prc: 0.3605\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3847 - Sensitivity: 0.0981 - tn: 11537.0000 - auc: 0.7562 - prc: 0.3976 - val_loss: 0.4007 - val_Sensitivity: 0.0699 - val_tn: 3813.0000 - val_auc: 0.7464 - val_prc: 0.3609\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3837 - Sensitivity: 0.1003 - tn: 11545.0000 - auc: 0.7563 - prc: 0.4064 - val_loss: 0.4007 - val_Sensitivity: 0.0699 - val_tn: 3812.0000 - val_auc: 0.7463 - val_prc: 0.3610\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3818 - Sensitivity: 0.0919 - tn: 11561.0000 - auc: 0.7619 - prc: 0.4091 - val_loss: 0.4009 - val_Sensitivity: 0.0712 - val_tn: 3815.0000 - val_auc: 0.7460 - val_prc: 0.3603\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3855 - Sensitivity: 0.0932 - tn: 11551.0000 - auc: 0.7534 - prc: 0.4068 - val_loss: 0.4009 - val_Sensitivity: 0.0686 - val_tn: 3818.0000 - val_auc: 0.7463 - val_prc: 0.3604\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3803 - Sensitivity: 0.0954 - tn: 11548.0000 - auc: 0.7641 - prc: 0.4118 - val_loss: 0.4009 - val_Sensitivity: 0.0712 - val_tn: 3812.0000 - val_auc: 0.7461 - val_prc: 0.3605\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3824 - Sensitivity: 0.1033 - tn: 11535.0000 - auc: 0.7592 - prc: 0.4128 - val_loss: 0.4009 - val_Sensitivity: 0.0724 - val_tn: 3810.0000 - val_auc: 0.7459 - val_prc: 0.3609\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3838 - Sensitivity: 0.0945 - tn: 11532.0000 - auc: 0.7580 - prc: 0.3990 - val_loss: 0.4010 - val_Sensitivity: 0.0699 - val_tn: 3814.0000 - val_auc: 0.7457 - val_prc: 0.3603\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3831 - Sensitivity: 0.1007 - tn: 11543.0000 - auc: 0.7577 - prc: 0.4093 - val_loss: 0.4006 - val_Sensitivity: 0.0737 - val_tn: 3812.0000 - val_auc: 0.7464 - val_prc: 0.3611\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3832 - Sensitivity: 0.1117 - tn: 11531.0000 - auc: 0.7580 - prc: 0.4105 - val_loss: 0.4008 - val_Sensitivity: 0.0724 - val_tn: 3813.0000 - val_auc: 0.7460 - val_prc: 0.3609\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3823 - Sensitivity: 0.0954 - tn: 11531.0000 - auc: 0.7607 - prc: 0.4067 - val_loss: 0.4008 - val_Sensitivity: 0.0737 - val_tn: 3810.0000 - val_auc: 0.7460 - val_prc: 0.3606\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3832 - Sensitivity: 0.0932 - tn: 11525.0000 - auc: 0.7583 - prc: 0.4065 - val_loss: 0.4009 - val_Sensitivity: 0.0788 - val_tn: 3809.0000 - val_auc: 0.7459 - val_prc: 0.3606\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3821 - Sensitivity: 0.0932 - tn: 11553.0000 - auc: 0.7604 - prc: 0.4101 - val_loss: 0.4009 - val_Sensitivity: 0.0712 - val_tn: 3813.0000 - val_auc: 0.7461 - val_prc: 0.3607\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3799 - Sensitivity: 0.1073 - tn: 11552.0000 - auc: 0.7637 - prc: 0.4192 - val_loss: 0.4010 - val_Sensitivity: 0.0699 - val_tn: 3814.0000 - val_auc: 0.7460 - val_prc: 0.3610\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3821 - Sensitivity: 0.1042 - tn: 11538.0000 - auc: 0.7591 - prc: 0.4100 - val_loss: 0.4012 - val_Sensitivity: 0.0712 - val_tn: 3813.0000 - val_auc: 0.7457 - val_prc: 0.3606\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3818 - Sensitivity: 0.1025 - tn: 11547.0000 - auc: 0.7597 - prc: 0.4111 - val_loss: 0.4011 - val_Sensitivity: 0.0724 - val_tn: 3812.0000 - val_auc: 0.7457 - val_prc: 0.3604\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3839 - Sensitivity: 0.0888 - tn: 11539.0000 - auc: 0.7588 - prc: 0.4021 - val_loss: 0.4011 - val_Sensitivity: 0.0724 - val_tn: 3813.0000 - val_auc: 0.7456 - val_prc: 0.3604\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3822 - Sensitivity: 0.1077 - tn: 11541.0000 - auc: 0.7598 - prc: 0.4105 - val_loss: 0.4009 - val_Sensitivity: 0.0737 - val_tn: 3811.0000 - val_auc: 0.7458 - val_prc: 0.3607\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3828 - Sensitivity: 0.0998 - tn: 11545.0000 - auc: 0.7583 - prc: 0.4107 - val_loss: 0.4012 - val_Sensitivity: 0.0724 - val_tn: 3813.0000 - val_auc: 0.7455 - val_prc: 0.3601\n",
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 28ms/step - loss: 1.0110 - Sensitivity: 0.4985 - tn: 6072.0000 - auc: 0.5084 - prc: 0.1661 - val_loss: 0.4700 - val_Sensitivity: 0.0025 - val_tn: 3840.0000 - val_auc: 0.5581 - val_prc: 0.2019\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.8077 - Sensitivity: 0.3830 - tn: 7736.0000 - auc: 0.5322 - prc: 0.1807 - val_loss: 0.4547 - val_Sensitivity: 0.0000e+00 - val_tn: 3847.0000 - val_auc: 0.6066 - val_prc: 0.2303\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6979 - Sensitivity: 0.3109 - tn: 8853.0000 - auc: 0.5510 - prc: 0.1904 - val_loss: 0.4487 - val_Sensitivity: 0.0012 - val_tn: 3847.0000 - val_auc: 0.6295 - val_prc: 0.2485\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.6417 - Sensitivity: 0.2433 - tn: 9551.0000 - auc: 0.5595 - prc: 0.1920 - val_loss: 0.4469 - val_Sensitivity: 0.0012 - val_tn: 3847.0000 - val_auc: 0.6453 - val_prc: 0.2596\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.6146 - Sensitivity: 0.2185 - tn: 9918.0000 - auc: 0.5669 - prc: 0.2029 - val_loss: 0.4450 - val_Sensitivity: 0.0012 - val_tn: 3847.0000 - val_auc: 0.6592 - val_prc: 0.2731\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5967 - Sensitivity: 0.1866 - tn: 10226.0000 - auc: 0.5769 - prc: 0.2041 - val_loss: 0.4424 - val_Sensitivity: 0.0012 - val_tn: 3847.0000 - val_auc: 0.6704 - val_prc: 0.2859\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5782 - Sensitivity: 0.1875 - tn: 10337.0000 - auc: 0.5843 - prc: 0.2120 - val_loss: 0.4407 - val_Sensitivity: 0.0012 - val_tn: 3846.0000 - val_auc: 0.6785 - val_prc: 0.3001\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5622 - Sensitivity: 0.1822 - tn: 10495.0000 - auc: 0.5992 - prc: 0.2233 - val_loss: 0.4402 - val_Sensitivity: 0.0025 - val_tn: 3844.0000 - val_auc: 0.6832 - val_prc: 0.3128\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5417 - Sensitivity: 0.1636 - tn: 10601.0000 - auc: 0.6134 - prc: 0.2235 - val_loss: 0.4388 - val_Sensitivity: 0.0075 - val_tn: 3842.0000 - val_auc: 0.6877 - val_prc: 0.3242\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5373 - Sensitivity: 0.1800 - tn: 10661.0000 - auc: 0.6163 - prc: 0.2330 - val_loss: 0.4387 - val_Sensitivity: 0.0150 - val_tn: 3842.0000 - val_auc: 0.6899 - val_prc: 0.3301\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5425 - Sensitivity: 0.1703 - tn: 10646.0000 - auc: 0.6091 - prc: 0.2297 - val_loss: 0.4380 - val_Sensitivity: 0.0213 - val_tn: 3838.0000 - val_auc: 0.6916 - val_prc: 0.3347\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5306 - Sensitivity: 0.1765 - tn: 10706.0000 - auc: 0.6203 - prc: 0.2441 - val_loss: 0.4377 - val_Sensitivity: 0.0250 - val_tn: 3837.0000 - val_auc: 0.6933 - val_prc: 0.3368\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5135 - Sensitivity: 0.1729 - tn: 10789.0000 - auc: 0.6294 - prc: 0.2468 - val_loss: 0.4377 - val_Sensitivity: 0.0275 - val_tn: 3835.0000 - val_auc: 0.6935 - val_prc: 0.3381\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5164 - Sensitivity: 0.1636 - tn: 10798.0000 - auc: 0.6254 - prc: 0.2465 - val_loss: 0.4364 - val_Sensitivity: 0.0350 - val_tn: 3832.0000 - val_auc: 0.6947 - val_prc: 0.3410\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.5030 - Sensitivity: 0.1552 - tn: 10856.0000 - auc: 0.6366 - prc: 0.2505 - val_loss: 0.4356 - val_Sensitivity: 0.0350 - val_tn: 3829.0000 - val_auc: 0.6951 - val_prc: 0.3422\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4959 - Sensitivity: 0.1628 - tn: 10867.0000 - auc: 0.6406 - prc: 0.2517 - val_loss: 0.4350 - val_Sensitivity: 0.0338 - val_tn: 3827.0000 - val_auc: 0.6958 - val_prc: 0.3437\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4889 - Sensitivity: 0.1720 - tn: 10914.0000 - auc: 0.6476 - prc: 0.2635 - val_loss: 0.4352 - val_Sensitivity: 0.0388 - val_tn: 3827.0000 - val_auc: 0.6955 - val_prc: 0.3446\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4831 - Sensitivity: 0.1681 - tn: 10930.0000 - auc: 0.6489 - prc: 0.2626 - val_loss: 0.4356 - val_Sensitivity: 0.0350 - val_tn: 3829.0000 - val_auc: 0.6956 - val_prc: 0.3451\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4799 - Sensitivity: 0.1375 - tn: 11016.0000 - auc: 0.6515 - prc: 0.2600 - val_loss: 0.4345 - val_Sensitivity: 0.0362 - val_tn: 3827.0000 - val_auc: 0.6966 - val_prc: 0.3462\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4760 - Sensitivity: 0.1460 - tn: 11006.0000 - auc: 0.6511 - prc: 0.2607 - val_loss: 0.4331 - val_Sensitivity: 0.0437 - val_tn: 3826.0000 - val_auc: 0.6962 - val_prc: 0.3463\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4738 - Sensitivity: 0.1442 - tn: 11025.0000 - auc: 0.6487 - prc: 0.2635 - val_loss: 0.4325 - val_Sensitivity: 0.0413 - val_tn: 3826.0000 - val_auc: 0.6965 - val_prc: 0.3464\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4646 - Sensitivity: 0.1508 - tn: 11042.0000 - auc: 0.6627 - prc: 0.2707 - val_loss: 0.4331 - val_Sensitivity: 0.0400 - val_tn: 3827.0000 - val_auc: 0.6965 - val_prc: 0.3469\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4648 - Sensitivity: 0.1424 - tn: 11087.0000 - auc: 0.6557 - prc: 0.2703 - val_loss: 0.4325 - val_Sensitivity: 0.0400 - val_tn: 3827.0000 - val_auc: 0.6970 - val_prc: 0.3474\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4572 - Sensitivity: 0.1296 - tn: 11163.0000 - auc: 0.6639 - prc: 0.2768 - val_loss: 0.4319 - val_Sensitivity: 0.0388 - val_tn: 3829.0000 - val_auc: 0.6977 - val_prc: 0.3482\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4503 - Sensitivity: 0.1331 - tn: 11145.0000 - auc: 0.6705 - prc: 0.2839 - val_loss: 0.4316 - val_Sensitivity: 0.0388 - val_tn: 3829.0000 - val_auc: 0.6984 - val_prc: 0.3489\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4541 - Sensitivity: 0.1256 - tn: 11199.0000 - auc: 0.6641 - prc: 0.2779 - val_loss: 0.4312 - val_Sensitivity: 0.0388 - val_tn: 3827.0000 - val_auc: 0.6988 - val_prc: 0.3495\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4479 - Sensitivity: 0.1238 - tn: 11225.0000 - auc: 0.6706 - prc: 0.2857 - val_loss: 0.4305 - val_Sensitivity: 0.0400 - val_tn: 3829.0000 - val_auc: 0.6994 - val_prc: 0.3500\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4463 - Sensitivity: 0.1230 - tn: 11216.0000 - auc: 0.6706 - prc: 0.2850 - val_loss: 0.4292 - val_Sensitivity: 0.0413 - val_tn: 3828.0000 - val_auc: 0.6993 - val_prc: 0.3495\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4403 - Sensitivity: 0.1238 - tn: 11269.0000 - auc: 0.6776 - prc: 0.2977 - val_loss: 0.4295 - val_Sensitivity: 0.0413 - val_tn: 3829.0000 - val_auc: 0.6996 - val_prc: 0.3498\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4373 - Sensitivity: 0.1088 - tn: 11288.0000 - auc: 0.6785 - prc: 0.2890 - val_loss: 0.4288 - val_Sensitivity: 0.0413 - val_tn: 3829.0000 - val_auc: 0.7001 - val_prc: 0.3507\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4319 - Sensitivity: 0.1163 - tn: 11329.0000 - auc: 0.6856 - prc: 0.3030 - val_loss: 0.4287 - val_Sensitivity: 0.0388 - val_tn: 3830.0000 - val_auc: 0.7005 - val_prc: 0.3514\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4340 - Sensitivity: 0.1097 - tn: 11328.0000 - auc: 0.6820 - prc: 0.2960 - val_loss: 0.4279 - val_Sensitivity: 0.0388 - val_tn: 3830.0000 - val_auc: 0.7008 - val_prc: 0.3519\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4320 - Sensitivity: 0.1039 - tn: 11344.0000 - auc: 0.6806 - prc: 0.2962 - val_loss: 0.4277 - val_Sensitivity: 0.0388 - val_tn: 3830.0000 - val_auc: 0.7013 - val_prc: 0.3520\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4283 - Sensitivity: 0.1048 - tn: 11374.0000 - auc: 0.6860 - prc: 0.2949 - val_loss: 0.4274 - val_Sensitivity: 0.0362 - val_tn: 3832.0000 - val_auc: 0.7014 - val_prc: 0.3525\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4288 - Sensitivity: 0.1013 - tn: 11379.0000 - auc: 0.6841 - prc: 0.2991 - val_loss: 0.4272 - val_Sensitivity: 0.0338 - val_tn: 3833.0000 - val_auc: 0.7014 - val_prc: 0.3527\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4268 - Sensitivity: 0.0889 - tn: 11410.0000 - auc: 0.6849 - prc: 0.2956 - val_loss: 0.4267 - val_Sensitivity: 0.0338 - val_tn: 3833.0000 - val_auc: 0.7015 - val_prc: 0.3530\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4229 - Sensitivity: 0.0907 - tn: 11435.0000 - auc: 0.6891 - prc: 0.3053 - val_loss: 0.4263 - val_Sensitivity: 0.0350 - val_tn: 3833.0000 - val_auc: 0.7021 - val_prc: 0.3533\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4264 - Sensitivity: 0.0862 - tn: 11476.0000 - auc: 0.6832 - prc: 0.3003 - val_loss: 0.4260 - val_Sensitivity: 0.0362 - val_tn: 3833.0000 - val_auc: 0.7029 - val_prc: 0.3540\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4163 - Sensitivity: 0.0823 - tn: 11447.0000 - auc: 0.6996 - prc: 0.3068 - val_loss: 0.4260 - val_Sensitivity: 0.0338 - val_tn: 3833.0000 - val_auc: 0.7029 - val_prc: 0.3543\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4162 - Sensitivity: 0.0880 - tn: 11484.0000 - auc: 0.6977 - prc: 0.3126 - val_loss: 0.4258 - val_Sensitivity: 0.0312 - val_tn: 3833.0000 - val_auc: 0.7026 - val_prc: 0.3537\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4174 - Sensitivity: 0.0933 - tn: 11478.0000 - auc: 0.6956 - prc: 0.3200 - val_loss: 0.4253 - val_Sensitivity: 0.0325 - val_tn: 3833.0000 - val_auc: 0.7033 - val_prc: 0.3543\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4167 - Sensitivity: 0.0778 - tn: 11491.0000 - auc: 0.6982 - prc: 0.3109 - val_loss: 0.4250 - val_Sensitivity: 0.0338 - val_tn: 3831.0000 - val_auc: 0.7038 - val_prc: 0.3554\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4177 - Sensitivity: 0.0845 - tn: 11476.0000 - auc: 0.6929 - prc: 0.3159 - val_loss: 0.4247 - val_Sensitivity: 0.0338 - val_tn: 3830.0000 - val_auc: 0.7040 - val_prc: 0.3551\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.4157 - Sensitivity: 0.0721 - tn: 11500.0000 - auc: 0.6985 - prc: 0.3139 - val_loss: 0.4244 - val_Sensitivity: 0.0338 - val_tn: 3830.0000 - val_auc: 0.7042 - val_prc: 0.3548\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4106 - Sensitivity: 0.0739 - tn: 11520.0000 - auc: 0.7086 - prc: 0.3290 - val_loss: 0.4246 - val_Sensitivity: 0.0312 - val_tn: 3830.0000 - val_auc: 0.7045 - val_prc: 0.3555\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4120 - Sensitivity: 0.0752 - tn: 11534.0000 - auc: 0.7024 - prc: 0.3236 - val_loss: 0.4241 - val_Sensitivity: 0.0325 - val_tn: 3830.0000 - val_auc: 0.7050 - val_prc: 0.3558\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4142 - Sensitivity: 0.0783 - tn: 11513.0000 - auc: 0.7002 - prc: 0.3210 - val_loss: 0.4236 - val_Sensitivity: 0.0338 - val_tn: 3830.0000 - val_auc: 0.7051 - val_prc: 0.3561\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4124 - Sensitivity: 0.0663 - tn: 11531.0000 - auc: 0.7030 - prc: 0.3175 - val_loss: 0.4234 - val_Sensitivity: 0.0350 - val_tn: 3830.0000 - val_auc: 0.7055 - val_prc: 0.3572\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4091 - Sensitivity: 0.0765 - tn: 11551.0000 - auc: 0.7074 - prc: 0.3378 - val_loss: 0.4234 - val_Sensitivity: 0.0350 - val_tn: 3831.0000 - val_auc: 0.7055 - val_prc: 0.3573\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4117 - Sensitivity: 0.0739 - tn: 11544.0000 - auc: 0.7038 - prc: 0.3297 - val_loss: 0.4232 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7054 - val_prc: 0.3573\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4061 - Sensitivity: 0.0672 - tn: 11543.0000 - auc: 0.7141 - prc: 0.3307 - val_loss: 0.4235 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7057 - val_prc: 0.3575\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4068 - Sensitivity: 0.0641 - tn: 11560.0000 - auc: 0.7115 - prc: 0.3334 - val_loss: 0.4231 - val_Sensitivity: 0.0338 - val_tn: 3831.0000 - val_auc: 0.7061 - val_prc: 0.3577\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4107 - Sensitivity: 0.0602 - tn: 11562.0000 - auc: 0.7036 - prc: 0.3246 - val_loss: 0.4230 - val_Sensitivity: 0.0312 - val_tn: 3833.0000 - val_auc: 0.7062 - val_prc: 0.3576\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4068 - Sensitivity: 0.0641 - tn: 11547.0000 - auc: 0.7104 - prc: 0.3338 - val_loss: 0.4227 - val_Sensitivity: 0.0312 - val_tn: 3832.0000 - val_auc: 0.7066 - val_prc: 0.3578\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4070 - Sensitivity: 0.0566 - tn: 11572.0000 - auc: 0.7096 - prc: 0.3301 - val_loss: 0.4227 - val_Sensitivity: 0.0312 - val_tn: 3832.0000 - val_auc: 0.7071 - val_prc: 0.3584\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4039 - Sensitivity: 0.0708 - tn: 11574.0000 - auc: 0.7161 - prc: 0.3390 - val_loss: 0.4225 - val_Sensitivity: 0.0338 - val_tn: 3831.0000 - val_auc: 0.7067 - val_prc: 0.3578\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4060 - Sensitivity: 0.0659 - tn: 11577.0000 - auc: 0.7124 - prc: 0.3418 - val_loss: 0.4225 - val_Sensitivity: 0.0338 - val_tn: 3830.0000 - val_auc: 0.7068 - val_prc: 0.3581\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4060 - Sensitivity: 0.0677 - tn: 11544.0000 - auc: 0.7130 - prc: 0.3399 - val_loss: 0.4227 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7072 - val_prc: 0.3583\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4042 - Sensitivity: 0.0619 - tn: 11581.0000 - auc: 0.7157 - prc: 0.3462 - val_loss: 0.4223 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7075 - val_prc: 0.3587\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4057 - Sensitivity: 0.0646 - tn: 11577.0000 - auc: 0.7144 - prc: 0.3428 - val_loss: 0.4222 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7072 - val_prc: 0.3586\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4013 - Sensitivity: 0.0650 - tn: 11574.0000 - auc: 0.7215 - prc: 0.3447 - val_loss: 0.4224 - val_Sensitivity: 0.0312 - val_tn: 3831.0000 - val_auc: 0.7079 - val_prc: 0.3593\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4059 - Sensitivity: 0.0641 - tn: 11566.0000 - auc: 0.7137 - prc: 0.3386 - val_loss: 0.4221 - val_Sensitivity: 0.0312 - val_tn: 3831.0000 - val_auc: 0.7081 - val_prc: 0.3596\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4028 - Sensitivity: 0.0610 - tn: 11592.0000 - auc: 0.7182 - prc: 0.3480 - val_loss: 0.4219 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7088 - val_prc: 0.3602\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4015 - Sensitivity: 0.0686 - tn: 11595.0000 - auc: 0.7201 - prc: 0.3531 - val_loss: 0.4215 - val_Sensitivity: 0.0338 - val_tn: 3829.0000 - val_auc: 0.7085 - val_prc: 0.3597\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4029 - Sensitivity: 0.0655 - tn: 11566.0000 - auc: 0.7191 - prc: 0.3477 - val_loss: 0.4214 - val_Sensitivity: 0.0350 - val_tn: 3829.0000 - val_auc: 0.7088 - val_prc: 0.3602\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.4017 - Sensitivity: 0.0602 - tn: 11577.0000 - auc: 0.7241 - prc: 0.3457 - val_loss: 0.4213 - val_Sensitivity: 0.0350 - val_tn: 3829.0000 - val_auc: 0.7089 - val_prc: 0.3603\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3998 - Sensitivity: 0.0615 - tn: 11581.0000 - auc: 0.7252 - prc: 0.3456 - val_loss: 0.4216 - val_Sensitivity: 0.0312 - val_tn: 3831.0000 - val_auc: 0.7090 - val_prc: 0.3598\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4017 - Sensitivity: 0.0540 - tn: 11592.0000 - auc: 0.7217 - prc: 0.3516 - val_loss: 0.4215 - val_Sensitivity: 0.0312 - val_tn: 3831.0000 - val_auc: 0.7096 - val_prc: 0.3605\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3999 - Sensitivity: 0.0615 - tn: 11583.0000 - auc: 0.7209 - prc: 0.3556 - val_loss: 0.4214 - val_Sensitivity: 0.0312 - val_tn: 3831.0000 - val_auc: 0.7097 - val_prc: 0.3606\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3988 - Sensitivity: 0.0584 - tn: 11581.0000 - auc: 0.7259 - prc: 0.3498 - val_loss: 0.4210 - val_Sensitivity: 0.0350 - val_tn: 3831.0000 - val_auc: 0.7098 - val_prc: 0.3605\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4011 - Sensitivity: 0.0575 - tn: 11586.0000 - auc: 0.7223 - prc: 0.3479 - val_loss: 0.4209 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7102 - val_prc: 0.3609\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3982 - Sensitivity: 0.0584 - tn: 11574.0000 - auc: 0.7263 - prc: 0.3473 - val_loss: 0.4210 - val_Sensitivity: 0.0338 - val_tn: 3831.0000 - val_auc: 0.7100 - val_prc: 0.3605\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4010 - Sensitivity: 0.0593 - tn: 11580.0000 - auc: 0.7249 - prc: 0.3524 - val_loss: 0.4207 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7107 - val_prc: 0.3610\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4003 - Sensitivity: 0.0544 - tn: 11595.0000 - auc: 0.7218 - prc: 0.3504 - val_loss: 0.4209 - val_Sensitivity: 0.0312 - val_tn: 3832.0000 - val_auc: 0.7105 - val_prc: 0.3607\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3981 - Sensitivity: 0.0548 - tn: 11583.0000 - auc: 0.7295 - prc: 0.3525 - val_loss: 0.4209 - val_Sensitivity: 0.0325 - val_tn: 3831.0000 - val_auc: 0.7109 - val_prc: 0.3611\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.4010 - Sensitivity: 0.0637 - tn: 11600.0000 - auc: 0.7219 - prc: 0.3584 - val_loss: 0.4205 - val_Sensitivity: 0.0362 - val_tn: 3831.0000 - val_auc: 0.7106 - val_prc: 0.3611\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3957 - Sensitivity: 0.0575 - tn: 11590.0000 - auc: 0.7315 - prc: 0.3653 - val_loss: 0.4204 - val_Sensitivity: 0.0362 - val_tn: 3831.0000 - val_auc: 0.7111 - val_prc: 0.3611\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3964 - Sensitivity: 0.0681 - tn: 11576.0000 - auc: 0.7313 - prc: 0.3637 - val_loss: 0.4204 - val_Sensitivity: 0.0338 - val_tn: 3831.0000 - val_auc: 0.7113 - val_prc: 0.3614\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3958 - Sensitivity: 0.0615 - tn: 11590.0000 - auc: 0.7340 - prc: 0.3620 - val_loss: 0.4205 - val_Sensitivity: 0.0338 - val_tn: 3832.0000 - val_auc: 0.7111 - val_prc: 0.3609\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3965 - Sensitivity: 0.0646 - tn: 11574.0000 - auc: 0.7304 - prc: 0.3656 - val_loss: 0.4205 - val_Sensitivity: 0.0362 - val_tn: 3831.0000 - val_auc: 0.7109 - val_prc: 0.3606\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3968 - Sensitivity: 0.0553 - tn: 11594.0000 - auc: 0.7304 - prc: 0.3604 - val_loss: 0.4205 - val_Sensitivity: 0.0375 - val_tn: 3830.0000 - val_auc: 0.7114 - val_prc: 0.3611\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3994 - Sensitivity: 0.0659 - tn: 11578.0000 - auc: 0.7247 - prc: 0.3536 - val_loss: 0.4202 - val_Sensitivity: 0.0375 - val_tn: 3830.0000 - val_auc: 0.7114 - val_prc: 0.3613\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3954 - Sensitivity: 0.0624 - tn: 11582.0000 - auc: 0.7316 - prc: 0.3649 - val_loss: 0.4203 - val_Sensitivity: 0.0375 - val_tn: 3831.0000 - val_auc: 0.7116 - val_prc: 0.3611\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3970 - Sensitivity: 0.0615 - tn: 11578.0000 - auc: 0.7312 - prc: 0.3570 - val_loss: 0.4202 - val_Sensitivity: 0.0375 - val_tn: 3830.0000 - val_auc: 0.7116 - val_prc: 0.3609\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3979 - Sensitivity: 0.0615 - tn: 11589.0000 - auc: 0.7270 - prc: 0.3603 - val_loss: 0.4203 - val_Sensitivity: 0.0350 - val_tn: 3830.0000 - val_auc: 0.7117 - val_prc: 0.3608\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3950 - Sensitivity: 0.0637 - tn: 11582.0000 - auc: 0.7341 - prc: 0.3628 - val_loss: 0.4203 - val_Sensitivity: 0.0362 - val_tn: 3830.0000 - val_auc: 0.7118 - val_prc: 0.3611\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3961 - Sensitivity: 0.0615 - tn: 11585.0000 - auc: 0.7318 - prc: 0.3563 - val_loss: 0.4203 - val_Sensitivity: 0.0350 - val_tn: 3831.0000 - val_auc: 0.7118 - val_prc: 0.3609\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3956 - Sensitivity: 0.0593 - tn: 11597.0000 - auc: 0.7312 - prc: 0.3685 - val_loss: 0.4201 - val_Sensitivity: 0.0362 - val_tn: 3831.0000 - val_auc: 0.7121 - val_prc: 0.3609\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3970 - Sensitivity: 0.0699 - tn: 11579.0000 - auc: 0.7291 - prc: 0.3631 - val_loss: 0.4199 - val_Sensitivity: 0.0388 - val_tn: 3830.0000 - val_auc: 0.7120 - val_prc: 0.3606\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3951 - Sensitivity: 0.0637 - tn: 11592.0000 - auc: 0.7328 - prc: 0.3699 - val_loss: 0.4199 - val_Sensitivity: 0.0388 - val_tn: 3830.0000 - val_auc: 0.7123 - val_prc: 0.3610\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3977 - Sensitivity: 0.0588 - tn: 11577.0000 - auc: 0.7299 - prc: 0.3588 - val_loss: 0.4198 - val_Sensitivity: 0.0375 - val_tn: 3831.0000 - val_auc: 0.7127 - val_prc: 0.3609\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3979 - Sensitivity: 0.0584 - tn: 11600.0000 - auc: 0.7277 - prc: 0.3625 - val_loss: 0.4198 - val_Sensitivity: 0.0362 - val_tn: 3830.0000 - val_auc: 0.7127 - val_prc: 0.3606\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.3937 - Sensitivity: 0.0593 - tn: 11605.0000 - auc: 0.7356 - prc: 0.3687 - val_loss: 0.4195 - val_Sensitivity: 0.0362 - val_tn: 3830.0000 - val_auc: 0.7129 - val_prc: 0.3613\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3956 - Sensitivity: 0.0632 - tn: 11585.0000 - auc: 0.7348 - prc: 0.3663 - val_loss: 0.4198 - val_Sensitivity: 0.0362 - val_tn: 3830.0000 - val_auc: 0.7127 - val_prc: 0.3612\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3944 - Sensitivity: 0.0557 - tn: 11597.0000 - auc: 0.7339 - prc: 0.3638 - val_loss: 0.4196 - val_Sensitivity: 0.0400 - val_tn: 3830.0000 - val_auc: 0.7130 - val_prc: 0.3608\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3949 - Sensitivity: 0.0686 - tn: 11588.0000 - auc: 0.7332 - prc: 0.3673 - val_loss: 0.4194 - val_Sensitivity: 0.0400 - val_tn: 3829.0000 - val_auc: 0.7131 - val_prc: 0.3607\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3953 - Sensitivity: 0.0637 - tn: 11587.0000 - auc: 0.7331 - prc: 0.3712 - val_loss: 0.4195 - val_Sensitivity: 0.0400 - val_tn: 3829.0000 - val_auc: 0.7131 - val_prc: 0.3611\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3921 - Sensitivity: 0.0677 - tn: 11586.0000 - auc: 0.7398 - prc: 0.3722 - val_loss: 0.4195 - val_Sensitivity: 0.0388 - val_tn: 3829.0000 - val_auc: 0.7133 - val_prc: 0.3611\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3913 - Sensitivity: 0.0637 - tn: 11582.0000 - auc: 0.7411 - prc: 0.3769 - val_loss: 0.4194 - val_Sensitivity: 0.0413 - val_tn: 3827.0000 - val_auc: 0.7133 - val_prc: 0.3609\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3951 - Sensitivity: 0.0624 - tn: 11587.0000 - auc: 0.7339 - prc: 0.3648 - val_loss: 0.4196 - val_Sensitivity: 0.0413 - val_tn: 3828.0000 - val_auc: 0.7131 - val_prc: 0.3610\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3956 - Sensitivity: 0.0681 - tn: 11560.0000 - auc: 0.7320 - prc: 0.3641 - val_loss: 0.4196 - val_Sensitivity: 0.0413 - val_tn: 3828.0000 - val_auc: 0.7131 - val_prc: 0.3608\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3948 - Sensitivity: 0.0721 - tn: 11561.0000 - auc: 0.7342 - prc: 0.3671 - val_loss: 0.4194 - val_Sensitivity: 0.0413 - val_tn: 3828.0000 - val_auc: 0.7133 - val_prc: 0.3609\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3961 - Sensitivity: 0.0708 - tn: 11582.0000 - auc: 0.7318 - prc: 0.3670 - val_loss: 0.4192 - val_Sensitivity: 0.0400 - val_tn: 3828.0000 - val_auc: 0.7133 - val_prc: 0.3614\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3937 - Sensitivity: 0.0672 - tn: 11586.0000 - auc: 0.7377 - prc: 0.3724 - val_loss: 0.4195 - val_Sensitivity: 0.0400 - val_tn: 3829.0000 - val_auc: 0.7137 - val_prc: 0.3617\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3929 - Sensitivity: 0.0628 - tn: 11584.0000 - auc: 0.7387 - prc: 0.3669 - val_loss: 0.4196 - val_Sensitivity: 0.0388 - val_tn: 3829.0000 - val_auc: 0.7133 - val_prc: 0.3619\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3925 - Sensitivity: 0.0588 - tn: 11587.0000 - auc: 0.7384 - prc: 0.3712 - val_loss: 0.4195 - val_Sensitivity: 0.0413 - val_tn: 3828.0000 - val_auc: 0.7131 - val_prc: 0.3614\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3925 - Sensitivity: 0.0655 - tn: 11584.0000 - auc: 0.7393 - prc: 0.3740 - val_loss: 0.4195 - val_Sensitivity: 0.0413 - val_tn: 3828.0000 - val_auc: 0.7134 - val_prc: 0.3613\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3898 - Sensitivity: 0.0619 - tn: 11588.0000 - auc: 0.7446 - prc: 0.3796 - val_loss: 0.4195 - val_Sensitivity: 0.0425 - val_tn: 3829.0000 - val_auc: 0.7136 - val_prc: 0.3617\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.3934 - Sensitivity: 0.0703 - tn: 11589.0000 - auc: 0.7370 - prc: 0.3720 - val_loss: 0.4194 - val_Sensitivity: 0.0425 - val_tn: 3828.0000 - val_auc: 0.7137 - val_prc: 0.3615\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.3895 - Sensitivity: 0.0650 - tn: 11586.0000 - auc: 0.7448 - prc: 0.3815 - val_loss: 0.4191 - val_Sensitivity: 0.0425 - val_tn: 3828.0000 - val_auc: 0.7144 - val_prc: 0.3620\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.3916 - Sensitivity: 0.0690 - tn: 11588.0000 - auc: 0.7404 - prc: 0.3799 - val_loss: 0.4192 - val_Sensitivity: 0.0425 - val_tn: 3829.0000 - val_auc: 0.7146 - val_prc: 0.3622\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3891 - Sensitivity: 0.0668 - tn: 11580.0000 - auc: 0.7462 - prc: 0.3802 - val_loss: 0.4194 - val_Sensitivity: 0.0425 - val_tn: 3829.0000 - val_auc: 0.7149 - val_prc: 0.3627\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3919 - Sensitivity: 0.0712 - tn: 11584.0000 - auc: 0.7380 - prc: 0.3773 - val_loss: 0.4190 - val_Sensitivity: 0.0450 - val_tn: 3826.0000 - val_auc: 0.7147 - val_prc: 0.3616\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3933 - Sensitivity: 0.0774 - tn: 11573.0000 - auc: 0.7377 - prc: 0.3765 - val_loss: 0.4188 - val_Sensitivity: 0.0437 - val_tn: 3827.0000 - val_auc: 0.7150 - val_prc: 0.3623\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3897 - Sensitivity: 0.0672 - tn: 11580.0000 - auc: 0.7442 - prc: 0.3756 - val_loss: 0.4192 - val_Sensitivity: 0.0425 - val_tn: 3829.0000 - val_auc: 0.7146 - val_prc: 0.3623\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3913 - Sensitivity: 0.0677 - tn: 11589.0000 - auc: 0.7422 - prc: 0.3764 - val_loss: 0.4192 - val_Sensitivity: 0.0425 - val_tn: 3828.0000 - val_auc: 0.7146 - val_prc: 0.3620\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3927 - Sensitivity: 0.0739 - tn: 11569.0000 - auc: 0.7393 - prc: 0.3775 - val_loss: 0.4191 - val_Sensitivity: 0.0413 - val_tn: 3829.0000 - val_auc: 0.7145 - val_prc: 0.3615\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3889 - Sensitivity: 0.0655 - tn: 11575.0000 - auc: 0.7462 - prc: 0.3820 - val_loss: 0.4194 - val_Sensitivity: 0.0388 - val_tn: 3829.0000 - val_auc: 0.7146 - val_prc: 0.3618\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3885 - Sensitivity: 0.0694 - tn: 11589.0000 - auc: 0.7457 - prc: 0.3876 - val_loss: 0.4196 - val_Sensitivity: 0.0388 - val_tn: 3829.0000 - val_auc: 0.7146 - val_prc: 0.3614\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3898 - Sensitivity: 0.0663 - tn: 11597.0000 - auc: 0.7447 - prc: 0.3806 - val_loss: 0.4196 - val_Sensitivity: 0.0400 - val_tn: 3828.0000 - val_auc: 0.7146 - val_prc: 0.3615\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3909 - Sensitivity: 0.0703 - tn: 11578.0000 - auc: 0.7419 - prc: 0.3766 - val_loss: 0.4195 - val_Sensitivity: 0.0425 - val_tn: 3826.0000 - val_auc: 0.7142 - val_prc: 0.3611\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3915 - Sensitivity: 0.0752 - tn: 11576.0000 - auc: 0.7402 - prc: 0.3772 - val_loss: 0.4195 - val_Sensitivity: 0.0413 - val_tn: 3826.0000 - val_auc: 0.7144 - val_prc: 0.3608\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3902 - Sensitivity: 0.0699 - tn: 11581.0000 - auc: 0.7425 - prc: 0.3803 - val_loss: 0.4193 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7145 - val_prc: 0.3606\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3893 - Sensitivity: 0.0787 - tn: 11568.0000 - auc: 0.7450 - prc: 0.3808 - val_loss: 0.4193 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7146 - val_prc: 0.3607\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3910 - Sensitivity: 0.0663 - tn: 11564.0000 - auc: 0.7425 - prc: 0.3744 - val_loss: 0.4197 - val_Sensitivity: 0.0413 - val_tn: 3825.0000 - val_auc: 0.7144 - val_prc: 0.3609\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3905 - Sensitivity: 0.0747 - tn: 11577.0000 - auc: 0.7429 - prc: 0.3809 - val_loss: 0.4192 - val_Sensitivity: 0.0450 - val_tn: 3825.0000 - val_auc: 0.7144 - val_prc: 0.3600\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3871 - Sensitivity: 0.0747 - tn: 11577.0000 - auc: 0.7502 - prc: 0.3894 - val_loss: 0.4197 - val_Sensitivity: 0.0413 - val_tn: 3825.0000 - val_auc: 0.7149 - val_prc: 0.3604\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3894 - Sensitivity: 0.0708 - tn: 11589.0000 - auc: 0.7454 - prc: 0.3828 - val_loss: 0.4199 - val_Sensitivity: 0.0400 - val_tn: 3826.0000 - val_auc: 0.7148 - val_prc: 0.3605\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3925 - Sensitivity: 0.0641 - tn: 11578.0000 - auc: 0.7390 - prc: 0.3687 - val_loss: 0.4195 - val_Sensitivity: 0.0400 - val_tn: 3826.0000 - val_auc: 0.7145 - val_prc: 0.3605\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3891 - Sensitivity: 0.0716 - tn: 11589.0000 - auc: 0.7442 - prc: 0.3777 - val_loss: 0.4195 - val_Sensitivity: 0.0400 - val_tn: 3825.0000 - val_auc: 0.7145 - val_prc: 0.3605\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3887 - Sensitivity: 0.0765 - tn: 11593.0000 - auc: 0.7448 - prc: 0.3880 - val_loss: 0.4193 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7147 - val_prc: 0.3607\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.3902 - Sensitivity: 0.0739 - tn: 11584.0000 - auc: 0.7441 - prc: 0.3798 - val_loss: 0.4193 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7149 - val_prc: 0.3608\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3886 - Sensitivity: 0.0761 - tn: 11575.0000 - auc: 0.7456 - prc: 0.3827 - val_loss: 0.4194 - val_Sensitivity: 0.0450 - val_tn: 3825.0000 - val_auc: 0.7147 - val_prc: 0.3601\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3875 - Sensitivity: 0.0770 - tn: 11562.0000 - auc: 0.7490 - prc: 0.3871 - val_loss: 0.4195 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7147 - val_prc: 0.3603\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.3891 - Sensitivity: 0.0730 - tn: 11576.0000 - auc: 0.7452 - prc: 0.3874 - val_loss: 0.4198 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7148 - val_prc: 0.3606\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3893 - Sensitivity: 0.0809 - tn: 11574.0000 - auc: 0.7460 - prc: 0.3863 - val_loss: 0.4195 - val_Sensitivity: 0.0437 - val_tn: 3826.0000 - val_auc: 0.7149 - val_prc: 0.3610\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.3860 - Sensitivity: 0.0721 - tn: 11580.0000 - auc: 0.7505 - prc: 0.3921 - val_loss: 0.4198 - val_Sensitivity: 0.0400 - val_tn: 3828.0000 - val_auc: 0.7150 - val_prc: 0.3613\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.3888 - Sensitivity: 0.0690 - tn: 11587.0000 - auc: 0.7462 - prc: 0.3802 - val_loss: 0.4195 - val_Sensitivity: 0.0425 - val_tn: 3825.0000 - val_auc: 0.7147 - val_prc: 0.3604\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.3872 - Sensitivity: 0.0801 - tn: 11575.0000 - auc: 0.7486 - prc: 0.3904 - val_loss: 0.4195 - val_Sensitivity: 0.0437 - val_tn: 3825.0000 - val_auc: 0.7148 - val_prc: 0.3611\n"
     ]
    }
   ],
   "source": [
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    model4 = keras.models.load_model('model4.h5')\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7252044798685402,\n",
       " 0.7319904503102976,\n",
       " 0.72477837735288,\n",
       " 0.732804192706886,\n",
       " 0.7270742497806476]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))\n",
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.695504978567075,\n",
       " 0.699707717536611,\n",
       " 0.7103089093750568,\n",
       " 0.7154917622807948,\n",
       " 0.6956690294429204]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrpreds = []\n",
    "model3 = LogisticRegression(penalty='none', warm_start=False)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))\n",
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))\n",
    "lr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.728 (0.724-0.733)\n"
     ]
    }
   ],
   "source": [
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print('Neural Network:', round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.703 (0.692-0.715)\n"
     ]
    }
   ],
   "source": [
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.358 (0.34-0.376)\n",
      "Logistic Regression: 0.319 (0.304-0.334)\n"
     ]
    }
   ],
   "source": [
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')\n",
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')\n",
    "with open('pan_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4647\n",
      "4647\n",
      "4647\n",
      "4646\n",
      "4646\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(annpreds[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pan_ann_tpr = []\n",
    "pan_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, thresholds = roc_curve(dy_test[x], annpreds[x])\n",
    "    pan_ann_tpr.append(tpr)\n",
    "    pan_ann_fpr.append(fpr)\n",
    "pan_ann_tpr_array = [np.array(x) for x in pan_ann_tpr]\n",
    "mean_pan_ann_tpr = [np.mean(k) for k in zip(*pan_ann_tpr_array)]\n",
    "pan_ann_fpr_array = [np.array(x) for x in pan_ann_fpr]\n",
    "mean_pan_ann_fpr = [np.mean(k) for k in zip(*pan_ann_fpr_array)]\n",
    "%store mean_pan_ann_tpr\n",
    "%store mean_pan_ann_fpr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    pan_ann_tpr[x] = np.random.permutation(pan_ann_tpr[x])\n",
    "pan_ann_tpr = [x[:1000] for x in pan_ann_tpr]\n",
    "for x in range(0,5):\n",
    "    pan_ann_tpr[x] = sorted(pan_ann_tpr[x])\n",
    "for x in range(0,5):\n",
    "    pan_ann_fpr[x] = np.random.permutation(pan_ann_fpr[x])\n",
    "pan_ann_fpr = [x[:1000] for x in pan_ann_fpr]\n",
    "for x in range(0,5):\n",
    "    pan_ann_fpr[x] = sorted(pan_ann_fpr[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pan_ann_tpr_array = [np.array(x) for x in pan_ann_tpr]\n",
    "mean_pan_ann_tpr = [np.mean(k) for k in zip(*pan_ann_tpr_array)]\n",
    "pan_ann_fpr_array = [np.array(x) for x in pan_ann_fpr]\n",
    "mean_pan_ann_fpr = [np.mean(k) for k in zip(*pan_ann_fpr_array)]\n",
    "%store mean_pan_ann_tpr\n",
    "%store mean_pan_ann_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_ann_tpr = []\n",
    "pan_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    pan_ann_tpr.append(tpr)\n",
    "    pan_ann_fpr.append(fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1165\n",
      "1146\n",
      "1203\n",
      "1142\n",
      "1143\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(pan_ann_fpr[x]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_ann_tpr' (list)\n",
      "Stored 'mean_pan_ann_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_tpr[x]) - 1100\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_tpr[x]))\n",
    "        pan_ann_tpr[x] = np.delete(pan_ann_tpr[x],ind)\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_fpr[x]) - 1100\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_fpr[x]))\n",
    "        pan_ann_fpr[x] = np.delete(pan_ann_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_pan_ann_tpr = [np.mean(k) for k in zip(*pan_ann_tpr)]\n",
    "\n",
    "mean_pan_ann_fpr = [np.mean(k) for k in zip(*pan_ann_fpr)]\n",
    "%store mean_pan_ann_tpr\n",
    "%store mean_pan_ann_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_lr_tpr = []\n",
    "pan_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    pan_lr_tpr.append(tpr)\n",
    "    pan_lr_fpr.append(fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154\n",
      "1184\n",
      "1202\n",
      "1168\n",
      "1174\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(pan_lr_fpr[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_lr_tpr' (list)\n",
      "Stored 'mean_pan_lr_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_tpr[x]) - 1100\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_tpr[x]))\n",
    "        pan_lr_tpr[x] = np.delete(pan_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_fpr[x]) - 1100\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_fpr[x]))\n",
    "        pan_lr_fpr[x] = np.delete(pan_lr_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_pan_lr_tpr = [np.mean(k) for k in zip(*pan_lr_tpr)]\n",
    "\n",
    "mean_pan_lr_fpr = [np.mean(k) for k in zip(*pan_lr_fpr)]\n",
    "%store mean_pan_lr_tpr\n",
    "%store mean_pan_lr_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_lr_rec = []\n",
    "pan_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    pan_lr_rec.append(rec)\n",
    "    pan_lr_prec.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4564\n",
      "4564\n",
      "4564\n",
      "4564\n",
      "4512\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(pan_lr_rec[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_lr_rec' (list)\n",
      "Stored 'mean_pan_lr_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_rec[x]) - 4512\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_rec[x]))\n",
    "        pan_lr_rec[x] = np.delete(pan_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_lr_prec[x]) - 4512\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_lr_prec[x]))\n",
    "        pan_lr_prec[x] = np.delete(pan_lr_prec[x],ind)\n",
    "\n",
    "mean_pan_lr_rec = [np.mean(k) for k in zip(*pan_lr_rec)]\n",
    "\n",
    "mean_pan_lr_prec = [np.mean(k) for k in zip(*pan_lr_prec)]\n",
    "%store mean_pan_lr_rec\n",
    "%store mean_pan_lr_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_ann_rec = []\n",
    "pan_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    pan_ann_rec.append(rec)\n",
    "    pan_ann_prec.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4644\n",
      "4622\n",
      "4607\n",
      "4644\n",
      "4647\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(pan_ann_rec[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_pan_ann_rec' (list)\n",
      "Stored 'mean_pan_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_rec[x]) - 4600\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_rec[x]))\n",
    "        pan_ann_rec[x] = np.delete(pan_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(pan_ann_prec[x]) - 4600\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(pan_ann_prec[x]))\n",
    "        pan_ann_prec[x] = np.delete(pan_ann_prec[x],ind)\n",
    "\n",
    "mean_pan_ann_rec = [np.mean(k) for k in zip(*pan_ann_rec)]\n",
    "\n",
    "mean_pan_ann_prec = [np.mean(k) for k in zip(*pan_ann_prec)]\n",
    "%store mean_pan_ann_rec\n",
    "%store mean_pan_ann_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pan_lr_rec = []\n",
    "pan_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    pan_lr_rec.append(rec)\n",
    "    pan_lr_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    pan_lr_rec[x] = np.random.permutation(pan_lr_rec[x])\n",
    "pan_lr_rec = [x[:1000] for x in pan_lr_rec]\n",
    "for x in range(0,5):\n",
    "    pan_lr_rec[x] = sorted(pan_lr_rec[x])\n",
    "for x in range(0,5):\n",
    "    pan_lr_prec[x] = np.random.permutation(pan_lr_prec[x])\n",
    "pan_lr_prec = [x[:1000] for x in pan_lr_prec]\n",
    "for x in range(0,5):\n",
    "    pan_lr_prec[x] = sorted(pan_lr_prec[x])\n",
    "pan_lr_rec_array = [np.array(x) for x in pan_lr_rec]\n",
    "mean_pan_lr_rec = [np.mean(k) for k in zip(*pan_lr_rec_array)]\n",
    "pan_lr_prec_array = [np.array(x) for x in pan_lr_prec]\n",
    "mean_pan_lr_prec = [np.mean(k) for k in zip(*pan_lr_prec_array)]\n",
    "%store mean_pan_lr_rec\n",
    "%store mean_pan_lr_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(mean_pan_ann_rec, mean_pan_ann_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresh = np.arange(0, 1, 0.01)\n",
    "#calculate recall at 10 thresholds\n",
    "annrecall_list = []\n",
    "for i in thresh:\n",
    "    annrecall_list.append(recall_score(dy_test[1], annpreds[1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "annspec_list = []\n",
    "for i in thresh:\n",
    "    annspec_list.append(specificity_score(dy_test[1], annpreds[1] > i))\n",
    "from matplotlib import pyplot as plt\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, annrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, annspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresh = np.arange(0, 1, 0.01)\n",
    "#calculate recall at 10 thresholds\n",
    "lrrecall_list = []\n",
    "for i in thresh:\n",
    "    lrrecall_list.append(recall_score(dy_test[1], lrpreds[1][:,1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "lrspec_list = []\n",
    "for i in thresh:\n",
    "    lrspec_list.append(specificity_score(dy_test[1], lrpreds[1][:,1] > i))\n",
    "from matplotlib import pyplot as plt\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, lrrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, lrspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6967320261437908\n",
      "0.5842349304482226\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(dy_test[1], lrpreds[1][:,1] > 0.15))\n",
    "print(specificity_score(dy_test[1], lrpreds[1][:,1] > 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3202614379084967\n",
      "0.9095826893353941\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(dy_test[1], annpreds[1] > 0.3))\n",
    "print(specificity_score(dy_test[1], annpreds[1] > 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8767393285809614e-08\n",
      "1.8767393285809614e-08\n",
      "9.134079483989441e-09\n",
      "0.005724029557307809\n",
      "0.0031678343144419924\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "\n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n",
    "\n",
    "\n",
    "p = []\n",
    "z = []\n",
    "# Model A (random) vs. \"good\" model B\n",
    "for x in range(0,5):\n",
    "    preds_A = annpreds[x]\n",
    "    preds_B = lrpreds[x][:,1]\n",
    "    actual = dy_test[x]\n",
    "\n",
    "    actual = actual.array\n",
    "\n",
    "    def group_preds_by_label(preds, actual):\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "\n",
    "\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "    # Two tailed test\n",
    "    z.append(z_score(var_A, var_B, covar_AB, auc_A, auc_B))\n",
    "    p.append(st.norm.sf(abs(z[x-1]))*2)\n",
    "\n",
    "\n",
    "for x in range(0,5):\n",
    "    print(p[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017783821081231715"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p)/(len(p))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
