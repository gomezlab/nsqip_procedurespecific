{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_hep.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25403, 70)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['HEP_BILELEAKAGE']\n",
    "X = data.drop(['HEP_BILELEAKAGE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for x in range(1,6):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['HEP_BILELEAKAGE'], axis=1))\n",
    "        dy_train.append(d[x]['HEP_BILELEAKAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['HEP_BILELEAKAGE'], axis=1))\n",
    "        dy_test.append(d[x]['HEP_BILELEAKAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfpreds = []\n",
    "xgbpreds = []\n",
    "model = RandomForestClassifier(n_estimators=1250, min_samples_split=2, min_samples_leaf=8, max_features='auto', max_depth=20, bootstrap=True)\n",
    "model2 = XGBClassifier(n_estimators=50, subsample=0.6, min_child_weight=10, max_depth=6, learning_rate=0.1, colsample_bytree=0.8)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model.fit(X, y)\n",
    "    model2.fit(X, y)\n",
    "    rfpreds.append(model.predict_proba(X_test))\n",
    "    xgbpreds.append(model2.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store rfpreds\n",
    "%store xgbpreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(1,6):\n",
    "    print(roc_auc_score(dy_test[x], rfpreds[x][:,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(1,6):\n",
    "    print(roc_auc_score(dy_test[x], xgbpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "input_shape = [dX_train[1].shape[1]]\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "for _ in range(2):\n",
    "    model4.add(keras.layers.Dense(1000))\n",
    "    model4.add(keras.layers.BatchNormalization())\n",
    "    model4.add(keras.layers.Dropout(0.8))\n",
    "    model4.add(keras.layers.Activation(\"relu\"))\n",
    "model4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "metrics = [keras.metrics.Recall(name='Sensitivity'), keras.metrics.TrueNegatives(name='tn'), keras.metrics.AUC(name='auc'), keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.00001,\n",
    "    restore_best_weights=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 3s 34ms/step - loss: 0.7096 - Sensitivity: 0.3210 - tn: 10344.0000 - auc: 0.5087 - prc: 0.0451 - val_loss: 1.7009 - val_Sensitivity: 1.0000 - val_tn: 0.0000e+00 - val_auc: 0.6286 - val_prc: 0.0641\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2650 - Sensitivity: 0.0688 - tn: 14066.0000 - auc: 0.5287 - prc: 0.0440 - val_loss: 1.1014 - val_Sensitivity: 1.0000 - val_tn: 16.0000 - val_auc: 0.6747 - val_prc: 0.0799\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.2352 - Sensitivity: 0.0441 - tn: 14430.0000 - auc: 0.5334 - prc: 0.0472 - val_loss: 0.6387 - val_Sensitivity: 0.5528 - val_tn: 3499.0000 - val_auc: 0.6947 - val_prc: 0.0894\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2129 - Sensitivity: 0.0600 - tn: 14494.0000 - auc: 0.5682 - prc: 0.0615 - val_loss: 0.5947 - val_Sensitivity: 0.5025 - val_tn: 3836.0000 - val_auc: 0.7029 - val_prc: 0.0917\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2134 - Sensitivity: 0.0547 - tn: 14492.0000 - auc: 0.5773 - prc: 0.0602 - val_loss: 0.3772 - val_Sensitivity: 0.2211 - val_tn: 4624.0000 - val_auc: 0.7080 - val_prc: 0.0928\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2044 - Sensitivity: 0.0494 - tn: 14496.0000 - auc: 0.6098 - prc: 0.0655 - val_loss: 0.3203 - val_Sensitivity: 0.1809 - val_tn: 4676.0000 - val_auc: 0.7115 - val_prc: 0.0938\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2037 - Sensitivity: 0.0406 - tn: 14542.0000 - auc: 0.6073 - prc: 0.0639 - val_loss: 0.2809 - val_Sensitivity: 0.1759 - val_tn: 4683.0000 - val_auc: 0.7138 - val_prc: 0.0949\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1955 - Sensitivity: 0.0635 - tn: 14502.0000 - auc: 0.6310 - prc: 0.0790 - val_loss: 0.2547 - val_Sensitivity: 0.1457 - val_tn: 4706.0000 - val_auc: 0.7209 - val_prc: 0.0958\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1986 - Sensitivity: 0.0494 - tn: 14540.0000 - auc: 0.6283 - prc: 0.0726 - val_loss: 0.2297 - val_Sensitivity: 0.1256 - val_tn: 4737.0000 - val_auc: 0.7251 - val_prc: 0.0975\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.2051 - Sensitivity: 0.0282 - tn: 14532.0000 - auc: 0.6208 - prc: 0.0667 - val_loss: 0.2306 - val_Sensitivity: 0.1558 - val_tn: 4715.0000 - val_auc: 0.7263 - val_prc: 0.1021\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1906 - Sensitivity: 0.0494 - tn: 14545.0000 - auc: 0.6570 - prc: 0.0808 - val_loss: 0.2507 - val_Sensitivity: 0.1960 - val_tn: 4663.0000 - val_auc: 0.7282 - val_prc: 0.0992\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1907 - Sensitivity: 0.0459 - tn: 14528.0000 - auc: 0.6473 - prc: 0.0779 - val_loss: 0.2437 - val_Sensitivity: 0.2010 - val_tn: 4668.0000 - val_auc: 0.7287 - val_prc: 0.1008\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1859 - Sensitivity: 0.0476 - tn: 14549.0000 - auc: 0.6589 - prc: 0.0862 - val_loss: 0.2481 - val_Sensitivity: 0.2060 - val_tn: 4663.0000 - val_auc: 0.7294 - val_prc: 0.0996\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1877 - Sensitivity: 0.0388 - tn: 14544.0000 - auc: 0.6572 - prc: 0.0800 - val_loss: 0.2479 - val_Sensitivity: 0.2161 - val_tn: 4640.0000 - val_auc: 0.7307 - val_prc: 0.0997\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1850 - Sensitivity: 0.0511 - tn: 14547.0000 - auc: 0.6615 - prc: 0.0830 - val_loss: 0.2210 - val_Sensitivity: 0.1508 - val_tn: 4719.0000 - val_auc: 0.7341 - val_prc: 0.1034\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1870 - Sensitivity: 0.0388 - tn: 14567.0000 - auc: 0.6595 - prc: 0.0767 - val_loss: 0.2108 - val_Sensitivity: 0.1256 - val_tn: 4750.0000 - val_auc: 0.7348 - val_prc: 0.1077\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1812 - Sensitivity: 0.0441 - tn: 14571.0000 - auc: 0.6553 - prc: 0.0881 - val_loss: 0.2280 - val_Sensitivity: 0.1759 - val_tn: 4694.0000 - val_auc: 0.7324 - val_prc: 0.1046\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1792 - Sensitivity: 0.0511 - tn: 14556.0000 - auc: 0.6725 - prc: 0.0839 - val_loss: 0.2374 - val_Sensitivity: 0.1960 - val_tn: 4669.0000 - val_auc: 0.7323 - val_prc: 0.1007\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1796 - Sensitivity: 0.0564 - tn: 14573.0000 - auc: 0.6759 - prc: 0.0921 - val_loss: 0.2253 - val_Sensitivity: 0.1658 - val_tn: 4697.0000 - val_auc: 0.7331 - val_prc: 0.1055\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1786 - Sensitivity: 0.0617 - tn: 14568.0000 - auc: 0.6818 - prc: 0.0940 - val_loss: 0.2110 - val_Sensitivity: 0.1407 - val_tn: 4759.0000 - val_auc: 0.7353 - val_prc: 0.1116\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1756 - Sensitivity: 0.0335 - tn: 14589.0000 - auc: 0.6931 - prc: 0.0867 - val_loss: 0.2088 - val_Sensitivity: 0.1206 - val_tn: 4771.0000 - val_auc: 0.7347 - val_prc: 0.1109\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1784 - Sensitivity: 0.0317 - tn: 14601.0000 - auc: 0.6746 - prc: 0.0842 - val_loss: 0.2113 - val_Sensitivity: 0.1156 - val_tn: 4774.0000 - val_auc: 0.7348 - val_prc: 0.1099\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1724 - Sensitivity: 0.0353 - tn: 14582.0000 - auc: 0.6972 - prc: 0.0919 - val_loss: 0.2202 - val_Sensitivity: 0.1307 - val_tn: 4746.0000 - val_auc: 0.7342 - val_prc: 0.1078\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1702 - Sensitivity: 0.0388 - tn: 14588.0000 - auc: 0.7032 - prc: 0.0919 - val_loss: 0.2141 - val_Sensitivity: 0.0955 - val_tn: 4776.0000 - val_auc: 0.7338 - val_prc: 0.1067\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1681 - Sensitivity: 0.0511 - tn: 14601.0000 - auc: 0.6972 - prc: 0.1070 - val_loss: 0.2259 - val_Sensitivity: 0.1106 - val_tn: 4738.0000 - val_auc: 0.7323 - val_prc: 0.1025\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1689 - Sensitivity: 0.0159 - tn: 14601.0000 - auc: 0.7004 - prc: 0.0889 - val_loss: 0.2235 - val_Sensitivity: 0.1106 - val_tn: 4752.0000 - val_auc: 0.7322 - val_prc: 0.1019\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1644 - Sensitivity: 0.0388 - tn: 14596.0000 - auc: 0.7059 - prc: 0.1019 - val_loss: 0.2202 - val_Sensitivity: 0.0955 - val_tn: 4768.0000 - val_auc: 0.7327 - val_prc: 0.1021\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1659 - Sensitivity: 0.0370 - tn: 14612.0000 - auc: 0.6941 - prc: 0.1079 - val_loss: 0.2166 - val_Sensitivity: 0.0854 - val_tn: 4794.0000 - val_auc: 0.7336 - val_prc: 0.1031\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1586 - Sensitivity: 0.0317 - tn: 14622.0000 - auc: 0.7222 - prc: 0.1100 - val_loss: 0.2203 - val_Sensitivity: 0.0854 - val_tn: 4777.0000 - val_auc: 0.7317 - val_prc: 0.0986\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1638 - Sensitivity: 0.0265 - tn: 14627.0000 - auc: 0.7093 - prc: 0.0966 - val_loss: 0.2310 - val_Sensitivity: 0.1055 - val_tn: 4763.0000 - val_auc: 0.7302 - val_prc: 0.0955\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1637 - Sensitivity: 0.0370 - tn: 14629.0000 - auc: 0.7172 - prc: 0.1095 - val_loss: 0.2121 - val_Sensitivity: 0.0905 - val_tn: 4808.0000 - val_auc: 0.7310 - val_prc: 0.1021\n",
      "Epoch 1/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1787 - Sensitivity: 0.0278 - tn: 14603.0000 - auc: 0.6684 - prc: 0.0816 - val_loss: 0.2237 - val_Sensitivity: 0.1818 - val_tn: 4703.0000 - val_auc: 0.7608 - val_prc: 0.1541\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1781 - Sensitivity: 0.0470 - tn: 14570.0000 - auc: 0.6870 - prc: 0.0872 - val_loss: 0.2265 - val_Sensitivity: 0.1545 - val_tn: 4718.0000 - val_auc: 0.7588 - val_prc: 0.1515\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1710 - Sensitivity: 0.0452 - tn: 14581.0000 - auc: 0.6941 - prc: 0.0946 - val_loss: 0.2220 - val_Sensitivity: 0.1318 - val_tn: 4744.0000 - val_auc: 0.7568 - val_prc: 0.1466\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1720 - Sensitivity: 0.0348 - tn: 14582.0000 - auc: 0.6883 - prc: 0.0887 - val_loss: 0.2336 - val_Sensitivity: 0.1500 - val_tn: 4693.0000 - val_auc: 0.7549 - val_prc: 0.1417\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1711 - Sensitivity: 0.0400 - tn: 14598.0000 - auc: 0.6891 - prc: 0.1050 - val_loss: 0.2233 - val_Sensitivity: 0.1227 - val_tn: 4744.0000 - val_auc: 0.7576 - val_prc: 0.1420\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1686 - Sensitivity: 0.0313 - tn: 14603.0000 - auc: 0.7041 - prc: 0.0988 - val_loss: 0.2254 - val_Sensitivity: 0.1318 - val_tn: 4746.0000 - val_auc: 0.7547 - val_prc: 0.1390\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1641 - Sensitivity: 0.0313 - tn: 14605.0000 - auc: 0.7117 - prc: 0.1052 - val_loss: 0.2273 - val_Sensitivity: 0.1182 - val_tn: 4742.0000 - val_auc: 0.7527 - val_prc: 0.1312\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1687 - Sensitivity: 0.0261 - tn: 14602.0000 - auc: 0.6886 - prc: 0.0969 - val_loss: 0.2201 - val_Sensitivity: 0.1000 - val_tn: 4769.0000 - val_auc: 0.7510 - val_prc: 0.1324\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1625 - Sensitivity: 0.0278 - tn: 14618.0000 - auc: 0.7055 - prc: 0.1012 - val_loss: 0.2168 - val_Sensitivity: 0.0955 - val_tn: 4771.0000 - val_auc: 0.7500 - val_prc: 0.1329\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1616 - Sensitivity: 0.0157 - tn: 14625.0000 - auc: 0.7050 - prc: 0.0938 - val_loss: 0.2263 - val_Sensitivity: 0.1136 - val_tn: 4756.0000 - val_auc: 0.7500 - val_prc: 0.1325\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1604 - Sensitivity: 0.0278 - tn: 14609.0000 - auc: 0.7172 - prc: 0.1047 - val_loss: 0.2300 - val_Sensitivity: 0.1182 - val_tn: 4752.0000 - val_auc: 0.7465 - val_prc: 0.1276\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1572 - Sensitivity: 0.0226 - tn: 14622.0000 - auc: 0.7284 - prc: 0.1137 - val_loss: 0.2275 - val_Sensitivity: 0.1000 - val_tn: 4763.0000 - val_auc: 0.7431 - val_prc: 0.1243\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1598 - Sensitivity: 0.0226 - tn: 14622.0000 - auc: 0.7141 - prc: 0.1050 - val_loss: 0.2251 - val_Sensitivity: 0.1000 - val_tn: 4767.0000 - val_auc: 0.7434 - val_prc: 0.1246\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1572 - Sensitivity: 0.0122 - tn: 14629.0000 - auc: 0.7276 - prc: 0.0993 - val_loss: 0.2265 - val_Sensitivity: 0.1091 - val_tn: 4758.0000 - val_auc: 0.7425 - val_prc: 0.1242\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1533 - Sensitivity: 0.0191 - tn: 14636.0000 - auc: 0.7484 - prc: 0.1278 - val_loss: 0.2284 - val_Sensitivity: 0.1273 - val_tn: 4756.0000 - val_auc: 0.7436 - val_prc: 0.1242\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1540 - Sensitivity: 0.0278 - tn: 14630.0000 - auc: 0.7360 - prc: 0.1214 - val_loss: 0.2295 - val_Sensitivity: 0.1182 - val_tn: 4745.0000 - val_auc: 0.7440 - val_prc: 0.1227\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1533 - Sensitivity: 0.0209 - tn: 14638.0000 - auc: 0.7430 - prc: 0.1182 - val_loss: 0.2203 - val_Sensitivity: 0.0818 - val_tn: 4781.0000 - val_auc: 0.7443 - val_prc: 0.1238\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1532 - Sensitivity: 0.0209 - tn: 14639.0000 - auc: 0.7458 - prc: 0.1296 - val_loss: 0.2255 - val_Sensitivity: 0.1000 - val_tn: 4765.0000 - val_auc: 0.7436 - val_prc: 0.1240\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1517 - Sensitivity: 0.0226 - tn: 14643.0000 - auc: 0.7397 - prc: 0.1185 - val_loss: 0.2219 - val_Sensitivity: 0.0864 - val_tn: 4780.0000 - val_auc: 0.7429 - val_prc: 0.1227\n",
      "Epoch 1/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1686 - Sensitivity: 0.0397 - tn: 14603.0000 - auc: 0.6914 - prc: 0.0953 - val_loss: 0.1968 - val_Sensitivity: 0.1333 - val_tn: 4856.0000 - val_auc: 0.7733 - val_prc: 0.1646\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1607 - Sensitivity: 0.0224 - tn: 14625.0000 - auc: 0.7205 - prc: 0.1092 - val_loss: 0.2057 - val_Sensitivity: 0.1167 - val_tn: 4850.0000 - val_auc: 0.7706 - val_prc: 0.1566\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1634 - Sensitivity: 0.0259 - tn: 14627.0000 - auc: 0.7120 - prc: 0.0997 - val_loss: 0.2073 - val_Sensitivity: 0.1167 - val_tn: 4847.0000 - val_auc: 0.7669 - val_prc: 0.1492\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1595 - Sensitivity: 0.0241 - tn: 14624.0000 - auc: 0.7220 - prc: 0.1051 - val_loss: 0.2149 - val_Sensitivity: 0.1278 - val_tn: 4825.0000 - val_auc: 0.7629 - val_prc: 0.1391\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1593 - Sensitivity: 0.0207 - tn: 14624.0000 - auc: 0.7234 - prc: 0.1048 - val_loss: 0.2166 - val_Sensitivity: 0.1222 - val_tn: 4827.0000 - val_auc: 0.7598 - val_prc: 0.1317\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1557 - Sensitivity: 0.0241 - tn: 14630.0000 - auc: 0.7336 - prc: 0.1104 - val_loss: 0.2107 - val_Sensitivity: 0.1056 - val_tn: 4841.0000 - val_auc: 0.7589 - val_prc: 0.1265\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1556 - Sensitivity: 0.0052 - tn: 14640.0000 - auc: 0.7335 - prc: 0.1108 - val_loss: 0.2017 - val_Sensitivity: 0.0889 - val_tn: 4857.0000 - val_auc: 0.7598 - val_prc: 0.1339\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1581 - Sensitivity: 0.0052 - tn: 14648.0000 - auc: 0.7214 - prc: 0.1001 - val_loss: 0.2150 - val_Sensitivity: 0.1056 - val_tn: 4833.0000 - val_auc: 0.7563 - val_prc: 0.1228\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1561 - Sensitivity: 0.0172 - tn: 14638.0000 - auc: 0.7325 - prc: 0.1071 - val_loss: 0.2167 - val_Sensitivity: 0.1056 - val_tn: 4833.0000 - val_auc: 0.7550 - val_prc: 0.1215\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1541 - Sensitivity: 0.0121 - tn: 14640.0000 - auc: 0.7346 - prc: 0.1075 - val_loss: 0.2085 - val_Sensitivity: 0.1000 - val_tn: 4850.0000 - val_auc: 0.7558 - val_prc: 0.1255\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1522 - Sensitivity: 0.0103 - tn: 14640.0000 - auc: 0.7473 - prc: 0.1183 - val_loss: 0.2151 - val_Sensitivity: 0.1056 - val_tn: 4839.0000 - val_auc: 0.7536 - val_prc: 0.1213\n",
      "Epoch 1/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1703 - Sensitivity: 0.0268 - tn: 14598.0000 - auc: 0.6917 - prc: 0.0919 - val_loss: 0.2080 - val_Sensitivity: 0.1341 - val_tn: 4821.0000 - val_auc: 0.7901 - val_prc: 0.1573\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1706 - Sensitivity: 0.0134 - tn: 14596.0000 - auc: 0.7050 - prc: 0.0952 - val_loss: 0.2080 - val_Sensitivity: 0.0950 - val_tn: 4843.0000 - val_auc: 0.7872 - val_prc: 0.1521\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1648 - Sensitivity: 0.0117 - tn: 14607.0000 - auc: 0.7064 - prc: 0.0966 - val_loss: 0.2139 - val_Sensitivity: 0.1006 - val_tn: 4835.0000 - val_auc: 0.7832 - val_prc: 0.1471\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1674 - Sensitivity: 0.0151 - tn: 14595.0000 - auc: 0.7031 - prc: 0.0951 - val_loss: 0.2150 - val_Sensitivity: 0.1006 - val_tn: 4833.0000 - val_auc: 0.7816 - val_prc: 0.1438\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1674 - Sensitivity: 0.0184 - tn: 14613.0000 - auc: 0.7025 - prc: 0.1007 - val_loss: 0.2082 - val_Sensitivity: 0.0950 - val_tn: 4849.0000 - val_auc: 0.7816 - val_prc: 0.1425\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1678 - Sensitivity: 0.0201 - tn: 14619.0000 - auc: 0.6913 - prc: 0.0986 - val_loss: 0.2150 - val_Sensitivity: 0.0950 - val_tn: 4835.0000 - val_auc: 0.7782 - val_prc: 0.1362\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1602 - Sensitivity: 0.0234 - tn: 14607.0000 - auc: 0.7314 - prc: 0.1157 - val_loss: 0.2159 - val_Sensitivity: 0.0782 - val_tn: 4834.0000 - val_auc: 0.7787 - val_prc: 0.1361\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1629 - Sensitivity: 0.0167 - tn: 14612.0000 - auc: 0.7146 - prc: 0.1075 - val_loss: 0.2093 - val_Sensitivity: 0.0726 - val_tn: 4849.0000 - val_auc: 0.7789 - val_prc: 0.1380\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1576 - Sensitivity: 0.0117 - tn: 14626.0000 - auc: 0.7293 - prc: 0.1184 - val_loss: 0.2142 - val_Sensitivity: 0.0726 - val_tn: 4843.0000 - val_auc: 0.7772 - val_prc: 0.1332\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1612 - Sensitivity: 0.0100 - tn: 14623.0000 - auc: 0.7247 - prc: 0.1065 - val_loss: 0.2170 - val_Sensitivity: 0.0726 - val_tn: 4842.0000 - val_auc: 0.7769 - val_prc: 0.1347\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1605 - Sensitivity: 0.0100 - tn: 14620.0000 - auc: 0.7298 - prc: 0.1063 - val_loss: 0.2146 - val_Sensitivity: 0.0726 - val_tn: 4852.0000 - val_auc: 0.7755 - val_prc: 0.1313\n",
      "Epoch 1/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1704 - Sensitivity: 0.0251 - tn: 14599.0000 - auc: 0.6999 - prc: 0.0983 - val_loss: 0.2099 - val_Sensitivity: 0.1173 - val_tn: 4828.0000 - val_auc: 0.7852 - val_prc: 0.1526\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1682 - Sensitivity: 0.0301 - tn: 14611.0000 - auc: 0.7000 - prc: 0.1083 - val_loss: 0.2202 - val_Sensitivity: 0.1229 - val_tn: 4827.0000 - val_auc: 0.7813 - val_prc: 0.1432\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1628 - Sensitivity: 0.0151 - tn: 14614.0000 - auc: 0.7157 - prc: 0.1056 - val_loss: 0.2348 - val_Sensitivity: 0.1285 - val_tn: 4799.0000 - val_auc: 0.7775 - val_prc: 0.1291\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1638 - Sensitivity: 0.0251 - tn: 14610.0000 - auc: 0.7139 - prc: 0.1057 - val_loss: 0.2218 - val_Sensitivity: 0.1173 - val_tn: 4819.0000 - val_auc: 0.7760 - val_prc: 0.1342\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1638 - Sensitivity: 0.0268 - tn: 14613.0000 - auc: 0.7184 - prc: 0.1190 - val_loss: 0.2223 - val_Sensitivity: 0.1006 - val_tn: 4811.0000 - val_auc: 0.7745 - val_prc: 0.1213\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1651 - Sensitivity: 0.0134 - tn: 14619.0000 - auc: 0.6964 - prc: 0.0993 - val_loss: 0.2162 - val_Sensitivity: 0.0894 - val_tn: 4824.0000 - val_auc: 0.7730 - val_prc: 0.1230\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1637 - Sensitivity: 0.0167 - tn: 14613.0000 - auc: 0.7049 - prc: 0.1041 - val_loss: 0.2172 - val_Sensitivity: 0.0726 - val_tn: 4837.0000 - val_auc: 0.7720 - val_prc: 0.1250\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1613 - Sensitivity: 0.0151 - tn: 14621.0000 - auc: 0.7162 - prc: 0.1108 - val_loss: 0.2206 - val_Sensitivity: 0.0726 - val_tn: 4820.0000 - val_auc: 0.7702 - val_prc: 0.1183\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1566 - Sensitivity: 0.0117 - tn: 14616.0000 - auc: 0.7301 - prc: 0.1143 - val_loss: 0.2228 - val_Sensitivity: 0.0782 - val_tn: 4820.0000 - val_auc: 0.7686 - val_prc: 0.1163\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1573 - Sensitivity: 0.0151 - tn: 14619.0000 - auc: 0.7381 - prc: 0.1237 - val_loss: 0.2086 - val_Sensitivity: 0.0559 - val_tn: 4847.0000 - val_auc: 0.7694 - val_prc: 0.1285\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1578 - Sensitivity: 0.0151 - tn: 14632.0000 - auc: 0.7322 - prc: 0.1257 - val_loss: 0.2112 - val_Sensitivity: 0.0559 - val_tn: 4848.0000 - val_auc: 0.7694 - val_prc: 0.1335\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1585 - Sensitivity: 0.0117 - tn: 14635.0000 - auc: 0.7240 - prc: 0.1250 - val_loss: 0.2110 - val_Sensitivity: 0.0615 - val_tn: 4852.0000 - val_auc: 0.7697 - val_prc: 0.1329\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1544 - Sensitivity: 0.0151 - tn: 14625.0000 - auc: 0.7438 - prc: 0.1226 - val_loss: 0.2140 - val_Sensitivity: 0.0615 - val_tn: 4848.0000 - val_auc: 0.7676 - val_prc: 0.1301\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1566 - Sensitivity: 0.0067 - tn: 14625.0000 - auc: 0.7351 - prc: 0.1145 - val_loss: 0.2170 - val_Sensitivity: 0.0670 - val_tn: 4840.0000 - val_auc: 0.7665 - val_prc: 0.1218\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1544 - Sensitivity: 0.0100 - tn: 14639.0000 - auc: 0.7422 - prc: 0.1268 - val_loss: 0.2161 - val_Sensitivity: 0.0615 - val_tn: 4842.0000 - val_auc: 0.7642 - val_prc: 0.1204\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1555 - Sensitivity: 0.0100 - tn: 14629.0000 - auc: 0.7397 - prc: 0.1144 - val_loss: 0.2184 - val_Sensitivity: 0.0615 - val_tn: 4827.0000 - val_auc: 0.7642 - val_prc: 0.1148\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1529 - Sensitivity: 0.0151 - tn: 14629.0000 - auc: 0.7467 - prc: 0.1296 - val_loss: 0.2093 - val_Sensitivity: 0.0615 - val_tn: 4837.0000 - val_auc: 0.7644 - val_prc: 0.1169\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1538 - Sensitivity: 0.0067 - tn: 14637.0000 - auc: 0.7436 - prc: 0.1236 - val_loss: 0.2197 - val_Sensitivity: 0.0615 - val_tn: 4830.0000 - val_auc: 0.7639 - val_prc: 0.1144\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1520 - Sensitivity: 0.0251 - tn: 14632.0000 - auc: 0.7524 - prc: 0.1402 - val_loss: 0.2136 - val_Sensitivity: 0.0559 - val_tn: 4835.0000 - val_auc: 0.7648 - val_prc: 0.1156\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1526 - Sensitivity: 0.0067 - tn: 14628.0000 - auc: 0.7520 - prc: 0.1295 - val_loss: 0.2259 - val_Sensitivity: 0.0726 - val_tn: 4825.0000 - val_auc: 0.7627 - val_prc: 0.1078\n"
     ]
    }
   ],
   "source": [
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7381540667895923,\n",
       " 0.7516799466418933,\n",
       " 0.7832669683798719,\n",
       " 0.800528775993916,\n",
       " 0.772037832148239]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))\n",
    "ann_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(dX_train[0], dy_train[0])\n",
    "def rf_feat_importance(model, X):\n",
    "    return pd.DataFrame({'cols':X.columns, 'imp':model.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "fi = rf_feat_importance(model, X)\n",
    "fi[:10]\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fit(dX_train[0], dy_train[0])\n",
    "fi = rf_feat_importance(model2, X)\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lrpreds = []\n",
    "model3 = LogisticRegression()\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.769 (0.738-0.8)\n"
     ]
    }
   ],
   "source": [
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print('Neural Network:', round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.724 (0.703-0.745)\n"
     ]
    }
   ],
   "source": [
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.149 (0.107-0.19)\n"
     ]
    }
   ],
   "source": [
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.097 (0.086-0.108)\n"
     ]
    }
   ],
   "source": [
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hep_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_ann_tpr = []\n",
    "hep_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_tpr.append(tpr)\n",
    "    hep_ann_fpr.append(fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "316\n",
      "362\n",
      "353\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(hep_ann_tpr[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_ann_tpr' (list)\n",
      "Stored 'mean_hep_ann_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    diff = len(hep_ann_tpr[x]) - 310\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_ann_tpr[x]))\n",
    "        hep_ann_tpr[x] = np.delete(hep_ann_tpr[x],ind)\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_ann_fpr[x]) - 310\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_ann_fpr[x]))\n",
    "        hep_ann_fpr[x] = np.delete(hep_ann_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_hep_ann_tpr = [np.mean(k) for k in zip(*hep_ann_tpr)]\n",
    "mean_hep_ann_fpr = [np.mean(k) for k in zip(*hep_ann_fpr)]\n",
    "%store mean_hep_ann_tpr\n",
    "%store mean_hep_ann_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_lr_tpr' (list)\n",
      "Stored 'mean_hep_lr_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hep_lr_tpr = []\n",
    "hep_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_tpr.append(tpr)\n",
    "    hep_lr_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_tpr[x]) - 310\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_tpr[x]))\n",
    "        hep_lr_tpr[x] = np.delete(hep_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_fpr[x]) - 310\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_fpr[x]))\n",
    "        hep_lr_fpr[x] = np.delete(hep_lr_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_hep_lr_tpr = [np.mean(k) for k in zip(*hep_lr_tpr)]\n",
    "mean_hep_lr_fpr = [np.mean(k) for k in zip(*hep_lr_fpr)]\n",
    "%store mean_hep_lr_tpr\n",
    "%store mean_hep_lr_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hep_lr_rec = []\n",
    "hep_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_rec.append(rec)\n",
    "    hep_lr_prec.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4970\n",
      "4982\n",
      "4866\n",
      "5047\n",
      "5047\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(hep_lr_rec[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_lr_rec' (list)\n",
      "Stored 'mean_hep_lr_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_rec[x]) - 4800\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_rec[x]))\n",
    "        hep_lr_rec[x] = np.delete(hep_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_prec[x]) - 4800\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_prec[x]))\n",
    "        hep_lr_prec[x] = np.delete(hep_lr_prec[x],ind)\n",
    "\n",
    "mean_hep_lr_rec = [np.mean(k) for k in zip(*hep_lr_rec)]\n",
    "mean_hep_lr_prec = [np.mean(k) for k in zip(*hep_lr_prec)]\n",
    "%store mean_hep_lr_rec\n",
    "%store mean_hep_lr_prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe544416c70>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mean_hep_lr_rec, mean_hep_lr_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_ann_rec' (list)\n",
      "Stored 'mean_hep_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hep_ann_rec = []\n",
    "hep_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_rec.append(rec)\n",
    "    hep_ann_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_ann_rec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_ann_rec[x]))\n",
    "        hep_ann_rec[x] = np.delete(hep_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_ann_prec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_ann_prec[x]))\n",
    "        hep_ann_prec[x] = np.delete(hep_ann_prec[x],ind)\n",
    "\n",
    "mean_hep_ann_rec = [np.mean(k) for k in zip(*hep_ann_rec)]\n",
    "mean_hep_ann_prec = [np.mean(k) for k in zip(*hep_ann_prec)]\n",
    "%store mean_hep_ann_rec\n",
    "%store mean_hep_ann_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hep_ann_tpr = []\n",
    "hep_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_tpr.append(tpr)\n",
    "    hep_ann_fpr.append(fpr)\n",
    "hep_ann_tpr_array = [np.array(x) for x in hep_ann_tpr]\n",
    "mean_hep_ann_tpr = [np.mean(k) for k in zip(*hep_ann_tpr_array)]\n",
    "hep_ann_fpr_array = [np.array(x) for x in hep_ann_fpr]\n",
    "mean_hep_ann_fpr = [np.mean(k) for k in zip(*hep_ann_fpr_array)]\n",
    "%store mean_hep_ann_tpr\n",
    "%store mean_hep_ann_fpr\n",
    "\n",
    "hep_lr_tpr = []\n",
    "hep_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_tpr.append(tpr)\n",
    "    hep_lr_fpr.append(fpr)\n",
    "hep_lr_tpr_array = [np.array(x) for x in hep_lr_tpr]\n",
    "mean_hep_lr_tpr = [np.mean(k) for k in zip(*hep_lr_tpr_array)]\n",
    "hep_lr_fpr_array = [np.array(x) for x in hep_lr_fpr]\n",
    "mean_hep_lr_fpr = [np.mean(k) for k in zip(*hep_lr_fpr_array)]\n",
    "%store mean_hep_lr_tpr\n",
    "%store mean_hep_lr_fpr\n",
    "hep_ann_rec = []\n",
    "hep_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_rec.append(rec)\n",
    "    hep_ann_prec.append(prec)\n",
    "hep_ann_rec_array = [np.array(x) for x in hep_ann_rec]\n",
    "mean_hep_ann_rec = [np.mean(k) for k in zip(*hep_ann_rec_array)]\n",
    "hep_ann_prec_array = [np.array(x) for x in hep_ann_prec]\n",
    "mean_hep_ann_prec = [np.mean(k) for k in zip(*hep_ann_prec_array)]\n",
    "%store mean_hep_ann_rec\n",
    "%store mean_hep_ann_prec\n",
    "\n",
    "hep_lr_rec = []\n",
    "hep_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_rec.append(rec)\n",
    "    hep_lr_prec.append(prec)\n",
    "hep_lr_rec_array = [np.array(x) for x in hep_lr_rec]\n",
    "mean_hep_lr_rec = [np.mean(k) for k in zip(*hep_lr_rec_array)]\n",
    "hep_lr_prec_array = [np.array(x) for x in hep_lr_prec]\n",
    "mean_hep_lr_prec = [np.mean(k) for k in zip(*hep_lr_prec_array)]\n",
    "%store mean_hep_lr_rec\n",
    "%store mean_hep_lr_prec\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
