{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_hep.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.drop(['CASEID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25403, 78)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['HEP_BILELEAKAGE']\n",
    "X = data.drop(['HEP_BILELEAKAGE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for x in range(0,5):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5081\n",
      "5081\n",
      "5081\n",
      "5081\n",
      "5081\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(d['test1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['HEP_BILELEAKAGE'], axis=1))\n",
    "        dy_train.append(d[x]['HEP_BILELEAKAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['HEP_BILELEAKAGE'], axis=1))\n",
    "        dy_test.append(d[x]['HEP_BILELEAKAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "input_shape = [dX_train[1].shape[1]]\n",
    "model4 = tf.keras.Sequential()\n",
    "model4.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(tf.keras.layers.BatchNormalization())\n",
    "for _ in range(2):\n",
    "    model4.add(tf.keras.layers.Dense(1000))\n",
    "    model4.add(tf.keras.layers.BatchNormalization())\n",
    "    model4.add(tf.keras.layers.Dropout(0.8))\n",
    "    model4.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model4.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "metrics = [tf.keras.metrics.Recall(name='Sensitivity'), tf.keras.metrics.TrueNegatives(name='tn'), tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=25,\n",
    "    min_delta=1e-6,\n",
    "    restore_best_weights=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save untrained model\n",
    "model4.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "30/30 [==============================] - 2s 35ms/step - loss: 1.1939 - Sensitivity: 0.5806 - tn: 6442.0000 - auc: 0.5112 - prc: 0.0421 - val_loss: 0.8487 - val_Sensitivity: 0.9439 - val_tn: 260.0000 - val_auc: 0.4667 - val_prc: 0.0353\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0123 - Sensitivity: 0.4766 - tn: 7632.0000 - auc: 0.4978 - prc: 0.0427 - val_loss: 0.7918 - val_Sensitivity: 0.9133 - val_tn: 605.0000 - val_auc: 0.5122 - val_prc: 0.0396\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.8929 - Sensitivity: 0.4333 - tn: 8546.0000 - auc: 0.5072 - prc: 0.0494 - val_loss: 0.7153 - val_Sensitivity: 0.6837 - val_tn: 1878.0000 - val_auc: 0.5469 - val_prc: 0.0448\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7706 - Sensitivity: 0.3813 - tn: 9511.0000 - auc: 0.5085 - prc: 0.0465 - val_loss: 0.6370 - val_Sensitivity: 0.2908 - val_tn: 3851.0000 - val_auc: 0.5793 - val_prc: 0.0526\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.6719 - Sensitivity: 0.3449 - tn: 10401.0000 - auc: 0.5140 - prc: 0.0428 - val_loss: 0.5797 - val_Sensitivity: 0.1633 - val_tn: 4610.0000 - val_auc: 0.6025 - val_prc: 0.0618\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5910 - Sensitivity: 0.2808 - tn: 11095.0000 - auc: 0.4966 - prc: 0.0436 - val_loss: 0.5395 - val_Sensitivity: 0.1327 - val_tn: 4724.0000 - val_auc: 0.6228 - val_prc: 0.0703\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5235 - Sensitivity: 0.2322 - tn: 11634.0000 - auc: 0.5124 - prc: 0.0460 - val_loss: 0.5015 - val_Sensitivity: 0.1276 - val_tn: 4754.0000 - val_auc: 0.6351 - val_prc: 0.0765\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4819 - Sensitivity: 0.2201 - tn: 12104.0000 - auc: 0.5038 - prc: 0.0455 - val_loss: 0.4686 - val_Sensitivity: 0.1020 - val_tn: 4765.0000 - val_auc: 0.6461 - val_prc: 0.0808\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4382 - Sensitivity: 0.1958 - tn: 12536.0000 - auc: 0.4989 - prc: 0.0448 - val_loss: 0.4417 - val_Sensitivity: 0.0969 - val_tn: 4770.0000 - val_auc: 0.6512 - val_prc: 0.0832\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4035 - Sensitivity: 0.1300 - tn: 12841.0000 - auc: 0.5085 - prc: 0.0412 - val_loss: 0.4164 - val_Sensitivity: 0.0918 - val_tn: 4779.0000 - val_auc: 0.6586 - val_prc: 0.0851\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3735 - Sensitivity: 0.1404 - tn: 13125.0000 - auc: 0.4979 - prc: 0.0421 - val_loss: 0.3978 - val_Sensitivity: 0.1020 - val_tn: 4777.0000 - val_auc: 0.6651 - val_prc: 0.0865\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3473 - Sensitivity: 0.1213 - tn: 13356.0000 - auc: 0.4895 - prc: 0.0455 - val_loss: 0.3825 - val_Sensitivity: 0.1020 - val_tn: 4767.0000 - val_auc: 0.6717 - val_prc: 0.0878\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3306 - Sensitivity: 0.1334 - tn: 13535.0000 - auc: 0.5232 - prc: 0.0481 - val_loss: 0.3661 - val_Sensitivity: 0.1071 - val_tn: 4767.0000 - val_auc: 0.6762 - val_prc: 0.0887\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3128 - Sensitivity: 0.1023 - tn: 13703.0000 - auc: 0.5162 - prc: 0.0434 - val_loss: 0.3526 - val_Sensitivity: 0.1020 - val_tn: 4768.0000 - val_auc: 0.6802 - val_prc: 0.0891\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2931 - Sensitivity: 0.1248 - tn: 13789.0000 - auc: 0.5344 - prc: 0.0502 - val_loss: 0.3443 - val_Sensitivity: 0.1173 - val_tn: 4763.0000 - val_auc: 0.6836 - val_prc: 0.0894\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2882 - Sensitivity: 0.0971 - tn: 13883.0000 - auc: 0.5234 - prc: 0.0472 - val_loss: 0.3296 - val_Sensitivity: 0.1122 - val_tn: 4759.0000 - val_auc: 0.6876 - val_prc: 0.0901\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2804 - Sensitivity: 0.0919 - tn: 13999.0000 - auc: 0.5056 - prc: 0.0474 - val_loss: 0.3220 - val_Sensitivity: 0.1122 - val_tn: 4753.0000 - val_auc: 0.6907 - val_prc: 0.0905\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2676 - Sensitivity: 0.0849 - tn: 14048.0000 - auc: 0.5305 - prc: 0.0506 - val_loss: 0.3189 - val_Sensitivity: 0.1327 - val_tn: 4747.0000 - val_auc: 0.6932 - val_prc: 0.0904\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2584 - Sensitivity: 0.0884 - tn: 14103.0000 - auc: 0.5505 - prc: 0.0538 - val_loss: 0.3158 - val_Sensitivity: 0.1378 - val_tn: 4744.0000 - val_auc: 0.6966 - val_prc: 0.0908\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2590 - Sensitivity: 0.0676 - tn: 14161.0000 - auc: 0.5213 - prc: 0.0451 - val_loss: 0.3091 - val_Sensitivity: 0.1378 - val_tn: 4740.0000 - val_auc: 0.6970 - val_prc: 0.0911\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2483 - Sensitivity: 0.0676 - tn: 14218.0000 - auc: 0.5399 - prc: 0.0545 - val_loss: 0.3010 - val_Sensitivity: 0.1378 - val_tn: 4741.0000 - val_auc: 0.6988 - val_prc: 0.0914\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2476 - Sensitivity: 0.0676 - tn: 14227.0000 - auc: 0.5156 - prc: 0.0513 - val_loss: 0.2984 - val_Sensitivity: 0.1378 - val_tn: 4737.0000 - val_auc: 0.7020 - val_prc: 0.0916\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2397 - Sensitivity: 0.0815 - tn: 14255.0000 - auc: 0.5530 - prc: 0.0555 - val_loss: 0.2983 - val_Sensitivity: 0.1429 - val_tn: 4732.0000 - val_auc: 0.7038 - val_prc: 0.0918\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2401 - Sensitivity: 0.0624 - tn: 14283.0000 - auc: 0.5335 - prc: 0.0492 - val_loss: 0.2942 - val_Sensitivity: 0.1429 - val_tn: 4729.0000 - val_auc: 0.7049 - val_prc: 0.0918\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2350 - Sensitivity: 0.0659 - tn: 14344.0000 - auc: 0.5305 - prc: 0.0536 - val_loss: 0.2884 - val_Sensitivity: 0.1378 - val_tn: 4728.0000 - val_auc: 0.7062 - val_prc: 0.0922\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2315 - Sensitivity: 0.0641 - tn: 14331.0000 - auc: 0.5356 - prc: 0.0551 - val_loss: 0.2862 - val_Sensitivity: 0.1429 - val_tn: 4727.0000 - val_auc: 0.7067 - val_prc: 0.0922\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2333 - Sensitivity: 0.0537 - tn: 14340.0000 - auc: 0.5392 - prc: 0.0515 - val_loss: 0.2819 - val_Sensitivity: 0.1480 - val_tn: 4726.0000 - val_auc: 0.7087 - val_prc: 0.0926\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2285 - Sensitivity: 0.0624 - tn: 14366.0000 - auc: 0.5396 - prc: 0.0498 - val_loss: 0.2760 - val_Sensitivity: 0.1429 - val_tn: 4723.0000 - val_auc: 0.7091 - val_prc: 0.0925\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2265 - Sensitivity: 0.0503 - tn: 14395.0000 - auc: 0.5435 - prc: 0.0516 - val_loss: 0.2717 - val_Sensitivity: 0.1429 - val_tn: 4715.0000 - val_auc: 0.7101 - val_prc: 0.0927\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2282 - Sensitivity: 0.0572 - tn: 14390.0000 - auc: 0.5514 - prc: 0.0544 - val_loss: 0.2704 - val_Sensitivity: 0.1531 - val_tn: 4710.0000 - val_auc: 0.7123 - val_prc: 0.0932\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.2241 - Sensitivity: 0.0572 - tn: 14403.0000 - auc: 0.5520 - prc: 0.0567 - val_loss: 0.2736 - val_Sensitivity: 0.1531 - val_tn: 4703.0000 - val_auc: 0.7122 - val_prc: 0.0932\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2215 - Sensitivity: 0.0520 - tn: 14422.0000 - auc: 0.5579 - prc: 0.0577 - val_loss: 0.2747 - val_Sensitivity: 0.1531 - val_tn: 4701.0000 - val_auc: 0.7129 - val_prc: 0.0933\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.2252 - Sensitivity: 0.0607 - tn: 14392.0000 - auc: 0.5551 - prc: 0.0578 - val_loss: 0.2695 - val_Sensitivity: 0.1531 - val_tn: 4704.0000 - val_auc: 0.7129 - val_prc: 0.0934\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2207 - Sensitivity: 0.0537 - tn: 14431.0000 - auc: 0.5679 - prc: 0.0599 - val_loss: 0.2631 - val_Sensitivity: 0.1480 - val_tn: 4708.0000 - val_auc: 0.7137 - val_prc: 0.0936\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.2188 - Sensitivity: 0.0607 - tn: 14443.0000 - auc: 0.5627 - prc: 0.0591 - val_loss: 0.2640 - val_Sensitivity: 0.1480 - val_tn: 4704.0000 - val_auc: 0.7141 - val_prc: 0.0935\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2234 - Sensitivity: 0.0364 - tn: 14441.0000 - auc: 0.5531 - prc: 0.0495 - val_loss: 0.2637 - val_Sensitivity: 0.1582 - val_tn: 4701.0000 - val_auc: 0.7136 - val_prc: 0.0932\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2134 - Sensitivity: 0.0763 - tn: 14440.0000 - auc: 0.5750 - prc: 0.0660 - val_loss: 0.2645 - val_Sensitivity: 0.1633 - val_tn: 4699.0000 - val_auc: 0.7141 - val_prc: 0.0932\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2209 - Sensitivity: 0.0537 - tn: 14434.0000 - auc: 0.5606 - prc: 0.0567 - val_loss: 0.2628 - val_Sensitivity: 0.1633 - val_tn: 4700.0000 - val_auc: 0.7143 - val_prc: 0.0935\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2167 - Sensitivity: 0.0520 - tn: 14461.0000 - auc: 0.5699 - prc: 0.0588 - val_loss: 0.2586 - val_Sensitivity: 0.1582 - val_tn: 4702.0000 - val_auc: 0.7155 - val_prc: 0.0935\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2151 - Sensitivity: 0.0624 - tn: 14432.0000 - auc: 0.5783 - prc: 0.0605 - val_loss: 0.2561 - val_Sensitivity: 0.1531 - val_tn: 4702.0000 - val_auc: 0.7158 - val_prc: 0.0936\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2170 - Sensitivity: 0.0433 - tn: 14480.0000 - auc: 0.5648 - prc: 0.0570 - val_loss: 0.2581 - val_Sensitivity: 0.1633 - val_tn: 4700.0000 - val_auc: 0.7164 - val_prc: 0.0936\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2087 - Sensitivity: 0.0520 - tn: 14470.0000 - auc: 0.5879 - prc: 0.0669 - val_loss: 0.2549 - val_Sensitivity: 0.1633 - val_tn: 4700.0000 - val_auc: 0.7172 - val_prc: 0.0938\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2117 - Sensitivity: 0.0503 - tn: 14456.0000 - auc: 0.5861 - prc: 0.0628 - val_loss: 0.2535 - val_Sensitivity: 0.1633 - val_tn: 4700.0000 - val_auc: 0.7165 - val_prc: 0.0937\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2112 - Sensitivity: 0.0485 - tn: 14471.0000 - auc: 0.5950 - prc: 0.0600 - val_loss: 0.2551 - val_Sensitivity: 0.1684 - val_tn: 4696.0000 - val_auc: 0.7169 - val_prc: 0.0939\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2103 - Sensitivity: 0.0468 - tn: 14471.0000 - auc: 0.6008 - prc: 0.0613 - val_loss: 0.2546 - val_Sensitivity: 0.1684 - val_tn: 4695.0000 - val_auc: 0.7171 - val_prc: 0.0939\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2147 - Sensitivity: 0.0520 - tn: 14463.0000 - auc: 0.5669 - prc: 0.0603 - val_loss: 0.2513 - val_Sensitivity: 0.1633 - val_tn: 4699.0000 - val_auc: 0.7180 - val_prc: 0.0941\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2201 - Sensitivity: 0.0433 - tn: 14452.0000 - auc: 0.5615 - prc: 0.0544 - val_loss: 0.2493 - val_Sensitivity: 0.1684 - val_tn: 4700.0000 - val_auc: 0.7177 - val_prc: 0.0941\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2100 - Sensitivity: 0.0728 - tn: 14485.0000 - auc: 0.5850 - prc: 0.0758 - val_loss: 0.2480 - val_Sensitivity: 0.1633 - val_tn: 4700.0000 - val_auc: 0.7190 - val_prc: 0.0940\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2128 - Sensitivity: 0.0555 - tn: 14484.0000 - auc: 0.5778 - prc: 0.0616 - val_loss: 0.2521 - val_Sensitivity: 0.1684 - val_tn: 4698.0000 - val_auc: 0.7181 - val_prc: 0.0939\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2118 - Sensitivity: 0.0537 - tn: 14500.0000 - auc: 0.5828 - prc: 0.0618 - val_loss: 0.2511 - val_Sensitivity: 0.1735 - val_tn: 4697.0000 - val_auc: 0.7187 - val_prc: 0.0939\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2015 - Sensitivity: 0.0416 - tn: 14504.0000 - auc: 0.6053 - prc: 0.0674 - val_loss: 0.2500 - val_Sensitivity: 0.1735 - val_tn: 4694.0000 - val_auc: 0.7186 - val_prc: 0.0940\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2046 - Sensitivity: 0.0451 - tn: 14501.0000 - auc: 0.6023 - prc: 0.0709 - val_loss: 0.2537 - val_Sensitivity: 0.1735 - val_tn: 4690.0000 - val_auc: 0.7190 - val_prc: 0.0941\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2042 - Sensitivity: 0.0832 - tn: 14502.0000 - auc: 0.6045 - prc: 0.0793 - val_loss: 0.2509 - val_Sensitivity: 0.1735 - val_tn: 4692.0000 - val_auc: 0.7182 - val_prc: 0.0939\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2087 - Sensitivity: 0.0520 - tn: 14474.0000 - auc: 0.6140 - prc: 0.0668 - val_loss: 0.2511 - val_Sensitivity: 0.1735 - val_tn: 4689.0000 - val_auc: 0.7182 - val_prc: 0.0938\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2032 - Sensitivity: 0.0485 - tn: 14501.0000 - auc: 0.5984 - prc: 0.0676 - val_loss: 0.2516 - val_Sensitivity: 0.1735 - val_tn: 4689.0000 - val_auc: 0.7190 - val_prc: 0.0938\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2064 - Sensitivity: 0.0537 - tn: 14497.0000 - auc: 0.6056 - prc: 0.0721 - val_loss: 0.2543 - val_Sensitivity: 0.1735 - val_tn: 4684.0000 - val_auc: 0.7188 - val_prc: 0.0937\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2043 - Sensitivity: 0.0520 - tn: 14514.0000 - auc: 0.6004 - prc: 0.0666 - val_loss: 0.2552 - val_Sensitivity: 0.1786 - val_tn: 4683.0000 - val_auc: 0.7181 - val_prc: 0.0935\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2005 - Sensitivity: 0.0416 - tn: 14511.0000 - auc: 0.6255 - prc: 0.0698 - val_loss: 0.2549 - val_Sensitivity: 0.1837 - val_tn: 4680.0000 - val_auc: 0.7183 - val_prc: 0.0935\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2024 - Sensitivity: 0.0572 - tn: 14498.0000 - auc: 0.6167 - prc: 0.0743 - val_loss: 0.2527 - val_Sensitivity: 0.1786 - val_tn: 4680.0000 - val_auc: 0.7188 - val_prc: 0.0936\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2099 - Sensitivity: 0.0520 - tn: 14496.0000 - auc: 0.5975 - prc: 0.0654 - val_loss: 0.2506 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7200 - val_prc: 0.0938\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2063 - Sensitivity: 0.0416 - tn: 14519.0000 - auc: 0.6043 - prc: 0.0633 - val_loss: 0.2479 - val_Sensitivity: 0.1735 - val_tn: 4683.0000 - val_auc: 0.7196 - val_prc: 0.0937\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2012 - Sensitivity: 0.0537 - tn: 14507.0000 - auc: 0.6137 - prc: 0.0719 - val_loss: 0.2482 - val_Sensitivity: 0.1735 - val_tn: 4682.0000 - val_auc: 0.7207 - val_prc: 0.0940\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2021 - Sensitivity: 0.0572 - tn: 14498.0000 - auc: 0.6173 - prc: 0.0704 - val_loss: 0.2508 - val_Sensitivity: 0.1786 - val_tn: 4680.0000 - val_auc: 0.7200 - val_prc: 0.0939\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2032 - Sensitivity: 0.0485 - tn: 14473.0000 - auc: 0.6166 - prc: 0.0681 - val_loss: 0.2480 - val_Sensitivity: 0.1735 - val_tn: 4682.0000 - val_auc: 0.7201 - val_prc: 0.0941\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2062 - Sensitivity: 0.0503 - tn: 14494.0000 - auc: 0.6066 - prc: 0.0691 - val_loss: 0.2494 - val_Sensitivity: 0.1735 - val_tn: 4680.0000 - val_auc: 0.7196 - val_prc: 0.0939\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2057 - Sensitivity: 0.0503 - tn: 14497.0000 - auc: 0.6046 - prc: 0.0692 - val_loss: 0.2491 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7197 - val_prc: 0.0939\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2000 - Sensitivity: 0.0503 - tn: 14529.0000 - auc: 0.6215 - prc: 0.0738 - val_loss: 0.2501 - val_Sensitivity: 0.1786 - val_tn: 4680.0000 - val_auc: 0.7196 - val_prc: 0.0939\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2032 - Sensitivity: 0.0537 - tn: 14506.0000 - auc: 0.6200 - prc: 0.0732 - val_loss: 0.2516 - val_Sensitivity: 0.1837 - val_tn: 4678.0000 - val_auc: 0.7194 - val_prc: 0.0936\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2060 - Sensitivity: 0.0451 - tn: 14512.0000 - auc: 0.5935 - prc: 0.0644 - val_loss: 0.2536 - val_Sensitivity: 0.1837 - val_tn: 4678.0000 - val_auc: 0.7201 - val_prc: 0.0939\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2033 - Sensitivity: 0.0537 - tn: 14500.0000 - auc: 0.6226 - prc: 0.0697 - val_loss: 0.2531 - val_Sensitivity: 0.1837 - val_tn: 4677.0000 - val_auc: 0.7197 - val_prc: 0.0938\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1980 - Sensitivity: 0.0555 - tn: 14515.0000 - auc: 0.6263 - prc: 0.0789 - val_loss: 0.2504 - val_Sensitivity: 0.1837 - val_tn: 4679.0000 - val_auc: 0.7195 - val_prc: 0.0938\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2000 - Sensitivity: 0.0381 - tn: 14515.0000 - auc: 0.6161 - prc: 0.0686 - val_loss: 0.2486 - val_Sensitivity: 0.1786 - val_tn: 4681.0000 - val_auc: 0.7198 - val_prc: 0.0940\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2045 - Sensitivity: 0.0485 - tn: 14503.0000 - auc: 0.6101 - prc: 0.0689 - val_loss: 0.2482 - val_Sensitivity: 0.1735 - val_tn: 4681.0000 - val_auc: 0.7199 - val_prc: 0.0939\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2032 - Sensitivity: 0.0572 - tn: 14500.0000 - auc: 0.6111 - prc: 0.0755 - val_loss: 0.2446 - val_Sensitivity: 0.1735 - val_tn: 4684.0000 - val_auc: 0.7191 - val_prc: 0.0940\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1973 - Sensitivity: 0.0624 - tn: 14521.0000 - auc: 0.6317 - prc: 0.0822 - val_loss: 0.2458 - val_Sensitivity: 0.1735 - val_tn: 4682.0000 - val_auc: 0.7188 - val_prc: 0.0937\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2056 - Sensitivity: 0.0537 - tn: 14503.0000 - auc: 0.5921 - prc: 0.0668 - val_loss: 0.2503 - val_Sensitivity: 0.1786 - val_tn: 4676.0000 - val_auc: 0.7194 - val_prc: 0.0935\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1945 - Sensitivity: 0.0572 - tn: 14503.0000 - auc: 0.6301 - prc: 0.0807 - val_loss: 0.2496 - val_Sensitivity: 0.1786 - val_tn: 4678.0000 - val_auc: 0.7181 - val_prc: 0.0934\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1944 - Sensitivity: 0.0572 - tn: 14511.0000 - auc: 0.6440 - prc: 0.0774 - val_loss: 0.2472 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7186 - val_prc: 0.0935\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2018 - Sensitivity: 0.0364 - tn: 14519.0000 - auc: 0.6294 - prc: 0.0687 - val_loss: 0.2445 - val_Sensitivity: 0.1786 - val_tn: 4683.0000 - val_auc: 0.7191 - val_prc: 0.0934\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1984 - Sensitivity: 0.0468 - tn: 14508.0000 - auc: 0.6255 - prc: 0.0720 - val_loss: 0.2454 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7194 - val_prc: 0.0936\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1930 - Sensitivity: 0.0763 - tn: 14526.0000 - auc: 0.6455 - prc: 0.0984 - val_loss: 0.2444 - val_Sensitivity: 0.1786 - val_tn: 4681.0000 - val_auc: 0.7194 - val_prc: 0.0936\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1926 - Sensitivity: 0.0468 - tn: 14530.0000 - auc: 0.6473 - prc: 0.0806 - val_loss: 0.2428 - val_Sensitivity: 0.1786 - val_tn: 4681.0000 - val_auc: 0.7196 - val_prc: 0.0937\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1962 - Sensitivity: 0.0485 - tn: 14495.0000 - auc: 0.6285 - prc: 0.0747 - val_loss: 0.2439 - val_Sensitivity: 0.1786 - val_tn: 4680.0000 - val_auc: 0.7194 - val_prc: 0.0936\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1972 - Sensitivity: 0.0416 - tn: 14506.0000 - auc: 0.6379 - prc: 0.0719 - val_loss: 0.2469 - val_Sensitivity: 0.1786 - val_tn: 4678.0000 - val_auc: 0.7190 - val_prc: 0.0934\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1946 - Sensitivity: 0.0433 - tn: 14527.0000 - auc: 0.6377 - prc: 0.0757 - val_loss: 0.2490 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7185 - val_prc: 0.0933\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1944 - Sensitivity: 0.0607 - tn: 14543.0000 - auc: 0.6282 - prc: 0.0775 - val_loss: 0.2475 - val_Sensitivity: 0.1786 - val_tn: 4676.0000 - val_auc: 0.7189 - val_prc: 0.0933\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1981 - Sensitivity: 0.0381 - tn: 14517.0000 - auc: 0.6298 - prc: 0.0669 - val_loss: 0.2498 - val_Sensitivity: 0.1786 - val_tn: 4672.0000 - val_auc: 0.7188 - val_prc: 0.0932\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1953 - Sensitivity: 0.0589 - tn: 14527.0000 - auc: 0.6221 - prc: 0.0809 - val_loss: 0.2504 - val_Sensitivity: 0.1837 - val_tn: 4671.0000 - val_auc: 0.7187 - val_prc: 0.0933\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.1940 - Sensitivity: 0.0607 - tn: 14509.0000 - auc: 0.6370 - prc: 0.0819 - val_loss: 0.2493 - val_Sensitivity: 0.1837 - val_tn: 4671.0000 - val_auc: 0.7188 - val_prc: 0.0931\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.1979 - Sensitivity: 0.0485 - tn: 14515.0000 - auc: 0.6298 - prc: 0.0725 - val_loss: 0.2468 - val_Sensitivity: 0.1786 - val_tn: 4676.0000 - val_auc: 0.7189 - val_prc: 0.0931\n",
      "Epoch 91/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.1934 - Sensitivity: 0.0745 - tn: 14512.0000 - auc: 0.6472 - prc: 0.0913 - val_loss: 0.2451 - val_Sensitivity: 0.1786 - val_tn: 4676.0000 - val_auc: 0.7194 - val_prc: 0.0932\n",
      "Epoch 92/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.1943 - Sensitivity: 0.0537 - tn: 14526.0000 - auc: 0.6421 - prc: 0.0838 - val_loss: 0.2420 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7196 - val_prc: 0.0934\n",
      "Epoch 93/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1872 - Sensitivity: 0.0433 - tn: 14532.0000 - auc: 0.6588 - prc: 0.0797 - val_loss: 0.2430 - val_Sensitivity: 0.1786 - val_tn: 4678.0000 - val_auc: 0.7197 - val_prc: 0.0936\n",
      "Epoch 94/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1954 - Sensitivity: 0.0555 - tn: 14537.0000 - auc: 0.6410 - prc: 0.0804 - val_loss: 0.2418 - val_Sensitivity: 0.1786 - val_tn: 4679.0000 - val_auc: 0.7190 - val_prc: 0.0936\n",
      "Epoch 95/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1900 - Sensitivity: 0.0589 - tn: 14525.0000 - auc: 0.6512 - prc: 0.0879 - val_loss: 0.2379 - val_Sensitivity: 0.1786 - val_tn: 4680.0000 - val_auc: 0.7193 - val_prc: 0.0938\n",
      "Epoch 96/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1892 - Sensitivity: 0.0555 - tn: 14537.0000 - auc: 0.6447 - prc: 0.0868 - val_loss: 0.2392 - val_Sensitivity: 0.1786 - val_tn: 4681.0000 - val_auc: 0.7196 - val_prc: 0.0938\n",
      "Epoch 97/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1993 - Sensitivity: 0.0503 - tn: 14527.0000 - auc: 0.6276 - prc: 0.0774 - val_loss: 0.2398 - val_Sensitivity: 0.1786 - val_tn: 4680.0000 - val_auc: 0.7192 - val_prc: 0.0938\n",
      "Epoch 98/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1917 - Sensitivity: 0.0589 - tn: 14533.0000 - auc: 0.6387 - prc: 0.0857 - val_loss: 0.2415 - val_Sensitivity: 0.1786 - val_tn: 4676.0000 - val_auc: 0.7192 - val_prc: 0.0938\n",
      "Epoch 99/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1926 - Sensitivity: 0.0555 - tn: 14527.0000 - auc: 0.6413 - prc: 0.0818 - val_loss: 0.2438 - val_Sensitivity: 0.1837 - val_tn: 4675.0000 - val_auc: 0.7188 - val_prc: 0.0936\n",
      "Epoch 100/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1928 - Sensitivity: 0.0364 - tn: 14534.0000 - auc: 0.6443 - prc: 0.0728 - val_loss: 0.2472 - val_Sensitivity: 0.1888 - val_tn: 4667.0000 - val_auc: 0.7193 - val_prc: 0.0934\n",
      "Epoch 101/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1891 - Sensitivity: 0.0537 - tn: 14538.0000 - auc: 0.6397 - prc: 0.0811 - val_loss: 0.2488 - val_Sensitivity: 0.1888 - val_tn: 4663.0000 - val_auc: 0.7192 - val_prc: 0.0934\n",
      "Epoch 102/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1860 - Sensitivity: 0.0503 - tn: 14530.0000 - auc: 0.6653 - prc: 0.0892 - val_loss: 0.2494 - val_Sensitivity: 0.1888 - val_tn: 4663.0000 - val_auc: 0.7192 - val_prc: 0.0932\n",
      "Epoch 103/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1911 - Sensitivity: 0.0624 - tn: 14525.0000 - auc: 0.6511 - prc: 0.0855 - val_loss: 0.2514 - val_Sensitivity: 0.1837 - val_tn: 4659.0000 - val_auc: 0.7188 - val_prc: 0.0929\n",
      "Epoch 104/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1912 - Sensitivity: 0.0451 - tn: 14528.0000 - auc: 0.6463 - prc: 0.0786 - val_loss: 0.2506 - val_Sensitivity: 0.1837 - val_tn: 4662.0000 - val_auc: 0.7185 - val_prc: 0.0928\n",
      "Epoch 105/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1982 - Sensitivity: 0.0537 - tn: 14515.0000 - auc: 0.6173 - prc: 0.0750 - val_loss: 0.2486 - val_Sensitivity: 0.1837 - val_tn: 4664.0000 - val_auc: 0.7186 - val_prc: 0.0928\n",
      "Epoch 106/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1875 - Sensitivity: 0.0728 - tn: 14535.0000 - auc: 0.6464 - prc: 0.0879 - val_loss: 0.2511 - val_Sensitivity: 0.1837 - val_tn: 4663.0000 - val_auc: 0.7188 - val_prc: 0.0926\n",
      "Epoch 107/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1911 - Sensitivity: 0.0537 - tn: 14540.0000 - auc: 0.6410 - prc: 0.0852 - val_loss: 0.2492 - val_Sensitivity: 0.1786 - val_tn: 4663.0000 - val_auc: 0.7185 - val_prc: 0.0926\n",
      "Epoch 108/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1906 - Sensitivity: 0.0364 - tn: 14519.0000 - auc: 0.6479 - prc: 0.0718 - val_loss: 0.2421 - val_Sensitivity: 0.1786 - val_tn: 4675.0000 - val_auc: 0.7191 - val_prc: 0.0930\n",
      "Epoch 109/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1866 - Sensitivity: 0.0624 - tn: 14523.0000 - auc: 0.6556 - prc: 0.0907 - val_loss: 0.2397 - val_Sensitivity: 0.1786 - val_tn: 4676.0000 - val_auc: 0.7192 - val_prc: 0.0931\n",
      "Epoch 110/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1804 - Sensitivity: 0.0451 - tn: 14536.0000 - auc: 0.6770 - prc: 0.0932 - val_loss: 0.2429 - val_Sensitivity: 0.1786 - val_tn: 4667.0000 - val_auc: 0.7190 - val_prc: 0.0928\n",
      "Epoch 111/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1842 - Sensitivity: 0.0641 - tn: 14537.0000 - auc: 0.6559 - prc: 0.0912 - val_loss: 0.2457 - val_Sensitivity: 0.1786 - val_tn: 4665.0000 - val_auc: 0.7189 - val_prc: 0.0927\n",
      "Epoch 112/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1896 - Sensitivity: 0.0503 - tn: 14530.0000 - auc: 0.6467 - prc: 0.0842 - val_loss: 0.2454 - val_Sensitivity: 0.1786 - val_tn: 4665.0000 - val_auc: 0.7183 - val_prc: 0.0926\n",
      "Epoch 113/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1935 - Sensitivity: 0.0485 - tn: 14527.0000 - auc: 0.6361 - prc: 0.0755 - val_loss: 0.2465 - val_Sensitivity: 0.1786 - val_tn: 4662.0000 - val_auc: 0.7181 - val_prc: 0.0924\n",
      "Epoch 114/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1889 - Sensitivity: 0.0589 - tn: 14537.0000 - auc: 0.6504 - prc: 0.0893 - val_loss: 0.2455 - val_Sensitivity: 0.1786 - val_tn: 4663.0000 - val_auc: 0.7182 - val_prc: 0.0924\n",
      "Epoch 115/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1828 - Sensitivity: 0.0503 - tn: 14560.0000 - auc: 0.6654 - prc: 0.0867 - val_loss: 0.2453 - val_Sensitivity: 0.1786 - val_tn: 4661.0000 - val_auc: 0.7181 - val_prc: 0.0924\n",
      "Epoch 116/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1867 - Sensitivity: 0.0416 - tn: 14558.0000 - auc: 0.6521 - prc: 0.0815 - val_loss: 0.2476 - val_Sensitivity: 0.1786 - val_tn: 4662.0000 - val_auc: 0.7177 - val_prc: 0.0920\n",
      "Epoch 117/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1871 - Sensitivity: 0.0451 - tn: 14526.0000 - auc: 0.6496 - prc: 0.0806 - val_loss: 0.2482 - val_Sensitivity: 0.1786 - val_tn: 4660.0000 - val_auc: 0.7178 - val_prc: 0.0920\n",
      "Epoch 118/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1844 - Sensitivity: 0.0277 - tn: 14553.0000 - auc: 0.6605 - prc: 0.0747 - val_loss: 0.2468 - val_Sensitivity: 0.1786 - val_tn: 4660.0000 - val_auc: 0.7184 - val_prc: 0.0921\n",
      "Epoch 119/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1829 - Sensitivity: 0.0503 - tn: 14554.0000 - auc: 0.6694 - prc: 0.0892 - val_loss: 0.2453 - val_Sensitivity: 0.1786 - val_tn: 4663.0000 - val_auc: 0.7186 - val_prc: 0.0922\n",
      "Epoch 120/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1831 - Sensitivity: 0.0451 - tn: 14541.0000 - auc: 0.6755 - prc: 0.0855 - val_loss: 0.2432 - val_Sensitivity: 0.1786 - val_tn: 4668.0000 - val_auc: 0.7182 - val_prc: 0.0922\n",
      "Epoch 1/500\n",
      "30/30 [==============================] - 2s 29ms/step - loss: 1.1989 - Sensitivity: 0.5690 - tn: 6358.0000 - auc: 0.5058 - prc: 0.0426 - val_loss: 0.8515 - val_Sensitivity: 0.9624 - val_tn: 239.0000 - val_auc: 0.4977 - val_prc: 0.0371\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0309 - Sensitivity: 0.4872 - tn: 7590.0000 - auc: 0.5025 - prc: 0.0433 - val_loss: 0.7826 - val_Sensitivity: 0.8978 - val_tn: 675.0000 - val_auc: 0.5300 - val_prc: 0.0390\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.8958 - Sensitivity: 0.4719 - tn: 8586.0000 - auc: 0.5271 - prc: 0.0441 - val_loss: 0.7063 - val_Sensitivity: 0.6183 - val_tn: 2092.0000 - val_auc: 0.5484 - val_prc: 0.0420\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7579 - Sensitivity: 0.4157 - tn: 9548.0000 - auc: 0.5427 - prc: 0.0501 - val_loss: 0.6408 - val_Sensitivity: 0.3172 - val_tn: 3814.0000 - val_auc: 0.5730 - val_prc: 0.0484\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.6758 - Sensitivity: 0.3322 - tn: 10353.0000 - auc: 0.5146 - prc: 0.0441 - val_loss: 0.5838 - val_Sensitivity: 0.1398 - val_tn: 4609.0000 - val_auc: 0.6016 - val_prc: 0.0576\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5881 - Sensitivity: 0.2981 - tn: 11098.0000 - auc: 0.5279 - prc: 0.0452 - val_loss: 0.5396 - val_Sensitivity: 0.1237 - val_tn: 4732.0000 - val_auc: 0.6274 - val_prc: 0.0682\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5332 - Sensitivity: 0.2368 - tn: 11629.0000 - auc: 0.5019 - prc: 0.0426 - val_loss: 0.5015 - val_Sensitivity: 0.1075 - val_tn: 4778.0000 - val_auc: 0.6408 - val_prc: 0.0744\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4856 - Sensitivity: 0.2095 - tn: 12062.0000 - auc: 0.5052 - prc: 0.0422 - val_loss: 0.4670 - val_Sensitivity: 0.1022 - val_tn: 4798.0000 - val_auc: 0.6544 - val_prc: 0.0806\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4361 - Sensitivity: 0.1959 - tn: 12518.0000 - auc: 0.5011 - prc: 0.0453 - val_loss: 0.4403 - val_Sensitivity: 0.1129 - val_tn: 4795.0000 - val_auc: 0.6664 - val_prc: 0.0860\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4032 - Sensitivity: 0.1755 - tn: 12815.0000 - auc: 0.5118 - prc: 0.0476 - val_loss: 0.4125 - val_Sensitivity: 0.1129 - val_tn: 4806.0000 - val_auc: 0.6752 - val_prc: 0.0896\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3665 - Sensitivity: 0.1380 - tn: 13181.0000 - auc: 0.5071 - prc: 0.0433 - val_loss: 0.3928 - val_Sensitivity: 0.1183 - val_tn: 4806.0000 - val_auc: 0.6826 - val_prc: 0.0928\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3527 - Sensitivity: 0.1380 - tn: 13325.0000 - auc: 0.5038 - prc: 0.0437 - val_loss: 0.3736 - val_Sensitivity: 0.1129 - val_tn: 4805.0000 - val_auc: 0.6883 - val_prc: 0.0949\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3269 - Sensitivity: 0.1261 - tn: 13513.0000 - auc: 0.5269 - prc: 0.0471 - val_loss: 0.3623 - val_Sensitivity: 0.1129 - val_tn: 4799.0000 - val_auc: 0.6921 - val_prc: 0.0952\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3164 - Sensitivity: 0.1482 - tn: 13634.0000 - auc: 0.5279 - prc: 0.0549 - val_loss: 0.3472 - val_Sensitivity: 0.1129 - val_tn: 4803.0000 - val_auc: 0.6965 - val_prc: 0.0963\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3082 - Sensitivity: 0.1005 - tn: 13726.0000 - auc: 0.5120 - prc: 0.0447 - val_loss: 0.3426 - val_Sensitivity: 0.1129 - val_tn: 4788.0000 - val_auc: 0.6995 - val_prc: 0.0969\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2884 - Sensitivity: 0.0835 - tn: 13927.0000 - auc: 0.5097 - prc: 0.0460 - val_loss: 0.3378 - val_Sensitivity: 0.1237 - val_tn: 4779.0000 - val_auc: 0.7040 - val_prc: 0.0979\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2766 - Sensitivity: 0.0698 - tn: 14003.0000 - auc: 0.5087 - prc: 0.0430 - val_loss: 0.3295 - val_Sensitivity: 0.1237 - val_tn: 4767.0000 - val_auc: 0.7064 - val_prc: 0.0980\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2750 - Sensitivity: 0.0681 - tn: 14007.0000 - auc: 0.5131 - prc: 0.0440 - val_loss: 0.3225 - val_Sensitivity: 0.1290 - val_tn: 4765.0000 - val_auc: 0.7086 - val_prc: 0.0979\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2574 - Sensitivity: 0.0835 - tn: 14122.0000 - auc: 0.5531 - prc: 0.0521 - val_loss: 0.3166 - val_Sensitivity: 0.1290 - val_tn: 4756.0000 - val_auc: 0.7094 - val_prc: 0.0976\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2546 - Sensitivity: 0.0784 - tn: 14129.0000 - auc: 0.5442 - prc: 0.0512 - val_loss: 0.3107 - val_Sensitivity: 0.1505 - val_tn: 4751.0000 - val_auc: 0.7097 - val_prc: 0.0974\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2519 - Sensitivity: 0.0647 - tn: 14201.0000 - auc: 0.5301 - prc: 0.0485 - val_loss: 0.3041 - val_Sensitivity: 0.1398 - val_tn: 4749.0000 - val_auc: 0.7110 - val_prc: 0.0974\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2513 - Sensitivity: 0.0494 - tn: 14227.0000 - auc: 0.5367 - prc: 0.0450 - val_loss: 0.2993 - val_Sensitivity: 0.1398 - val_tn: 4750.0000 - val_auc: 0.7117 - val_prc: 0.0975\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2394 - Sensitivity: 0.0579 - tn: 14257.0000 - auc: 0.5567 - prc: 0.0511 - val_loss: 0.2896 - val_Sensitivity: 0.1344 - val_tn: 4754.0000 - val_auc: 0.7124 - val_prc: 0.0979\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2473 - Sensitivity: 0.0511 - tn: 14276.0000 - auc: 0.5198 - prc: 0.0463 - val_loss: 0.2767 - val_Sensitivity: 0.1344 - val_tn: 4762.0000 - val_auc: 0.7110 - val_prc: 0.0985\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2400 - Sensitivity: 0.0528 - tn: 14331.0000 - auc: 0.5344 - prc: 0.0487 - val_loss: 0.2754 - val_Sensitivity: 0.1398 - val_tn: 4752.0000 - val_auc: 0.7130 - val_prc: 0.0989\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2400 - Sensitivity: 0.0494 - tn: 14316.0000 - auc: 0.5328 - prc: 0.0487 - val_loss: 0.2725 - val_Sensitivity: 0.1398 - val_tn: 4755.0000 - val_auc: 0.7133 - val_prc: 0.0992\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2409 - Sensitivity: 0.0409 - tn: 14337.0000 - auc: 0.5227 - prc: 0.0439 - val_loss: 0.2699 - val_Sensitivity: 0.1505 - val_tn: 4755.0000 - val_auc: 0.7148 - val_prc: 0.1000\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2340 - Sensitivity: 0.0596 - tn: 14352.0000 - auc: 0.5487 - prc: 0.0528 - val_loss: 0.2688 - val_Sensitivity: 0.1505 - val_tn: 4751.0000 - val_auc: 0.7151 - val_prc: 0.1000\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2201 - Sensitivity: 0.0426 - tn: 14386.0000 - auc: 0.5690 - prc: 0.0609 - val_loss: 0.2674 - val_Sensitivity: 0.1667 - val_tn: 4748.0000 - val_auc: 0.7161 - val_prc: 0.1002\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2284 - Sensitivity: 0.0545 - tn: 14390.0000 - auc: 0.5494 - prc: 0.0496 - val_loss: 0.2670 - val_Sensitivity: 0.1828 - val_tn: 4741.0000 - val_auc: 0.7171 - val_prc: 0.1004\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2241 - Sensitivity: 0.0409 - tn: 14401.0000 - auc: 0.5594 - prc: 0.0529 - val_loss: 0.2647 - val_Sensitivity: 0.1828 - val_tn: 4737.0000 - val_auc: 0.7180 - val_prc: 0.1005\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2194 - Sensitivity: 0.0545 - tn: 14437.0000 - auc: 0.5666 - prc: 0.0585 - val_loss: 0.2654 - val_Sensitivity: 0.1935 - val_tn: 4730.0000 - val_auc: 0.7182 - val_prc: 0.1005\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2271 - Sensitivity: 0.0477 - tn: 14436.0000 - auc: 0.5388 - prc: 0.0536 - val_loss: 0.2624 - val_Sensitivity: 0.1935 - val_tn: 4730.0000 - val_auc: 0.7193 - val_prc: 0.1005\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2243 - Sensitivity: 0.0443 - tn: 14431.0000 - auc: 0.5543 - prc: 0.0526 - val_loss: 0.2613 - val_Sensitivity: 0.1882 - val_tn: 4728.0000 - val_auc: 0.7202 - val_prc: 0.1005\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2262 - Sensitivity: 0.0392 - tn: 14440.0000 - auc: 0.5620 - prc: 0.0547 - val_loss: 0.2544 - val_Sensitivity: 0.1828 - val_tn: 4734.0000 - val_auc: 0.7185 - val_prc: 0.1005\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2224 - Sensitivity: 0.0375 - tn: 14426.0000 - auc: 0.5653 - prc: 0.0513 - val_loss: 0.2522 - val_Sensitivity: 0.1828 - val_tn: 4734.0000 - val_auc: 0.7185 - val_prc: 0.1005\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2183 - Sensitivity: 0.0426 - tn: 14465.0000 - auc: 0.5821 - prc: 0.0615 - val_loss: 0.2478 - val_Sensitivity: 0.1828 - val_tn: 4741.0000 - val_auc: 0.7175 - val_prc: 0.1007\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2143 - Sensitivity: 0.0511 - tn: 14432.0000 - auc: 0.5798 - prc: 0.0609 - val_loss: 0.2473 - val_Sensitivity: 0.1828 - val_tn: 4739.0000 - val_auc: 0.7177 - val_prc: 0.1009\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2119 - Sensitivity: 0.0443 - tn: 14451.0000 - auc: 0.5812 - prc: 0.0605 - val_loss: 0.2484 - val_Sensitivity: 0.1828 - val_tn: 4731.0000 - val_auc: 0.7187 - val_prc: 0.1009\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2175 - Sensitivity: 0.0375 - tn: 14463.0000 - auc: 0.5791 - prc: 0.0581 - val_loss: 0.2502 - val_Sensitivity: 0.1882 - val_tn: 4725.0000 - val_auc: 0.7202 - val_prc: 0.1008\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2135 - Sensitivity: 0.0392 - tn: 14474.0000 - auc: 0.5836 - prc: 0.0578 - val_loss: 0.2508 - val_Sensitivity: 0.1882 - val_tn: 4723.0000 - val_auc: 0.7218 - val_prc: 0.1010\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2146 - Sensitivity: 0.0392 - tn: 14498.0000 - auc: 0.5661 - prc: 0.0596 - val_loss: 0.2580 - val_Sensitivity: 0.2043 - val_tn: 4708.0000 - val_auc: 0.7216 - val_prc: 0.1007\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2129 - Sensitivity: 0.0409 - tn: 14459.0000 - auc: 0.5928 - prc: 0.0632 - val_loss: 0.2565 - val_Sensitivity: 0.2043 - val_tn: 4707.0000 - val_auc: 0.7224 - val_prc: 0.1006\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2133 - Sensitivity: 0.0494 - tn: 14465.0000 - auc: 0.5829 - prc: 0.0612 - val_loss: 0.2516 - val_Sensitivity: 0.1935 - val_tn: 4713.0000 - val_auc: 0.7207 - val_prc: 0.1003\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2130 - Sensitivity: 0.0596 - tn: 14471.0000 - auc: 0.5788 - prc: 0.0654 - val_loss: 0.2490 - val_Sensitivity: 0.1935 - val_tn: 4713.0000 - val_auc: 0.7201 - val_prc: 0.1004\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2220 - Sensitivity: 0.0426 - tn: 14479.0000 - auc: 0.5513 - prc: 0.0521 - val_loss: 0.2467 - val_Sensitivity: 0.1935 - val_tn: 4714.0000 - val_auc: 0.7204 - val_prc: 0.1005\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2140 - Sensitivity: 0.0324 - tn: 14469.0000 - auc: 0.5894 - prc: 0.0558 - val_loss: 0.2456 - val_Sensitivity: 0.1935 - val_tn: 4715.0000 - val_auc: 0.7203 - val_prc: 0.1007\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2129 - Sensitivity: 0.0358 - tn: 14488.0000 - auc: 0.5852 - prc: 0.0593 - val_loss: 0.2458 - val_Sensitivity: 0.1935 - val_tn: 4713.0000 - val_auc: 0.7197 - val_prc: 0.1004\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2089 - Sensitivity: 0.0630 - tn: 14491.0000 - auc: 0.6011 - prc: 0.0724 - val_loss: 0.2443 - val_Sensitivity: 0.1935 - val_tn: 4715.0000 - val_auc: 0.7175 - val_prc: 0.1003\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2146 - Sensitivity: 0.0460 - tn: 14500.0000 - auc: 0.5751 - prc: 0.0601 - val_loss: 0.2456 - val_Sensitivity: 0.1935 - val_tn: 4713.0000 - val_auc: 0.7179 - val_prc: 0.1004\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2120 - Sensitivity: 0.0511 - tn: 14499.0000 - auc: 0.5793 - prc: 0.0700 - val_loss: 0.2460 - val_Sensitivity: 0.1935 - val_tn: 4712.0000 - val_auc: 0.7194 - val_prc: 0.1009\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2106 - Sensitivity: 0.0511 - tn: 14480.0000 - auc: 0.5885 - prc: 0.0671 - val_loss: 0.2434 - val_Sensitivity: 0.1935 - val_tn: 4716.0000 - val_auc: 0.7203 - val_prc: 0.1011\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2119 - Sensitivity: 0.0443 - tn: 14468.0000 - auc: 0.5934 - prc: 0.0589 - val_loss: 0.2429 - val_Sensitivity: 0.1935 - val_tn: 4717.0000 - val_auc: 0.7200 - val_prc: 0.1010\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2118 - Sensitivity: 0.0392 - tn: 14499.0000 - auc: 0.5901 - prc: 0.0610 - val_loss: 0.2433 - val_Sensitivity: 0.1989 - val_tn: 4716.0000 - val_auc: 0.7200 - val_prc: 0.1009\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2140 - Sensitivity: 0.0477 - tn: 14496.0000 - auc: 0.5687 - prc: 0.0608 - val_loss: 0.2416 - val_Sensitivity: 0.1935 - val_tn: 4718.0000 - val_auc: 0.7201 - val_prc: 0.1010\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2036 - Sensitivity: 0.0494 - tn: 14511.0000 - auc: 0.6073 - prc: 0.0689 - val_loss: 0.2436 - val_Sensitivity: 0.2043 - val_tn: 4710.0000 - val_auc: 0.7206 - val_prc: 0.1013\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2135 - Sensitivity: 0.0443 - tn: 14489.0000 - auc: 0.5944 - prc: 0.0625 - val_loss: 0.2422 - val_Sensitivity: 0.2043 - val_tn: 4712.0000 - val_auc: 0.7208 - val_prc: 0.1012\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2100 - Sensitivity: 0.0511 - tn: 14486.0000 - auc: 0.5977 - prc: 0.0670 - val_loss: 0.2395 - val_Sensitivity: 0.2043 - val_tn: 4715.0000 - val_auc: 0.7203 - val_prc: 0.1013\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2073 - Sensitivity: 0.0443 - tn: 14507.0000 - auc: 0.6031 - prc: 0.0642 - val_loss: 0.2394 - val_Sensitivity: 0.2043 - val_tn: 4716.0000 - val_auc: 0.7209 - val_prc: 0.1013\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2063 - Sensitivity: 0.0392 - tn: 14507.0000 - auc: 0.6073 - prc: 0.0654 - val_loss: 0.2425 - val_Sensitivity: 0.2097 - val_tn: 4707.0000 - val_auc: 0.7215 - val_prc: 0.1012\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2041 - Sensitivity: 0.0477 - tn: 14488.0000 - auc: 0.6084 - prc: 0.0660 - val_loss: 0.2446 - val_Sensitivity: 0.2097 - val_tn: 4702.0000 - val_auc: 0.7221 - val_prc: 0.1012\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2049 - Sensitivity: 0.0426 - tn: 14513.0000 - auc: 0.6053 - prc: 0.0692 - val_loss: 0.2428 - val_Sensitivity: 0.2097 - val_tn: 4705.0000 - val_auc: 0.7223 - val_prc: 0.1014\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2075 - Sensitivity: 0.0528 - tn: 14494.0000 - auc: 0.5987 - prc: 0.0702 - val_loss: 0.2416 - val_Sensitivity: 0.2097 - val_tn: 4707.0000 - val_auc: 0.7214 - val_prc: 0.1014\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2067 - Sensitivity: 0.0443 - tn: 14499.0000 - auc: 0.5983 - prc: 0.0649 - val_loss: 0.2444 - val_Sensitivity: 0.2097 - val_tn: 4701.0000 - val_auc: 0.7216 - val_prc: 0.1013\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2040 - Sensitivity: 0.0375 - tn: 14510.0000 - auc: 0.6151 - prc: 0.0662 - val_loss: 0.2422 - val_Sensitivity: 0.2097 - val_tn: 4705.0000 - val_auc: 0.7212 - val_prc: 0.1013\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2104 - Sensitivity: 0.0409 - tn: 14500.0000 - auc: 0.5962 - prc: 0.0620 - val_loss: 0.2433 - val_Sensitivity: 0.2097 - val_tn: 4702.0000 - val_auc: 0.7207 - val_prc: 0.1010\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2079 - Sensitivity: 0.0494 - tn: 14504.0000 - auc: 0.6048 - prc: 0.0670 - val_loss: 0.2411 - val_Sensitivity: 0.2097 - val_tn: 4707.0000 - val_auc: 0.7205 - val_prc: 0.1011\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2082 - Sensitivity: 0.0375 - tn: 14521.0000 - auc: 0.6025 - prc: 0.0684 - val_loss: 0.2423 - val_Sensitivity: 0.2097 - val_tn: 4700.0000 - val_auc: 0.7208 - val_prc: 0.1013\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2015 - Sensitivity: 0.0375 - tn: 14514.0000 - auc: 0.6287 - prc: 0.0692 - val_loss: 0.2439 - val_Sensitivity: 0.2151 - val_tn: 4696.0000 - val_auc: 0.7209 - val_prc: 0.1014\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2009 - Sensitivity: 0.0443 - tn: 14508.0000 - auc: 0.6193 - prc: 0.0728 - val_loss: 0.2445 - val_Sensitivity: 0.2151 - val_tn: 4696.0000 - val_auc: 0.7207 - val_prc: 0.1012\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2039 - Sensitivity: 0.0494 - tn: 14500.0000 - auc: 0.6183 - prc: 0.0694 - val_loss: 0.2420 - val_Sensitivity: 0.2097 - val_tn: 4700.0000 - val_auc: 0.7206 - val_prc: 0.1013\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2032 - Sensitivity: 0.0392 - tn: 14489.0000 - auc: 0.6225 - prc: 0.0656 - val_loss: 0.2380 - val_Sensitivity: 0.2097 - val_tn: 4711.0000 - val_auc: 0.7204 - val_prc: 0.1016\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2007 - Sensitivity: 0.0409 - tn: 14512.0000 - auc: 0.6199 - prc: 0.0651 - val_loss: 0.2395 - val_Sensitivity: 0.2097 - val_tn: 4706.0000 - val_auc: 0.7208 - val_prc: 0.1018\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2035 - Sensitivity: 0.0494 - tn: 14504.0000 - auc: 0.6146 - prc: 0.0734 - val_loss: 0.2380 - val_Sensitivity: 0.2097 - val_tn: 4708.0000 - val_auc: 0.7205 - val_prc: 0.1015\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2064 - Sensitivity: 0.0460 - tn: 14509.0000 - auc: 0.6037 - prc: 0.0705 - val_loss: 0.2378 - val_Sensitivity: 0.2097 - val_tn: 4709.0000 - val_auc: 0.7205 - val_prc: 0.1015\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2011 - Sensitivity: 0.0409 - tn: 14505.0000 - auc: 0.6230 - prc: 0.0701 - val_loss: 0.2363 - val_Sensitivity: 0.2151 - val_tn: 4709.0000 - val_auc: 0.7191 - val_prc: 0.1017\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2078 - Sensitivity: 0.0460 - tn: 14505.0000 - auc: 0.6058 - prc: 0.0620 - val_loss: 0.2353 - val_Sensitivity: 0.2151 - val_tn: 4709.0000 - val_auc: 0.7187 - val_prc: 0.1016\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1960 - Sensitivity: 0.0511 - tn: 14505.0000 - auc: 0.6353 - prc: 0.0760 - val_loss: 0.2369 - val_Sensitivity: 0.2204 - val_tn: 4705.0000 - val_auc: 0.7189 - val_prc: 0.1017\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1984 - Sensitivity: 0.0545 - tn: 14506.0000 - auc: 0.6377 - prc: 0.0796 - val_loss: 0.2361 - val_Sensitivity: 0.2151 - val_tn: 4708.0000 - val_auc: 0.7196 - val_prc: 0.1017\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1999 - Sensitivity: 0.0341 - tn: 14531.0000 - auc: 0.6137 - prc: 0.0686 - val_loss: 0.2378 - val_Sensitivity: 0.2151 - val_tn: 4707.0000 - val_auc: 0.7199 - val_prc: 0.1018\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2046 - Sensitivity: 0.0392 - tn: 14496.0000 - auc: 0.6184 - prc: 0.0678 - val_loss: 0.2400 - val_Sensitivity: 0.2204 - val_tn: 4701.0000 - val_auc: 0.7200 - val_prc: 0.1017\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2056 - Sensitivity: 0.0460 - tn: 14496.0000 - auc: 0.6069 - prc: 0.0679 - val_loss: 0.2375 - val_Sensitivity: 0.2204 - val_tn: 4705.0000 - val_auc: 0.7200 - val_prc: 0.1019\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2003 - Sensitivity: 0.0375 - tn: 14522.0000 - auc: 0.6191 - prc: 0.0658 - val_loss: 0.2377 - val_Sensitivity: 0.2204 - val_tn: 4702.0000 - val_auc: 0.7195 - val_prc: 0.1017\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1967 - Sensitivity: 0.0545 - tn: 14521.0000 - auc: 0.6339 - prc: 0.0748 - val_loss: 0.2390 - val_Sensitivity: 0.2204 - val_tn: 4702.0000 - val_auc: 0.7200 - val_prc: 0.1016\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1954 - Sensitivity: 0.0443 - tn: 14514.0000 - auc: 0.6368 - prc: 0.0796 - val_loss: 0.2399 - val_Sensitivity: 0.2204 - val_tn: 4701.0000 - val_auc: 0.7198 - val_prc: 0.1015\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1925 - Sensitivity: 0.0460 - tn: 14525.0000 - auc: 0.6526 - prc: 0.0785 - val_loss: 0.2397 - val_Sensitivity: 0.2204 - val_tn: 4699.0000 - val_auc: 0.7203 - val_prc: 0.1013\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1993 - Sensitivity: 0.0409 - tn: 14510.0000 - auc: 0.6268 - prc: 0.0702 - val_loss: 0.2420 - val_Sensitivity: 0.2204 - val_tn: 4694.0000 - val_auc: 0.7198 - val_prc: 0.1011\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1985 - Sensitivity: 0.0477 - tn: 14531.0000 - auc: 0.6255 - prc: 0.0804 - val_loss: 0.2442 - val_Sensitivity: 0.2204 - val_tn: 4688.0000 - val_auc: 0.7196 - val_prc: 0.1007\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1969 - Sensitivity: 0.0545 - tn: 14506.0000 - auc: 0.6341 - prc: 0.0801 - val_loss: 0.2411 - val_Sensitivity: 0.2204 - val_tn: 4694.0000 - val_auc: 0.7187 - val_prc: 0.1009\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1931 - Sensitivity: 0.0477 - tn: 14540.0000 - auc: 0.6366 - prc: 0.0855 - val_loss: 0.2425 - val_Sensitivity: 0.2204 - val_tn: 4693.0000 - val_auc: 0.7195 - val_prc: 0.1006\n",
      "Epoch 91/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1927 - Sensitivity: 0.0358 - tn: 14529.0000 - auc: 0.6390 - prc: 0.0781 - val_loss: 0.2442 - val_Sensitivity: 0.2204 - val_tn: 4689.0000 - val_auc: 0.7196 - val_prc: 0.1007\n",
      "Epoch 92/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1938 - Sensitivity: 0.0443 - tn: 14532.0000 - auc: 0.6354 - prc: 0.0776 - val_loss: 0.2447 - val_Sensitivity: 0.2204 - val_tn: 4686.0000 - val_auc: 0.7196 - val_prc: 0.1007\n",
      "Epoch 93/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1997 - Sensitivity: 0.0392 - tn: 14511.0000 - auc: 0.6276 - prc: 0.0735 - val_loss: 0.2451 - val_Sensitivity: 0.2204 - val_tn: 4682.0000 - val_auc: 0.7196 - val_prc: 0.1005\n",
      "Epoch 94/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1982 - Sensitivity: 0.0358 - tn: 14495.0000 - auc: 0.6280 - prc: 0.0724 - val_loss: 0.2419 - val_Sensitivity: 0.2151 - val_tn: 4690.0000 - val_auc: 0.7204 - val_prc: 0.1009\n",
      "Epoch 95/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1970 - Sensitivity: 0.0375 - tn: 14513.0000 - auc: 0.6415 - prc: 0.0733 - val_loss: 0.2385 - val_Sensitivity: 0.2151 - val_tn: 4696.0000 - val_auc: 0.7188 - val_prc: 0.1005\n",
      "Epoch 96/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1949 - Sensitivity: 0.0613 - tn: 14531.0000 - auc: 0.6331 - prc: 0.0893 - val_loss: 0.2379 - val_Sensitivity: 0.2151 - val_tn: 4698.0000 - val_auc: 0.7189 - val_prc: 0.1008\n",
      "Epoch 97/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1955 - Sensitivity: 0.0307 - tn: 14529.0000 - auc: 0.6331 - prc: 0.0703 - val_loss: 0.2416 - val_Sensitivity: 0.2151 - val_tn: 4690.0000 - val_auc: 0.7190 - val_prc: 0.1007\n",
      "Epoch 98/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1948 - Sensitivity: 0.0392 - tn: 14525.0000 - auc: 0.6391 - prc: 0.0739 - val_loss: 0.2379 - val_Sensitivity: 0.2151 - val_tn: 4696.0000 - val_auc: 0.7188 - val_prc: 0.1008\n",
      "Epoch 99/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1950 - Sensitivity: 0.0392 - tn: 14521.0000 - auc: 0.6299 - prc: 0.0702 - val_loss: 0.2387 - val_Sensitivity: 0.2151 - val_tn: 4696.0000 - val_auc: 0.7195 - val_prc: 0.1009\n",
      "Epoch 100/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1941 - Sensitivity: 0.0477 - tn: 14520.0000 - auc: 0.6417 - prc: 0.0821 - val_loss: 0.2354 - val_Sensitivity: 0.2097 - val_tn: 4701.0000 - val_auc: 0.7184 - val_prc: 0.1009\n",
      "Epoch 101/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1930 - Sensitivity: 0.0375 - tn: 14529.0000 - auc: 0.6458 - prc: 0.0756 - val_loss: 0.2348 - val_Sensitivity: 0.2097 - val_tn: 4702.0000 - val_auc: 0.7192 - val_prc: 0.1011\n",
      "Epoch 102/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1937 - Sensitivity: 0.0460 - tn: 14509.0000 - auc: 0.6538 - prc: 0.0782 - val_loss: 0.2348 - val_Sensitivity: 0.2097 - val_tn: 4702.0000 - val_auc: 0.7186 - val_prc: 0.1011\n",
      "Epoch 103/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1976 - Sensitivity: 0.0375 - tn: 14514.0000 - auc: 0.6282 - prc: 0.0725 - val_loss: 0.2365 - val_Sensitivity: 0.2097 - val_tn: 4701.0000 - val_auc: 0.7186 - val_prc: 0.1010\n",
      "Epoch 104/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1947 - Sensitivity: 0.0443 - tn: 14528.0000 - auc: 0.6433 - prc: 0.0788 - val_loss: 0.2367 - val_Sensitivity: 0.2151 - val_tn: 4701.0000 - val_auc: 0.7185 - val_prc: 0.1009\n",
      "Epoch 105/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1992 - Sensitivity: 0.0341 - tn: 14532.0000 - auc: 0.6179 - prc: 0.0669 - val_loss: 0.2408 - val_Sensitivity: 0.2151 - val_tn: 4692.0000 - val_auc: 0.7191 - val_prc: 0.1006\n",
      "Epoch 106/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1928 - Sensitivity: 0.0511 - tn: 14532.0000 - auc: 0.6452 - prc: 0.0823 - val_loss: 0.2411 - val_Sensitivity: 0.2204 - val_tn: 4691.0000 - val_auc: 0.7189 - val_prc: 0.1004\n",
      "Epoch 107/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1880 - Sensitivity: 0.0477 - tn: 14516.0000 - auc: 0.6499 - prc: 0.0845 - val_loss: 0.2434 - val_Sensitivity: 0.2204 - val_tn: 4687.0000 - val_auc: 0.7195 - val_prc: 0.1001\n",
      "Epoch 108/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1964 - Sensitivity: 0.0375 - tn: 14521.0000 - auc: 0.6418 - prc: 0.0709 - val_loss: 0.2442 - val_Sensitivity: 0.2204 - val_tn: 4685.0000 - val_auc: 0.7202 - val_prc: 0.1001\n",
      "Epoch 109/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1910 - Sensitivity: 0.0409 - tn: 14543.0000 - auc: 0.6461 - prc: 0.0794 - val_loss: 0.2419 - val_Sensitivity: 0.2204 - val_tn: 4691.0000 - val_auc: 0.7200 - val_prc: 0.1001\n",
      "Epoch 110/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1946 - Sensitivity: 0.0324 - tn: 14542.0000 - auc: 0.6443 - prc: 0.0747 - val_loss: 0.2434 - val_Sensitivity: 0.2258 - val_tn: 4688.0000 - val_auc: 0.7207 - val_prc: 0.1001\n",
      "Epoch 111/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1891 - Sensitivity: 0.0426 - tn: 14542.0000 - auc: 0.6518 - prc: 0.0809 - val_loss: 0.2437 - val_Sensitivity: 0.2258 - val_tn: 4689.0000 - val_auc: 0.7208 - val_prc: 0.0999\n",
      "Epoch 112/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1899 - Sensitivity: 0.0409 - tn: 14517.0000 - auc: 0.6516 - prc: 0.0798 - val_loss: 0.2404 - val_Sensitivity: 0.2258 - val_tn: 4697.0000 - val_auc: 0.7203 - val_prc: 0.1002\n",
      "Epoch 113/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1888 - Sensitivity: 0.0375 - tn: 14539.0000 - auc: 0.6570 - prc: 0.0781 - val_loss: 0.2386 - val_Sensitivity: 0.2151 - val_tn: 4700.0000 - val_auc: 0.7198 - val_prc: 0.1004\n",
      "Epoch 114/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1857 - Sensitivity: 0.0375 - tn: 14552.0000 - auc: 0.6552 - prc: 0.0830 - val_loss: 0.2391 - val_Sensitivity: 0.2204 - val_tn: 4699.0000 - val_auc: 0.7193 - val_prc: 0.1003\n",
      "Epoch 115/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1926 - Sensitivity: 0.0460 - tn: 14527.0000 - auc: 0.6349 - prc: 0.0759 - val_loss: 0.2391 - val_Sensitivity: 0.2204 - val_tn: 4699.0000 - val_auc: 0.7187 - val_prc: 0.1001\n",
      "Epoch 116/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1938 - Sensitivity: 0.0392 - tn: 14525.0000 - auc: 0.6412 - prc: 0.0697 - val_loss: 0.2377 - val_Sensitivity: 0.2151 - val_tn: 4701.0000 - val_auc: 0.7187 - val_prc: 0.1004\n",
      "Epoch 117/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1895 - Sensitivity: 0.0375 - tn: 14528.0000 - auc: 0.6571 - prc: 0.0818 - val_loss: 0.2392 - val_Sensitivity: 0.2204 - val_tn: 4699.0000 - val_auc: 0.7193 - val_prc: 0.1005\n",
      "Epoch 118/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1855 - Sensitivity: 0.0460 - tn: 14541.0000 - auc: 0.6626 - prc: 0.0827 - val_loss: 0.2388 - val_Sensitivity: 0.2151 - val_tn: 4699.0000 - val_auc: 0.7186 - val_prc: 0.1002\n",
      "Epoch 119/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1895 - Sensitivity: 0.0409 - tn: 14555.0000 - auc: 0.6422 - prc: 0.0780 - val_loss: 0.2408 - val_Sensitivity: 0.2258 - val_tn: 4697.0000 - val_auc: 0.7188 - val_prc: 0.1001\n",
      "Epoch 120/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1876 - Sensitivity: 0.0358 - tn: 14545.0000 - auc: 0.6575 - prc: 0.0798 - val_loss: 0.2404 - val_Sensitivity: 0.2258 - val_tn: 4697.0000 - val_auc: 0.7185 - val_prc: 0.1003\n",
      "Epoch 121/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1857 - Sensitivity: 0.0358 - tn: 14545.0000 - auc: 0.6695 - prc: 0.0817 - val_loss: 0.2409 - val_Sensitivity: 0.2258 - val_tn: 4694.0000 - val_auc: 0.7182 - val_prc: 0.1001\n",
      "Epoch 122/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1854 - Sensitivity: 0.0443 - tn: 14549.0000 - auc: 0.6545 - prc: 0.0847 - val_loss: 0.2406 - val_Sensitivity: 0.2258 - val_tn: 4693.0000 - val_auc: 0.7183 - val_prc: 0.1000\n",
      "Epoch 123/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1938 - Sensitivity: 0.0511 - tn: 14538.0000 - auc: 0.6427 - prc: 0.0734 - val_loss: 0.2392 - val_Sensitivity: 0.2258 - val_tn: 4697.0000 - val_auc: 0.7184 - val_prc: 0.1002\n",
      "Epoch 124/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1860 - Sensitivity: 0.0341 - tn: 14556.0000 - auc: 0.6646 - prc: 0.0837 - val_loss: 0.2406 - val_Sensitivity: 0.2258 - val_tn: 4692.0000 - val_auc: 0.7189 - val_prc: 0.1000\n",
      "Epoch 125/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1874 - Sensitivity: 0.0307 - tn: 14537.0000 - auc: 0.6512 - prc: 0.0753 - val_loss: 0.2415 - val_Sensitivity: 0.2312 - val_tn: 4691.0000 - val_auc: 0.7186 - val_prc: 0.0999\n",
      "Epoch 126/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1860 - Sensitivity: 0.0273 - tn: 14551.0000 - auc: 0.6523 - prc: 0.0780 - val_loss: 0.2403 - val_Sensitivity: 0.2312 - val_tn: 4692.0000 - val_auc: 0.7186 - val_prc: 0.0998\n",
      "Epoch 127/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1888 - Sensitivity: 0.0409 - tn: 14540.0000 - auc: 0.6573 - prc: 0.0788 - val_loss: 0.2375 - val_Sensitivity: 0.2204 - val_tn: 4701.0000 - val_auc: 0.7172 - val_prc: 0.0996\n",
      "Epoch 1/500\n",
      "30/30 [==============================] - 2s 28ms/step - loss: 1.2140 - Sensitivity: 0.5490 - tn: 6394.0000 - auc: 0.4934 - prc: 0.0394 - val_loss: 0.8462 - val_Sensitivity: 0.9621 - val_tn: 273.0000 - val_auc: 0.4888 - val_prc: 0.0422\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0308 - Sensitivity: 0.4831 - tn: 7579.0000 - auc: 0.4938 - prc: 0.0389 - val_loss: 0.7838 - val_Sensitivity: 0.9384 - val_tn: 651.0000 - val_auc: 0.5552 - val_prc: 0.0488\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.8954 - Sensitivity: 0.3850 - tn: 8632.0000 - auc: 0.4671 - prc: 0.0381 - val_loss: 0.7078 - val_Sensitivity: 0.7251 - val_tn: 1986.0000 - val_auc: 0.5997 - val_prc: 0.0609\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.7611 - Sensitivity: 0.3922 - tn: 9600.0000 - auc: 0.5153 - prc: 0.0448 - val_loss: 0.6450 - val_Sensitivity: 0.4455 - val_tn: 3689.0000 - val_auc: 0.6405 - val_prc: 0.0769\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6596 - Sensitivity: 0.3601 - tn: 10439.0000 - auc: 0.5273 - prc: 0.0456 - val_loss: 0.5958 - val_Sensitivity: 0.2559 - val_tn: 4464.0000 - val_auc: 0.6611 - val_prc: 0.0888\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5923 - Sensitivity: 0.2834 - tn: 11031.0000 - auc: 0.5095 - prc: 0.0419 - val_loss: 0.5450 - val_Sensitivity: 0.1706 - val_tn: 4678.0000 - val_auc: 0.6723 - val_prc: 0.0969\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5276 - Sensitivity: 0.2602 - tn: 11657.0000 - auc: 0.5059 - prc: 0.0438 - val_loss: 0.4999 - val_Sensitivity: 0.1280 - val_tn: 4743.0000 - val_auc: 0.6763 - val_prc: 0.1013\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4801 - Sensitivity: 0.1854 - tn: 12181.0000 - auc: 0.4973 - prc: 0.0394 - val_loss: 0.4652 - val_Sensitivity: 0.0995 - val_tn: 4764.0000 - val_auc: 0.6801 - val_prc: 0.1041\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4255 - Sensitivity: 0.2157 - tn: 12520.0000 - auc: 0.5215 - prc: 0.0453 - val_loss: 0.4383 - val_Sensitivity: 0.1043 - val_tn: 4766.0000 - val_auc: 0.6855 - val_prc: 0.1070\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3937 - Sensitivity: 0.1836 - tn: 12898.0000 - auc: 0.5283 - prc: 0.0463 - val_loss: 0.4146 - val_Sensitivity: 0.1043 - val_tn: 4766.0000 - val_auc: 0.6889 - val_prc: 0.1096\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3678 - Sensitivity: 0.1337 - tn: 13182.0000 - auc: 0.4979 - prc: 0.0427 - val_loss: 0.3999 - val_Sensitivity: 0.1090 - val_tn: 4760.0000 - val_auc: 0.6915 - val_prc: 0.1109\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3456 - Sensitivity: 0.1444 - tn: 13347.0000 - auc: 0.5169 - prc: 0.0449 - val_loss: 0.3836 - val_Sensitivity: 0.1185 - val_tn: 4753.0000 - val_auc: 0.6924 - val_prc: 0.1115\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3227 - Sensitivity: 0.1301 - tn: 13572.0000 - auc: 0.5228 - prc: 0.0467 - val_loss: 0.3627 - val_Sensitivity: 0.1137 - val_tn: 4764.0000 - val_auc: 0.6913 - val_prc: 0.1120\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3140 - Sensitivity: 0.0980 - tn: 13677.0000 - auc: 0.5058 - prc: 0.0404 - val_loss: 0.3482 - val_Sensitivity: 0.1137 - val_tn: 4768.0000 - val_auc: 0.6924 - val_prc: 0.1128\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3021 - Sensitivity: 0.0838 - tn: 13800.0000 - auc: 0.5045 - prc: 0.0400 - val_loss: 0.3408 - val_Sensitivity: 0.1137 - val_tn: 4761.0000 - val_auc: 0.6944 - val_prc: 0.1128\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2836 - Sensitivity: 0.0838 - tn: 13896.0000 - auc: 0.5315 - prc: 0.0459 - val_loss: 0.3334 - val_Sensitivity: 0.1137 - val_tn: 4754.0000 - val_auc: 0.6962 - val_prc: 0.1133\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2694 - Sensitivity: 0.0909 - tn: 13999.0000 - auc: 0.5401 - prc: 0.0472 - val_loss: 0.3294 - val_Sensitivity: 0.1280 - val_tn: 4739.0000 - val_auc: 0.6997 - val_prc: 0.1139\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2562 - Sensitivity: 0.0820 - tn: 14088.0000 - auc: 0.5463 - prc: 0.0484 - val_loss: 0.3230 - val_Sensitivity: 0.1327 - val_tn: 4732.0000 - val_auc: 0.7024 - val_prc: 0.1144\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2637 - Sensitivity: 0.0713 - tn: 14110.0000 - auc: 0.5183 - prc: 0.0442 - val_loss: 0.3125 - val_Sensitivity: 0.1327 - val_tn: 4739.0000 - val_auc: 0.7019 - val_prc: 0.1149\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2571 - Sensitivity: 0.0463 - tn: 14204.0000 - auc: 0.5163 - prc: 0.0412 - val_loss: 0.3061 - val_Sensitivity: 0.1327 - val_tn: 4734.0000 - val_auc: 0.7033 - val_prc: 0.1154\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2526 - Sensitivity: 0.0410 - tn: 14202.0000 - auc: 0.5132 - prc: 0.0395 - val_loss: 0.3040 - val_Sensitivity: 0.1327 - val_tn: 4724.0000 - val_auc: 0.7063 - val_prc: 0.1156\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2348 - Sensitivity: 0.0481 - tn: 14272.0000 - auc: 0.5454 - prc: 0.0481 - val_loss: 0.3022 - val_Sensitivity: 0.1469 - val_tn: 4721.0000 - val_auc: 0.7081 - val_prc: 0.1156\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2445 - Sensitivity: 0.0570 - tn: 14303.0000 - auc: 0.5176 - prc: 0.0448 - val_loss: 0.2946 - val_Sensitivity: 0.1422 - val_tn: 4723.0000 - val_auc: 0.7091 - val_prc: 0.1158\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2349 - Sensitivity: 0.0463 - tn: 14308.0000 - auc: 0.5225 - prc: 0.0449 - val_loss: 0.2877 - val_Sensitivity: 0.1374 - val_tn: 4725.0000 - val_auc: 0.7087 - val_prc: 0.1160\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2372 - Sensitivity: 0.0570 - tn: 14329.0000 - auc: 0.5331 - prc: 0.0467 - val_loss: 0.2870 - val_Sensitivity: 0.1469 - val_tn: 4719.0000 - val_auc: 0.7108 - val_prc: 0.1166\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2314 - Sensitivity: 0.0481 - tn: 14359.0000 - auc: 0.5280 - prc: 0.0463 - val_loss: 0.2848 - val_Sensitivity: 0.1517 - val_tn: 4714.0000 - val_auc: 0.7125 - val_prc: 0.1165\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2217 - Sensitivity: 0.0624 - tn: 14383.0000 - auc: 0.5522 - prc: 0.0554 - val_loss: 0.2828 - val_Sensitivity: 0.1564 - val_tn: 4712.0000 - val_auc: 0.7126 - val_prc: 0.1166\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2323 - Sensitivity: 0.0463 - tn: 14367.0000 - auc: 0.5322 - prc: 0.0453 - val_loss: 0.2739 - val_Sensitivity: 0.1422 - val_tn: 4723.0000 - val_auc: 0.7119 - val_prc: 0.1168\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2204 - Sensitivity: 0.0642 - tn: 14429.0000 - auc: 0.5578 - prc: 0.0554 - val_loss: 0.2716 - val_Sensitivity: 0.1422 - val_tn: 4720.0000 - val_auc: 0.7139 - val_prc: 0.1175\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2191 - Sensitivity: 0.0624 - tn: 14413.0000 - auc: 0.5613 - prc: 0.0565 - val_loss: 0.2707 - val_Sensitivity: 0.1517 - val_tn: 4714.0000 - val_auc: 0.7138 - val_prc: 0.1177\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2215 - Sensitivity: 0.0410 - tn: 14438.0000 - auc: 0.5468 - prc: 0.0478 - val_loss: 0.2731 - val_Sensitivity: 0.1564 - val_tn: 4707.0000 - val_auc: 0.7155 - val_prc: 0.1177\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2223 - Sensitivity: 0.0357 - tn: 14414.0000 - auc: 0.5390 - prc: 0.0451 - val_loss: 0.2673 - val_Sensitivity: 0.1517 - val_tn: 4713.0000 - val_auc: 0.7152 - val_prc: 0.1182\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2155 - Sensitivity: 0.0606 - tn: 14429.0000 - auc: 0.5668 - prc: 0.0578 - val_loss: 0.2643 - val_Sensitivity: 0.1564 - val_tn: 4712.0000 - val_auc: 0.7152 - val_prc: 0.1186\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2189 - Sensitivity: 0.0428 - tn: 14446.0000 - auc: 0.5503 - prc: 0.0539 - val_loss: 0.2616 - val_Sensitivity: 0.1564 - val_tn: 4714.0000 - val_auc: 0.7148 - val_prc: 0.1187\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2141 - Sensitivity: 0.0535 - tn: 14478.0000 - auc: 0.5571 - prc: 0.0552 - val_loss: 0.2616 - val_Sensitivity: 0.1564 - val_tn: 4706.0000 - val_auc: 0.7167 - val_prc: 0.1191\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2152 - Sensitivity: 0.0428 - tn: 14472.0000 - auc: 0.5619 - prc: 0.0530 - val_loss: 0.2597 - val_Sensitivity: 0.1564 - val_tn: 4706.0000 - val_auc: 0.7168 - val_prc: 0.1193\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2107 - Sensitivity: 0.0357 - tn: 14461.0000 - auc: 0.5884 - prc: 0.0533 - val_loss: 0.2574 - val_Sensitivity: 0.1564 - val_tn: 4706.0000 - val_auc: 0.7181 - val_prc: 0.1196\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2143 - Sensitivity: 0.0410 - tn: 14473.0000 - auc: 0.5549 - prc: 0.0535 - val_loss: 0.2582 - val_Sensitivity: 0.1564 - val_tn: 4705.0000 - val_auc: 0.7187 - val_prc: 0.1196\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2129 - Sensitivity: 0.0517 - tn: 14481.0000 - auc: 0.5673 - prc: 0.0535 - val_loss: 0.2560 - val_Sensitivity: 0.1564 - val_tn: 4706.0000 - val_auc: 0.7184 - val_prc: 0.1196\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2110 - Sensitivity: 0.0428 - tn: 14492.0000 - auc: 0.5679 - prc: 0.0528 - val_loss: 0.2561 - val_Sensitivity: 0.1564 - val_tn: 4701.0000 - val_auc: 0.7196 - val_prc: 0.1196\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2151 - Sensitivity: 0.0410 - tn: 14472.0000 - auc: 0.5740 - prc: 0.0539 - val_loss: 0.2558 - val_Sensitivity: 0.1611 - val_tn: 4701.0000 - val_auc: 0.7195 - val_prc: 0.1199\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2076 - Sensitivity: 0.0357 - tn: 14493.0000 - auc: 0.5855 - prc: 0.0575 - val_loss: 0.2561 - val_Sensitivity: 0.1611 - val_tn: 4699.0000 - val_auc: 0.7198 - val_prc: 0.1202\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2029 - Sensitivity: 0.0535 - tn: 14488.0000 - auc: 0.5969 - prc: 0.0617 - val_loss: 0.2572 - val_Sensitivity: 0.1706 - val_tn: 4693.0000 - val_auc: 0.7207 - val_prc: 0.1203\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2089 - Sensitivity: 0.0321 - tn: 14496.0000 - auc: 0.5739 - prc: 0.0518 - val_loss: 0.2565 - val_Sensitivity: 0.1706 - val_tn: 4692.0000 - val_auc: 0.7209 - val_prc: 0.1204\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2101 - Sensitivity: 0.0428 - tn: 14518.0000 - auc: 0.5635 - prc: 0.0565 - val_loss: 0.2559 - val_Sensitivity: 0.1659 - val_tn: 4692.0000 - val_auc: 0.7226 - val_prc: 0.1206\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2035 - Sensitivity: 0.0553 - tn: 14511.0000 - auc: 0.5907 - prc: 0.0644 - val_loss: 0.2521 - val_Sensitivity: 0.1706 - val_tn: 4694.0000 - val_auc: 0.7219 - val_prc: 0.1210\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2055 - Sensitivity: 0.0624 - tn: 14477.0000 - auc: 0.6010 - prc: 0.0621 - val_loss: 0.2499 - val_Sensitivity: 0.1611 - val_tn: 4697.0000 - val_auc: 0.7220 - val_prc: 0.1211\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2076 - Sensitivity: 0.0553 - tn: 14503.0000 - auc: 0.5849 - prc: 0.0663 - val_loss: 0.2454 - val_Sensitivity: 0.1611 - val_tn: 4702.0000 - val_auc: 0.7213 - val_prc: 0.1213\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2022 - Sensitivity: 0.0446 - tn: 14527.0000 - auc: 0.5898 - prc: 0.0610 - val_loss: 0.2482 - val_Sensitivity: 0.1659 - val_tn: 4695.0000 - val_auc: 0.7220 - val_prc: 0.1213\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2098 - Sensitivity: 0.0339 - tn: 14517.0000 - auc: 0.5765 - prc: 0.0535 - val_loss: 0.2496 - val_Sensitivity: 0.1754 - val_tn: 4692.0000 - val_auc: 0.7216 - val_prc: 0.1212\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2028 - Sensitivity: 0.0374 - tn: 14499.0000 - auc: 0.5909 - prc: 0.0575 - val_loss: 0.2493 - val_Sensitivity: 0.1754 - val_tn: 4688.0000 - val_auc: 0.7218 - val_prc: 0.1212\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2028 - Sensitivity: 0.0410 - tn: 14498.0000 - auc: 0.5964 - prc: 0.0584 - val_loss: 0.2470 - val_Sensitivity: 0.1754 - val_tn: 4692.0000 - val_auc: 0.7208 - val_prc: 0.1214\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2016 - Sensitivity: 0.0303 - tn: 14521.0000 - auc: 0.6045 - prc: 0.0576 - val_loss: 0.2483 - val_Sensitivity: 0.1754 - val_tn: 4688.0000 - val_auc: 0.7227 - val_prc: 0.1215\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2026 - Sensitivity: 0.0392 - tn: 14529.0000 - auc: 0.6020 - prc: 0.0605 - val_loss: 0.2510 - val_Sensitivity: 0.1754 - val_tn: 4685.0000 - val_auc: 0.7252 - val_prc: 0.1218\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2086 - Sensitivity: 0.0446 - tn: 14500.0000 - auc: 0.5923 - prc: 0.0608 - val_loss: 0.2494 - val_Sensitivity: 0.1754 - val_tn: 4686.0000 - val_auc: 0.7245 - val_prc: 0.1217\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1960 - Sensitivity: 0.0553 - tn: 14527.0000 - auc: 0.6208 - prc: 0.0706 - val_loss: 0.2482 - val_Sensitivity: 0.1706 - val_tn: 4689.0000 - val_auc: 0.7246 - val_prc: 0.1216\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2066 - Sensitivity: 0.0267 - tn: 14534.0000 - auc: 0.5846 - prc: 0.0558 - val_loss: 0.2504 - val_Sensitivity: 0.1754 - val_tn: 4684.0000 - val_auc: 0.7246 - val_prc: 0.1211\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1960 - Sensitivity: 0.0535 - tn: 14506.0000 - auc: 0.6271 - prc: 0.0669 - val_loss: 0.2520 - val_Sensitivity: 0.1754 - val_tn: 4683.0000 - val_auc: 0.7256 - val_prc: 0.1210\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1989 - Sensitivity: 0.0481 - tn: 14523.0000 - auc: 0.6095 - prc: 0.0656 - val_loss: 0.2499 - val_Sensitivity: 0.1754 - val_tn: 4684.0000 - val_auc: 0.7256 - val_prc: 0.1213\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2029 - Sensitivity: 0.0463 - tn: 14512.0000 - auc: 0.5969 - prc: 0.0633 - val_loss: 0.2505 - val_Sensitivity: 0.1754 - val_tn: 4684.0000 - val_auc: 0.7274 - val_prc: 0.1217\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2015 - Sensitivity: 0.0446 - tn: 14497.0000 - auc: 0.5936 - prc: 0.0640 - val_loss: 0.2476 - val_Sensitivity: 0.1754 - val_tn: 4686.0000 - val_auc: 0.7262 - val_prc: 0.1215\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1989 - Sensitivity: 0.0553 - tn: 14524.0000 - auc: 0.6091 - prc: 0.0676 - val_loss: 0.2474 - val_Sensitivity: 0.1754 - val_tn: 4686.0000 - val_auc: 0.7264 - val_prc: 0.1218\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1991 - Sensitivity: 0.0410 - tn: 14534.0000 - auc: 0.6001 - prc: 0.0658 - val_loss: 0.2477 - val_Sensitivity: 0.1754 - val_tn: 4684.0000 - val_auc: 0.7268 - val_prc: 0.1218\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1990 - Sensitivity: 0.0410 - tn: 14531.0000 - auc: 0.6060 - prc: 0.0632 - val_loss: 0.2458 - val_Sensitivity: 0.1754 - val_tn: 4685.0000 - val_auc: 0.7259 - val_prc: 0.1220\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1964 - Sensitivity: 0.0517 - tn: 14542.0000 - auc: 0.6321 - prc: 0.0721 - val_loss: 0.2417 - val_Sensitivity: 0.1706 - val_tn: 4687.0000 - val_auc: 0.7253 - val_prc: 0.1221\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1984 - Sensitivity: 0.0392 - tn: 14537.0000 - auc: 0.6162 - prc: 0.0637 - val_loss: 0.2449 - val_Sensitivity: 0.1801 - val_tn: 4685.0000 - val_auc: 0.7258 - val_prc: 0.1220\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1945 - Sensitivity: 0.0499 - tn: 14540.0000 - auc: 0.6166 - prc: 0.0692 - val_loss: 0.2467 - val_Sensitivity: 0.1801 - val_tn: 4683.0000 - val_auc: 0.7264 - val_prc: 0.1222\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2021 - Sensitivity: 0.0392 - tn: 14522.0000 - auc: 0.6058 - prc: 0.0620 - val_loss: 0.2455 - val_Sensitivity: 0.1754 - val_tn: 4685.0000 - val_auc: 0.7255 - val_prc: 0.1222\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2037 - Sensitivity: 0.0285 - tn: 14534.0000 - auc: 0.5925 - prc: 0.0563 - val_loss: 0.2446 - val_Sensitivity: 0.1754 - val_tn: 4686.0000 - val_auc: 0.7251 - val_prc: 0.1219\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1938 - Sensitivity: 0.0481 - tn: 14534.0000 - auc: 0.6313 - prc: 0.0755 - val_loss: 0.2485 - val_Sensitivity: 0.1848 - val_tn: 4679.0000 - val_auc: 0.7264 - val_prc: 0.1217\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1930 - Sensitivity: 0.0481 - tn: 14539.0000 - auc: 0.6263 - prc: 0.0701 - val_loss: 0.2486 - val_Sensitivity: 0.1754 - val_tn: 4682.0000 - val_auc: 0.7269 - val_prc: 0.1216\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1904 - Sensitivity: 0.0553 - tn: 14550.0000 - auc: 0.6245 - prc: 0.0812 - val_loss: 0.2458 - val_Sensitivity: 0.1706 - val_tn: 4683.0000 - val_auc: 0.7264 - val_prc: 0.1221\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1940 - Sensitivity: 0.0517 - tn: 14537.0000 - auc: 0.6260 - prc: 0.0701 - val_loss: 0.2481 - val_Sensitivity: 0.1801 - val_tn: 4681.0000 - val_auc: 0.7273 - val_prc: 0.1220\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1934 - Sensitivity: 0.0499 - tn: 14549.0000 - auc: 0.6196 - prc: 0.0721 - val_loss: 0.2477 - val_Sensitivity: 0.1754 - val_tn: 4682.0000 - val_auc: 0.7273 - val_prc: 0.1218\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1920 - Sensitivity: 0.0303 - tn: 14571.0000 - auc: 0.6205 - prc: 0.0647 - val_loss: 0.2476 - val_Sensitivity: 0.1801 - val_tn: 4677.0000 - val_auc: 0.7272 - val_prc: 0.1221\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1902 - Sensitivity: 0.0357 - tn: 14559.0000 - auc: 0.6275 - prc: 0.0723 - val_loss: 0.2501 - val_Sensitivity: 0.1943 - val_tn: 4670.0000 - val_auc: 0.7266 - val_prc: 0.1219\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1946 - Sensitivity: 0.0446 - tn: 14532.0000 - auc: 0.6232 - prc: 0.0661 - val_loss: 0.2478 - val_Sensitivity: 0.1896 - val_tn: 4674.0000 - val_auc: 0.7268 - val_prc: 0.1218\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1977 - Sensitivity: 0.0463 - tn: 14535.0000 - auc: 0.6239 - prc: 0.0683 - val_loss: 0.2458 - val_Sensitivity: 0.1801 - val_tn: 4678.0000 - val_auc: 0.7251 - val_prc: 0.1218\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1861 - Sensitivity: 0.0428 - tn: 14539.0000 - auc: 0.6587 - prc: 0.0708 - val_loss: 0.2462 - val_Sensitivity: 0.1801 - val_tn: 4677.0000 - val_auc: 0.7252 - val_prc: 0.1217\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1885 - Sensitivity: 0.0446 - tn: 14530.0000 - auc: 0.6400 - prc: 0.0731 - val_loss: 0.2469 - val_Sensitivity: 0.1754 - val_tn: 4677.0000 - val_auc: 0.7259 - val_prc: 0.1216\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2014 - Sensitivity: 0.0410 - tn: 14507.0000 - auc: 0.6091 - prc: 0.0611 - val_loss: 0.2451 - val_Sensitivity: 0.1754 - val_tn: 4680.0000 - val_auc: 0.7262 - val_prc: 0.1218\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1921 - Sensitivity: 0.0428 - tn: 14537.0000 - auc: 0.6297 - prc: 0.0690 - val_loss: 0.2424 - val_Sensitivity: 0.1706 - val_tn: 4684.0000 - val_auc: 0.7257 - val_prc: 0.1220\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2018 - Sensitivity: 0.0392 - tn: 14528.0000 - auc: 0.6085 - prc: 0.0633 - val_loss: 0.2429 - val_Sensitivity: 0.1706 - val_tn: 4684.0000 - val_auc: 0.7256 - val_prc: 0.1219\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1943 - Sensitivity: 0.0446 - tn: 14550.0000 - auc: 0.6228 - prc: 0.0720 - val_loss: 0.2439 - val_Sensitivity: 0.1706 - val_tn: 4682.0000 - val_auc: 0.7262 - val_prc: 0.1224\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1912 - Sensitivity: 0.0392 - tn: 14540.0000 - auc: 0.6301 - prc: 0.0689 - val_loss: 0.2421 - val_Sensitivity: 0.1706 - val_tn: 4683.0000 - val_auc: 0.7251 - val_prc: 0.1221\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1917 - Sensitivity: 0.0463 - tn: 14547.0000 - auc: 0.6253 - prc: 0.0750 - val_loss: 0.2444 - val_Sensitivity: 0.1706 - val_tn: 4681.0000 - val_auc: 0.7250 - val_prc: 0.1219\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1907 - Sensitivity: 0.0446 - tn: 14548.0000 - auc: 0.6372 - prc: 0.0712 - val_loss: 0.2474 - val_Sensitivity: 0.1801 - val_tn: 4675.0000 - val_auc: 0.7251 - val_prc: 0.1218\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1936 - Sensitivity: 0.0446 - tn: 14560.0000 - auc: 0.6255 - prc: 0.0758 - val_loss: 0.2501 - val_Sensitivity: 0.1801 - val_tn: 4668.0000 - val_auc: 0.7256 - val_prc: 0.1214\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1914 - Sensitivity: 0.0410 - tn: 14546.0000 - auc: 0.6346 - prc: 0.0714 - val_loss: 0.2523 - val_Sensitivity: 0.1848 - val_tn: 4663.0000 - val_auc: 0.7251 - val_prc: 0.1209\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1935 - Sensitivity: 0.0446 - tn: 14521.0000 - auc: 0.6379 - prc: 0.0686 - val_loss: 0.2505 - val_Sensitivity: 0.1801 - val_tn: 4664.0000 - val_auc: 0.7260 - val_prc: 0.1213\n",
      "Epoch 1/500\n",
      "30/30 [==============================] - 2s 34ms/step - loss: 1.2201 - Sensitivity: 0.5215 - tn: 6334.0000 - auc: 0.4730 - prc: 0.0411 - val_loss: 0.8597 - val_Sensitivity: 0.9635 - val_tn: 242.0000 - val_auc: 0.4779 - val_prc: 0.0377\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0192 - Sensitivity: 0.4940 - tn: 7571.0000 - auc: 0.5192 - prc: 0.0433 - val_loss: 0.8033 - val_Sensitivity: 0.9219 - val_tn: 516.0000 - val_auc: 0.5251 - val_prc: 0.0401\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.9018 - Sensitivity: 0.4286 - tn: 8549.0000 - auc: 0.5140 - prc: 0.0435 - val_loss: 0.7208 - val_Sensitivity: 0.7344 - val_tn: 1751.0000 - val_auc: 0.5575 - val_prc: 0.0439\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7663 - Sensitivity: 0.3855 - tn: 9573.0000 - auc: 0.5169 - prc: 0.0452 - val_loss: 0.6499 - val_Sensitivity: 0.3906 - val_tn: 3501.0000 - val_auc: 0.6049 - val_prc: 0.0514\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6679 - Sensitivity: 0.3236 - tn: 10338.0000 - auc: 0.5069 - prc: 0.0445 - val_loss: 0.5908 - val_Sensitivity: 0.1406 - val_tn: 4537.0000 - val_auc: 0.6418 - val_prc: 0.0605\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5904 - Sensitivity: 0.2823 - tn: 11039.0000 - auc: 0.5044 - prc: 0.0435 - val_loss: 0.5460 - val_Sensitivity: 0.1042 - val_tn: 4723.0000 - val_auc: 0.6793 - val_prc: 0.0745\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.5327 - Sensitivity: 0.2513 - tn: 11565.0000 - auc: 0.5041 - prc: 0.0440 - val_loss: 0.5119 - val_Sensitivity: 0.1198 - val_tn: 4764.0000 - val_auc: 0.7015 - val_prc: 0.0846\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4821 - Sensitivity: 0.2255 - tn: 12079.0000 - auc: 0.5127 - prc: 0.0470 - val_loss: 0.4818 - val_Sensitivity: 0.1250 - val_tn: 4766.0000 - val_auc: 0.7132 - val_prc: 0.0930\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.4495 - Sensitivity: 0.1566 - tn: 12496.0000 - auc: 0.4839 - prc: 0.0393 - val_loss: 0.4541 - val_Sensitivity: 0.1094 - val_tn: 4766.0000 - val_auc: 0.7205 - val_prc: 0.0978\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.4087 - Sensitivity: 0.1497 - tn: 12850.0000 - auc: 0.4906 - prc: 0.0411 - val_loss: 0.4259 - val_Sensitivity: 0.1042 - val_tn: 4775.0000 - val_auc: 0.7248 - val_prc: 0.1009\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.3776 - Sensitivity: 0.1411 - tn: 13116.0000 - auc: 0.5017 - prc: 0.0450 - val_loss: 0.4066 - val_Sensitivity: 0.0990 - val_tn: 4783.0000 - val_auc: 0.7302 - val_prc: 0.1031\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.3533 - Sensitivity: 0.1119 - tn: 13332.0000 - auc: 0.5068 - prc: 0.0428 - val_loss: 0.3855 - val_Sensitivity: 0.0990 - val_tn: 4787.0000 - val_auc: 0.7344 - val_prc: 0.1053\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.3400 - Sensitivity: 0.1188 - tn: 13501.0000 - auc: 0.4809 - prc: 0.0422 - val_loss: 0.3743 - val_Sensitivity: 0.1146 - val_tn: 4779.0000 - val_auc: 0.7370 - val_prc: 0.1072\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.3070 - Sensitivity: 0.1067 - tn: 13698.0000 - auc: 0.5287 - prc: 0.0505 - val_loss: 0.3579 - val_Sensitivity: 0.1042 - val_tn: 4785.0000 - val_auc: 0.7400 - val_prc: 0.1086\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.3067 - Sensitivity: 0.0843 - tn: 13763.0000 - auc: 0.4848 - prc: 0.0398 - val_loss: 0.3484 - val_Sensitivity: 0.1094 - val_tn: 4779.0000 - val_auc: 0.7414 - val_prc: 0.1092\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2848 - Sensitivity: 0.0895 - tn: 13910.0000 - auc: 0.5277 - prc: 0.0472 - val_loss: 0.3406 - val_Sensitivity: 0.1094 - val_tn: 4774.0000 - val_auc: 0.7430 - val_prc: 0.1102\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2849 - Sensitivity: 0.0585 - tn: 13948.0000 - auc: 0.5008 - prc: 0.0406 - val_loss: 0.3369 - val_Sensitivity: 0.1094 - val_tn: 4758.0000 - val_auc: 0.7446 - val_prc: 0.1110\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2635 - Sensitivity: 0.0826 - tn: 14053.0000 - auc: 0.5462 - prc: 0.0480 - val_loss: 0.3334 - val_Sensitivity: 0.1198 - val_tn: 4749.0000 - val_auc: 0.7475 - val_prc: 0.1119\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2653 - Sensitivity: 0.0723 - tn: 14063.0000 - auc: 0.5160 - prc: 0.0476 - val_loss: 0.3240 - val_Sensitivity: 0.1198 - val_tn: 4748.0000 - val_auc: 0.7489 - val_prc: 0.1122\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2673 - Sensitivity: 0.0568 - tn: 14170.0000 - auc: 0.4948 - prc: 0.0427 - val_loss: 0.3162 - val_Sensitivity: 0.1146 - val_tn: 4754.0000 - val_auc: 0.7521 - val_prc: 0.1127\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2553 - Sensitivity: 0.0534 - tn: 14192.0000 - auc: 0.5254 - prc: 0.0459 - val_loss: 0.3159 - val_Sensitivity: 0.1354 - val_tn: 4742.0000 - val_auc: 0.7530 - val_prc: 0.1130\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2492 - Sensitivity: 0.0723 - tn: 14196.0000 - auc: 0.5340 - prc: 0.0520 - val_loss: 0.3090 - val_Sensitivity: 0.1354 - val_tn: 4743.0000 - val_auc: 0.7537 - val_prc: 0.1133\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2563 - Sensitivity: 0.0396 - tn: 14252.0000 - auc: 0.4961 - prc: 0.0403 - val_loss: 0.3036 - val_Sensitivity: 0.1354 - val_tn: 4743.0000 - val_auc: 0.7555 - val_prc: 0.1141\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2422 - Sensitivity: 0.0534 - tn: 14281.0000 - auc: 0.5341 - prc: 0.0466 - val_loss: 0.3010 - val_Sensitivity: 0.1406 - val_tn: 4739.0000 - val_auc: 0.7566 - val_prc: 0.1143\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2360 - Sensitivity: 0.0516 - tn: 14327.0000 - auc: 0.5418 - prc: 0.0493 - val_loss: 0.3040 - val_Sensitivity: 0.1615 - val_tn: 4734.0000 - val_auc: 0.7585 - val_prc: 0.1144\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2360 - Sensitivity: 0.0551 - tn: 14322.0000 - auc: 0.5437 - prc: 0.0543 - val_loss: 0.3017 - val_Sensitivity: 0.1719 - val_tn: 4724.0000 - val_auc: 0.7590 - val_prc: 0.1146\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2312 - Sensitivity: 0.0620 - tn: 14372.0000 - auc: 0.5505 - prc: 0.0545 - val_loss: 0.2982 - val_Sensitivity: 0.1719 - val_tn: 4723.0000 - val_auc: 0.7580 - val_prc: 0.1144\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2343 - Sensitivity: 0.0516 - tn: 14389.0000 - auc: 0.5365 - prc: 0.0506 - val_loss: 0.2937 - val_Sensitivity: 0.1823 - val_tn: 4724.0000 - val_auc: 0.7599 - val_prc: 0.1151\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2250 - Sensitivity: 0.0568 - tn: 14375.0000 - auc: 0.5551 - prc: 0.0586 - val_loss: 0.2903 - val_Sensitivity: 0.1823 - val_tn: 4722.0000 - val_auc: 0.7590 - val_prc: 0.1147\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2330 - Sensitivity: 0.0293 - tn: 14369.0000 - auc: 0.5403 - prc: 0.0494 - val_loss: 0.2861 - val_Sensitivity: 0.1875 - val_tn: 4722.0000 - val_auc: 0.7603 - val_prc: 0.1155\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2312 - Sensitivity: 0.0499 - tn: 14398.0000 - auc: 0.5426 - prc: 0.0485 - val_loss: 0.2851 - val_Sensitivity: 0.1875 - val_tn: 4720.0000 - val_auc: 0.7606 - val_prc: 0.1159\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2194 - Sensitivity: 0.0620 - tn: 14416.0000 - auc: 0.5637 - prc: 0.0600 - val_loss: 0.2770 - val_Sensitivity: 0.1875 - val_tn: 4724.0000 - val_auc: 0.7607 - val_prc: 0.1160\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2291 - Sensitivity: 0.0568 - tn: 14419.0000 - auc: 0.5423 - prc: 0.0544 - val_loss: 0.2741 - val_Sensitivity: 0.1823 - val_tn: 4727.0000 - val_auc: 0.7609 - val_prc: 0.1163\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2260 - Sensitivity: 0.0275 - tn: 14426.0000 - auc: 0.5645 - prc: 0.0513 - val_loss: 0.2701 - val_Sensitivity: 0.1719 - val_tn: 4728.0000 - val_auc: 0.7615 - val_prc: 0.1165\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2192 - Sensitivity: 0.0534 - tn: 14441.0000 - auc: 0.5701 - prc: 0.0585 - val_loss: 0.2706 - val_Sensitivity: 0.1875 - val_tn: 4721.0000 - val_auc: 0.7610 - val_prc: 0.1164\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2195 - Sensitivity: 0.0465 - tn: 14454.0000 - auc: 0.5633 - prc: 0.0526 - val_loss: 0.2720 - val_Sensitivity: 0.1979 - val_tn: 4716.0000 - val_auc: 0.7611 - val_prc: 0.1163\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2259 - Sensitivity: 0.0241 - tn: 14435.0000 - auc: 0.5371 - prc: 0.0477 - val_loss: 0.2748 - val_Sensitivity: 0.2083 - val_tn: 4712.0000 - val_auc: 0.7613 - val_prc: 0.1160\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2246 - Sensitivity: 0.0361 - tn: 14445.0000 - auc: 0.5499 - prc: 0.0530 - val_loss: 0.2774 - val_Sensitivity: 0.2135 - val_tn: 4706.0000 - val_auc: 0.7623 - val_prc: 0.1159\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2220 - Sensitivity: 0.0568 - tn: 14464.0000 - auc: 0.5640 - prc: 0.0587 - val_loss: 0.2759 - val_Sensitivity: 0.2083 - val_tn: 4706.0000 - val_auc: 0.7616 - val_prc: 0.1162\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2126 - Sensitivity: 0.0534 - tn: 14463.0000 - auc: 0.5899 - prc: 0.0621 - val_loss: 0.2716 - val_Sensitivity: 0.2083 - val_tn: 4709.0000 - val_auc: 0.7633 - val_prc: 0.1165\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2153 - Sensitivity: 0.0293 - tn: 14484.0000 - auc: 0.5710 - prc: 0.0531 - val_loss: 0.2718 - val_Sensitivity: 0.2240 - val_tn: 4703.0000 - val_auc: 0.7635 - val_prc: 0.1164\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2274 - Sensitivity: 0.0327 - tn: 14477.0000 - auc: 0.5521 - prc: 0.0501 - val_loss: 0.2732 - val_Sensitivity: 0.2292 - val_tn: 4698.0000 - val_auc: 0.7643 - val_prc: 0.1166\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2173 - Sensitivity: 0.0448 - tn: 14496.0000 - auc: 0.5553 - prc: 0.0571 - val_loss: 0.2732 - val_Sensitivity: 0.2344 - val_tn: 4696.0000 - val_auc: 0.7643 - val_prc: 0.1167\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2157 - Sensitivity: 0.0448 - tn: 14475.0000 - auc: 0.5741 - prc: 0.0596 - val_loss: 0.2726 - val_Sensitivity: 0.2344 - val_tn: 4695.0000 - val_auc: 0.7649 - val_prc: 0.1170\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2108 - Sensitivity: 0.0482 - tn: 14493.0000 - auc: 0.5796 - prc: 0.0615 - val_loss: 0.2697 - val_Sensitivity: 0.2292 - val_tn: 4699.0000 - val_auc: 0.7640 - val_prc: 0.1169\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2173 - Sensitivity: 0.0258 - tn: 14474.0000 - auc: 0.5629 - prc: 0.0535 - val_loss: 0.2695 - val_Sensitivity: 0.2344 - val_tn: 4694.0000 - val_auc: 0.7643 - val_prc: 0.1169\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2172 - Sensitivity: 0.0310 - tn: 14480.0000 - auc: 0.5716 - prc: 0.0525 - val_loss: 0.2657 - val_Sensitivity: 0.2292 - val_tn: 4698.0000 - val_auc: 0.7638 - val_prc: 0.1170\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2122 - Sensitivity: 0.0413 - tn: 14499.0000 - auc: 0.5705 - prc: 0.0582 - val_loss: 0.2690 - val_Sensitivity: 0.2344 - val_tn: 4690.0000 - val_auc: 0.7643 - val_prc: 0.1172\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2116 - Sensitivity: 0.0620 - tn: 14478.0000 - auc: 0.5867 - prc: 0.0663 - val_loss: 0.2656 - val_Sensitivity: 0.2344 - val_tn: 4693.0000 - val_auc: 0.7645 - val_prc: 0.1173\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2120 - Sensitivity: 0.0499 - tn: 14505.0000 - auc: 0.5932 - prc: 0.0605 - val_loss: 0.2625 - val_Sensitivity: 0.2292 - val_tn: 4695.0000 - val_auc: 0.7649 - val_prc: 0.1179\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2181 - Sensitivity: 0.0344 - tn: 14479.0000 - auc: 0.5644 - prc: 0.0526 - val_loss: 0.2630 - val_Sensitivity: 0.2344 - val_tn: 4691.0000 - val_auc: 0.7643 - val_prc: 0.1179\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2130 - Sensitivity: 0.0361 - tn: 14516.0000 - auc: 0.5715 - prc: 0.0569 - val_loss: 0.2668 - val_Sensitivity: 0.2448 - val_tn: 4684.0000 - val_auc: 0.7647 - val_prc: 0.1175\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2161 - Sensitivity: 0.0413 - tn: 14479.0000 - auc: 0.5775 - prc: 0.0580 - val_loss: 0.2681 - val_Sensitivity: 0.2448 - val_tn: 4682.0000 - val_auc: 0.7646 - val_prc: 0.1174\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2064 - Sensitivity: 0.0430 - tn: 14498.0000 - auc: 0.6012 - prc: 0.0598 - val_loss: 0.2677 - val_Sensitivity: 0.2448 - val_tn: 4681.0000 - val_auc: 0.7635 - val_prc: 0.1172\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2121 - Sensitivity: 0.0396 - tn: 14515.0000 - auc: 0.5792 - prc: 0.0591 - val_loss: 0.2664 - val_Sensitivity: 0.2344 - val_tn: 4684.0000 - val_auc: 0.7635 - val_prc: 0.1170\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2147 - Sensitivity: 0.0413 - tn: 14509.0000 - auc: 0.5726 - prc: 0.0594 - val_loss: 0.2648 - val_Sensitivity: 0.2396 - val_tn: 4683.0000 - val_auc: 0.7641 - val_prc: 0.1171\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2103 - Sensitivity: 0.0344 - tn: 14495.0000 - auc: 0.5777 - prc: 0.0592 - val_loss: 0.2661 - val_Sensitivity: 0.2448 - val_tn: 4684.0000 - val_auc: 0.7638 - val_prc: 0.1168\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2074 - Sensitivity: 0.0361 - tn: 14505.0000 - auc: 0.5948 - prc: 0.0633 - val_loss: 0.2665 - val_Sensitivity: 0.2448 - val_tn: 4678.0000 - val_auc: 0.7635 - val_prc: 0.1168\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2107 - Sensitivity: 0.0448 - tn: 14498.0000 - auc: 0.5832 - prc: 0.0609 - val_loss: 0.2590 - val_Sensitivity: 0.2292 - val_tn: 4689.0000 - val_auc: 0.7634 - val_prc: 0.1172\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2082 - Sensitivity: 0.0396 - tn: 14492.0000 - auc: 0.5906 - prc: 0.0606 - val_loss: 0.2606 - val_Sensitivity: 0.2344 - val_tn: 4688.0000 - val_auc: 0.7635 - val_prc: 0.1173\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2108 - Sensitivity: 0.0275 - tn: 14525.0000 - auc: 0.5870 - prc: 0.0590 - val_loss: 0.2628 - val_Sensitivity: 0.2500 - val_tn: 4677.0000 - val_auc: 0.7637 - val_prc: 0.1173\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2092 - Sensitivity: 0.0413 - tn: 14492.0000 - auc: 0.5923 - prc: 0.0614 - val_loss: 0.2619 - val_Sensitivity: 0.2448 - val_tn: 4679.0000 - val_auc: 0.7641 - val_prc: 0.1175\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2072 - Sensitivity: 0.0430 - tn: 14503.0000 - auc: 0.6028 - prc: 0.0660 - val_loss: 0.2641 - val_Sensitivity: 0.2552 - val_tn: 4672.0000 - val_auc: 0.7634 - val_prc: 0.1169\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2088 - Sensitivity: 0.0361 - tn: 14489.0000 - auc: 0.6141 - prc: 0.0629 - val_loss: 0.2637 - val_Sensitivity: 0.2552 - val_tn: 4677.0000 - val_auc: 0.7641 - val_prc: 0.1170\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2124 - Sensitivity: 0.0327 - tn: 14502.0000 - auc: 0.5895 - prc: 0.0598 - val_loss: 0.2642 - val_Sensitivity: 0.2552 - val_tn: 4678.0000 - val_auc: 0.7636 - val_prc: 0.1169\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2104 - Sensitivity: 0.0361 - tn: 14510.0000 - auc: 0.5862 - prc: 0.0621 - val_loss: 0.2646 - val_Sensitivity: 0.2552 - val_tn: 4676.0000 - val_auc: 0.7629 - val_prc: 0.1164\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2086 - Sensitivity: 0.0293 - tn: 14495.0000 - auc: 0.6078 - prc: 0.0668 - val_loss: 0.2630 - val_Sensitivity: 0.2344 - val_tn: 4680.0000 - val_auc: 0.7629 - val_prc: 0.1166\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2047 - Sensitivity: 0.0465 - tn: 14505.0000 - auc: 0.6117 - prc: 0.0681 - val_loss: 0.2647 - val_Sensitivity: 0.2604 - val_tn: 4676.0000 - val_auc: 0.7616 - val_prc: 0.1163\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2086 - Sensitivity: 0.0430 - tn: 14496.0000 - auc: 0.5914 - prc: 0.0620 - val_loss: 0.2628 - val_Sensitivity: 0.2604 - val_tn: 4675.0000 - val_auc: 0.7617 - val_prc: 0.1164\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2043 - Sensitivity: 0.0379 - tn: 14527.0000 - auc: 0.6042 - prc: 0.0661 - val_loss: 0.2641 - val_Sensitivity: 0.2656 - val_tn: 4670.0000 - val_auc: 0.7622 - val_prc: 0.1165\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2071 - Sensitivity: 0.0482 - tn: 14510.0000 - auc: 0.5991 - prc: 0.0702 - val_loss: 0.2618 - val_Sensitivity: 0.2656 - val_tn: 4671.0000 - val_auc: 0.7616 - val_prc: 0.1164\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2020 - Sensitivity: 0.0448 - tn: 14510.0000 - auc: 0.6232 - prc: 0.0708 - val_loss: 0.2600 - val_Sensitivity: 0.2708 - val_tn: 4675.0000 - val_auc: 0.7618 - val_prc: 0.1166\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2038 - Sensitivity: 0.0379 - tn: 14522.0000 - auc: 0.6178 - prc: 0.0648 - val_loss: 0.2620 - val_Sensitivity: 0.2760 - val_tn: 4667.0000 - val_auc: 0.7616 - val_prc: 0.1165\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2090 - Sensitivity: 0.0361 - tn: 14500.0000 - auc: 0.6051 - prc: 0.0593 - val_loss: 0.2564 - val_Sensitivity: 0.2500 - val_tn: 4678.0000 - val_auc: 0.7625 - val_prc: 0.1167\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2046 - Sensitivity: 0.0448 - tn: 14533.0000 - auc: 0.6058 - prc: 0.0689 - val_loss: 0.2550 - val_Sensitivity: 0.2500 - val_tn: 4679.0000 - val_auc: 0.7625 - val_prc: 0.1168\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2056 - Sensitivity: 0.0413 - tn: 14519.0000 - auc: 0.6072 - prc: 0.0674 - val_loss: 0.2492 - val_Sensitivity: 0.2448 - val_tn: 4682.0000 - val_auc: 0.7618 - val_prc: 0.1171\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1958 - Sensitivity: 0.0516 - tn: 14520.0000 - auc: 0.6296 - prc: 0.0734 - val_loss: 0.2513 - val_Sensitivity: 0.2448 - val_tn: 4680.0000 - val_auc: 0.7617 - val_prc: 0.1168\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2034 - Sensitivity: 0.0637 - tn: 14505.0000 - auc: 0.6097 - prc: 0.0767 - val_loss: 0.2493 - val_Sensitivity: 0.2396 - val_tn: 4681.0000 - val_auc: 0.7621 - val_prc: 0.1171\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1968 - Sensitivity: 0.0430 - tn: 14526.0000 - auc: 0.6240 - prc: 0.0702 - val_loss: 0.2503 - val_Sensitivity: 0.2396 - val_tn: 4682.0000 - val_auc: 0.7616 - val_prc: 0.1170\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.1992 - Sensitivity: 0.0344 - tn: 14537.0000 - auc: 0.6222 - prc: 0.0751 - val_loss: 0.2546 - val_Sensitivity: 0.2500 - val_tn: 4678.0000 - val_auc: 0.7612 - val_prc: 0.1167\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1985 - Sensitivity: 0.0448 - tn: 14520.0000 - auc: 0.6231 - prc: 0.0742 - val_loss: 0.2582 - val_Sensitivity: 0.2552 - val_tn: 4672.0000 - val_auc: 0.7612 - val_prc: 0.1162\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1976 - Sensitivity: 0.0361 - tn: 14521.0000 - auc: 0.6225 - prc: 0.0681 - val_loss: 0.2574 - val_Sensitivity: 0.2500 - val_tn: 4670.0000 - val_auc: 0.7607 - val_prc: 0.1164\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2032 - Sensitivity: 0.0430 - tn: 14525.0000 - auc: 0.6099 - prc: 0.0686 - val_loss: 0.2536 - val_Sensitivity: 0.2448 - val_tn: 4675.0000 - val_auc: 0.7623 - val_prc: 0.1171\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2039 - Sensitivity: 0.0379 - tn: 14527.0000 - auc: 0.6101 - prc: 0.0694 - val_loss: 0.2538 - val_Sensitivity: 0.2448 - val_tn: 4673.0000 - val_auc: 0.7614 - val_prc: 0.1169\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2066 - Sensitivity: 0.0310 - tn: 14508.0000 - auc: 0.6005 - prc: 0.0609 - val_loss: 0.2566 - val_Sensitivity: 0.2552 - val_tn: 4672.0000 - val_auc: 0.7607 - val_prc: 0.1167\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2007 - Sensitivity: 0.0361 - tn: 14524.0000 - auc: 0.6153 - prc: 0.0653 - val_loss: 0.2564 - val_Sensitivity: 0.2552 - val_tn: 4673.0000 - val_auc: 0.7606 - val_prc: 0.1168\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2006 - Sensitivity: 0.0396 - tn: 14541.0000 - auc: 0.6126 - prc: 0.0676 - val_loss: 0.2522 - val_Sensitivity: 0.2500 - val_tn: 4674.0000 - val_auc: 0.7602 - val_prc: 0.1172\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2040 - Sensitivity: 0.0430 - tn: 14535.0000 - auc: 0.5997 - prc: 0.0675 - val_loss: 0.2559 - val_Sensitivity: 0.2552 - val_tn: 4668.0000 - val_auc: 0.7608 - val_prc: 0.1170\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1953 - Sensitivity: 0.0585 - tn: 14522.0000 - auc: 0.6255 - prc: 0.0791 - val_loss: 0.2600 - val_Sensitivity: 0.2604 - val_tn: 4663.0000 - val_auc: 0.7612 - val_prc: 0.1167\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2024 - Sensitivity: 0.0293 - tn: 14533.0000 - auc: 0.6178 - prc: 0.0667 - val_loss: 0.2597 - val_Sensitivity: 0.2604 - val_tn: 4662.0000 - val_auc: 0.7597 - val_prc: 0.1165\n",
      "Epoch 91/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2007 - Sensitivity: 0.0413 - tn: 14526.0000 - auc: 0.6214 - prc: 0.0712 - val_loss: 0.2580 - val_Sensitivity: 0.2500 - val_tn: 4667.0000 - val_auc: 0.7598 - val_prc: 0.1166\n",
      "Epoch 92/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1966 - Sensitivity: 0.0396 - tn: 14518.0000 - auc: 0.6282 - prc: 0.0727 - val_loss: 0.2561 - val_Sensitivity: 0.2500 - val_tn: 4670.0000 - val_auc: 0.7599 - val_prc: 0.1168\n",
      "Epoch 93/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1996 - Sensitivity: 0.0465 - tn: 14514.0000 - auc: 0.6209 - prc: 0.0710 - val_loss: 0.2569 - val_Sensitivity: 0.2500 - val_tn: 4670.0000 - val_auc: 0.7594 - val_prc: 0.1164\n",
      "Epoch 94/500\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.1972 - Sensitivity: 0.0327 - tn: 14532.0000 - auc: 0.6179 - prc: 0.0686 - val_loss: 0.2569 - val_Sensitivity: 0.2448 - val_tn: 4669.0000 - val_auc: 0.7599 - val_prc: 0.1167\n",
      "Epoch 95/500\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.1990 - Sensitivity: 0.0310 - tn: 14521.0000 - auc: 0.6252 - prc: 0.0676 - val_loss: 0.2583 - val_Sensitivity: 0.2552 - val_tn: 4667.0000 - val_auc: 0.7595 - val_prc: 0.1167\n",
      "Epoch 96/500\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.1946 - Sensitivity: 0.0344 - tn: 14528.0000 - auc: 0.6310 - prc: 0.0689 - val_loss: 0.2570 - val_Sensitivity: 0.2448 - val_tn: 4668.0000 - val_auc: 0.7594 - val_prc: 0.1168\n",
      "Epoch 97/500\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.1999 - Sensitivity: 0.0448 - tn: 14518.0000 - auc: 0.6179 - prc: 0.0662 - val_loss: 0.2557 - val_Sensitivity: 0.2448 - val_tn: 4672.0000 - val_auc: 0.7594 - val_prc: 0.1168\n",
      "Epoch 98/500\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.1919 - Sensitivity: 0.0534 - tn: 14540.0000 - auc: 0.6318 - prc: 0.0756 - val_loss: 0.2570 - val_Sensitivity: 0.2448 - val_tn: 4674.0000 - val_auc: 0.7592 - val_prc: 0.1166\n",
      "Epoch 99/500\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.1979 - Sensitivity: 0.0224 - tn: 14521.0000 - auc: 0.6076 - prc: 0.0591 - val_loss: 0.2600 - val_Sensitivity: 0.2448 - val_tn: 4670.0000 - val_auc: 0.7592 - val_prc: 0.1164\n",
      "Epoch 100/500\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.1959 - Sensitivity: 0.0465 - tn: 14539.0000 - auc: 0.6230 - prc: 0.0742 - val_loss: 0.2604 - val_Sensitivity: 0.2448 - val_tn: 4667.0000 - val_auc: 0.7598 - val_prc: 0.1165\n",
      "Epoch 101/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.1932 - Sensitivity: 0.0413 - tn: 14530.0000 - auc: 0.6327 - prc: 0.0728 - val_loss: 0.2614 - val_Sensitivity: 0.2448 - val_tn: 4669.0000 - val_auc: 0.7597 - val_prc: 0.1163\n",
      "Epoch 1/500\n",
      "30/30 [==============================] - 3s 41ms/step - loss: 1.2060 - Sensitivity: 0.5542 - tn: 6379.0000 - auc: 0.4962 - prc: 0.0400 - val_loss: 0.8571 - val_Sensitivity: 0.9602 - val_tn: 206.0000 - val_auc: 0.4742 - val_prc: 0.0368\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0243 - Sensitivity: 0.4930 - tn: 7555.0000 - auc: 0.5164 - prc: 0.0466 - val_loss: 0.8034 - val_Sensitivity: 0.9403 - val_tn: 465.0000 - val_auc: 0.5211 - val_prc: 0.0399\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.8787 - Sensitivity: 0.4143 - tn: 8647.0000 - auc: 0.5057 - prc: 0.0425 - val_loss: 0.7258 - val_Sensitivity: 0.7562 - val_tn: 1641.0000 - val_auc: 0.5566 - val_prc: 0.0457\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.7785 - Sensitivity: 0.4126 - tn: 9591.0000 - auc: 0.5242 - prc: 0.0444 - val_loss: 0.6461 - val_Sensitivity: 0.3184 - val_tn: 3680.0000 - val_auc: 0.5823 - val_prc: 0.0516\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.6723 - Sensitivity: 0.3147 - tn: 10337.0000 - auc: 0.5051 - prc: 0.0406 - val_loss: 0.5833 - val_Sensitivity: 0.1343 - val_tn: 4582.0000 - val_auc: 0.6039 - val_prc: 0.0588\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.5977 - Sensitivity: 0.2762 - tn: 11048.0000 - auc: 0.5017 - prc: 0.0407 - val_loss: 0.5293 - val_Sensitivity: 0.0597 - val_tn: 4756.0000 - val_auc: 0.6172 - val_prc: 0.0656\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.5313 - Sensitivity: 0.2343 - tn: 11669.0000 - auc: 0.4969 - prc: 0.0419 - val_loss: 0.4939 - val_Sensitivity: 0.0746 - val_tn: 4772.0000 - val_auc: 0.6329 - val_prc: 0.0725\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4735 - Sensitivity: 0.2378 - tn: 12151.0000 - auc: 0.5176 - prc: 0.0459 - val_loss: 0.4696 - val_Sensitivity: 0.0945 - val_tn: 4768.0000 - val_auc: 0.6487 - val_prc: 0.0797\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4330 - Sensitivity: 0.2028 - tn: 12510.0000 - auc: 0.5005 - prc: 0.0438 - val_loss: 0.4406 - val_Sensitivity: 0.0896 - val_tn: 4782.0000 - val_auc: 0.6591 - val_prc: 0.0844\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.4024 - Sensitivity: 0.1556 - tn: 12830.0000 - auc: 0.4940 - prc: 0.0416 - val_loss: 0.4177 - val_Sensitivity: 0.0796 - val_tn: 4790.0000 - val_auc: 0.6669 - val_prc: 0.0877\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.3751 - Sensitivity: 0.1276 - tn: 13099.0000 - auc: 0.5126 - prc: 0.0413 - val_loss: 0.4051 - val_Sensitivity: 0.0945 - val_tn: 4774.0000 - val_auc: 0.6754 - val_prc: 0.0907\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.3456 - Sensitivity: 0.1381 - tn: 13311.0000 - auc: 0.5347 - prc: 0.0463 - val_loss: 0.3844 - val_Sensitivity: 0.0896 - val_tn: 4778.0000 - val_auc: 0.6793 - val_prc: 0.0929\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3284 - Sensitivity: 0.0962 - tn: 13533.0000 - auc: 0.4982 - prc: 0.0409 - val_loss: 0.3673 - val_Sensitivity: 0.0945 - val_tn: 4778.0000 - val_auc: 0.6850 - val_prc: 0.0950\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.3195 - Sensitivity: 0.0927 - tn: 13680.0000 - auc: 0.4913 - prc: 0.0431 - val_loss: 0.3505 - val_Sensitivity: 0.0945 - val_tn: 4779.0000 - val_auc: 0.6909 - val_prc: 0.0967\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2912 - Sensitivity: 0.0769 - tn: 13852.0000 - auc: 0.5142 - prc: 0.0422 - val_loss: 0.3446 - val_Sensitivity: 0.1045 - val_tn: 4774.0000 - val_auc: 0.6968 - val_prc: 0.0985\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2848 - Sensitivity: 0.0979 - tn: 13895.0000 - auc: 0.5132 - prc: 0.0458 - val_loss: 0.3394 - val_Sensitivity: 0.1144 - val_tn: 4761.0000 - val_auc: 0.7029 - val_prc: 0.0997\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2702 - Sensitivity: 0.0787 - tn: 14030.0000 - auc: 0.5382 - prc: 0.0484 - val_loss: 0.3302 - val_Sensitivity: 0.1194 - val_tn: 4765.0000 - val_auc: 0.7067 - val_prc: 0.1005\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2619 - Sensitivity: 0.0717 - tn: 14104.0000 - auc: 0.5286 - prc: 0.0444 - val_loss: 0.3239 - val_Sensitivity: 0.1244 - val_tn: 4759.0000 - val_auc: 0.7090 - val_prc: 0.1015\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2650 - Sensitivity: 0.0717 - tn: 14115.0000 - auc: 0.5040 - prc: 0.0433 - val_loss: 0.3191 - val_Sensitivity: 0.1244 - val_tn: 4754.0000 - val_auc: 0.7140 - val_prc: 0.1022\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2535 - Sensitivity: 0.0664 - tn: 14189.0000 - auc: 0.5219 - prc: 0.0464 - val_loss: 0.3133 - val_Sensitivity: 0.1393 - val_tn: 4751.0000 - val_auc: 0.7160 - val_prc: 0.1025\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2482 - Sensitivity: 0.0682 - tn: 14200.0000 - auc: 0.5481 - prc: 0.0492 - val_loss: 0.3055 - val_Sensitivity: 0.1294 - val_tn: 4754.0000 - val_auc: 0.7188 - val_prc: 0.1032\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2442 - Sensitivity: 0.0455 - tn: 14254.0000 - auc: 0.5354 - prc: 0.0461 - val_loss: 0.2990 - val_Sensitivity: 0.1294 - val_tn: 4756.0000 - val_auc: 0.7204 - val_prc: 0.1037\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2423 - Sensitivity: 0.0542 - tn: 14283.0000 - auc: 0.5318 - prc: 0.0473 - val_loss: 0.2981 - val_Sensitivity: 0.1493 - val_tn: 4745.0000 - val_auc: 0.7232 - val_prc: 0.1042\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2366 - Sensitivity: 0.0542 - tn: 14285.0000 - auc: 0.5420 - prc: 0.0472 - val_loss: 0.2917 - val_Sensitivity: 0.1443 - val_tn: 4751.0000 - val_auc: 0.7257 - val_prc: 0.1047\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2350 - Sensitivity: 0.0612 - tn: 14335.0000 - auc: 0.5291 - prc: 0.0477 - val_loss: 0.2863 - val_Sensitivity: 0.1393 - val_tn: 4751.0000 - val_auc: 0.7271 - val_prc: 0.1049\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2336 - Sensitivity: 0.0524 - tn: 14335.0000 - auc: 0.5348 - prc: 0.0500 - val_loss: 0.2836 - val_Sensitivity: 0.1443 - val_tn: 4744.0000 - val_auc: 0.7285 - val_prc: 0.1052\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2444 - Sensitivity: 0.0367 - tn: 14365.0000 - auc: 0.4968 - prc: 0.0400 - val_loss: 0.2855 - val_Sensitivity: 0.1493 - val_tn: 4736.0000 - val_auc: 0.7312 - val_prc: 0.1059\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.2274 - Sensitivity: 0.0490 - tn: 14378.0000 - auc: 0.5453 - prc: 0.0502 - val_loss: 0.2822 - val_Sensitivity: 0.1592 - val_tn: 4730.0000 - val_auc: 0.7333 - val_prc: 0.1064\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2238 - Sensitivity: 0.0577 - tn: 14404.0000 - auc: 0.5498 - prc: 0.0530 - val_loss: 0.2803 - val_Sensitivity: 0.1592 - val_tn: 4729.0000 - val_auc: 0.7345 - val_prc: 0.1067\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2267 - Sensitivity: 0.0490 - tn: 14395.0000 - auc: 0.5483 - prc: 0.0521 - val_loss: 0.2762 - val_Sensitivity: 0.1592 - val_tn: 4731.0000 - val_auc: 0.7367 - val_prc: 0.1076\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.2233 - Sensitivity: 0.0490 - tn: 14399.0000 - auc: 0.5422 - prc: 0.0486 - val_loss: 0.2725 - val_Sensitivity: 0.1692 - val_tn: 4730.0000 - val_auc: 0.7369 - val_prc: 0.1077\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2139 - Sensitivity: 0.0402 - tn: 14448.0000 - auc: 0.5702 - prc: 0.0544 - val_loss: 0.2749 - val_Sensitivity: 0.1791 - val_tn: 4724.0000 - val_auc: 0.7392 - val_prc: 0.1079\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2193 - Sensitivity: 0.0524 - tn: 14450.0000 - auc: 0.5516 - prc: 0.0517 - val_loss: 0.2727 - val_Sensitivity: 0.1741 - val_tn: 4722.0000 - val_auc: 0.7397 - val_prc: 0.1083\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2158 - Sensitivity: 0.0490 - tn: 14455.0000 - auc: 0.5622 - prc: 0.0544 - val_loss: 0.2640 - val_Sensitivity: 0.1692 - val_tn: 4728.0000 - val_auc: 0.7408 - val_prc: 0.1087\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2241 - Sensitivity: 0.0332 - tn: 14458.0000 - auc: 0.5452 - prc: 0.0466 - val_loss: 0.2656 - val_Sensitivity: 0.1741 - val_tn: 4724.0000 - val_auc: 0.7412 - val_prc: 0.1089\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2103 - Sensitivity: 0.0490 - tn: 14466.0000 - auc: 0.5836 - prc: 0.0564 - val_loss: 0.2636 - val_Sensitivity: 0.1741 - val_tn: 4720.0000 - val_auc: 0.7420 - val_prc: 0.1093\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2162 - Sensitivity: 0.0402 - tn: 14468.0000 - auc: 0.5546 - prc: 0.0528 - val_loss: 0.2627 - val_Sensitivity: 0.1741 - val_tn: 4719.0000 - val_auc: 0.7420 - val_prc: 0.1091\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2187 - Sensitivity: 0.0367 - tn: 14471.0000 - auc: 0.5490 - prc: 0.0494 - val_loss: 0.2594 - val_Sensitivity: 0.1741 - val_tn: 4719.0000 - val_auc: 0.7436 - val_prc: 0.1097\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2063 - Sensitivity: 0.0594 - tn: 14480.0000 - auc: 0.5941 - prc: 0.0654 - val_loss: 0.2552 - val_Sensitivity: 0.1692 - val_tn: 4721.0000 - val_auc: 0.7422 - val_prc: 0.1097\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2175 - Sensitivity: 0.0262 - tn: 14490.0000 - auc: 0.5560 - prc: 0.0500 - val_loss: 0.2603 - val_Sensitivity: 0.1741 - val_tn: 4717.0000 - val_auc: 0.7430 - val_prc: 0.1098\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2159 - Sensitivity: 0.0297 - tn: 14489.0000 - auc: 0.5597 - prc: 0.0511 - val_loss: 0.2569 - val_Sensitivity: 0.1741 - val_tn: 4717.0000 - val_auc: 0.7430 - val_prc: 0.1100\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2118 - Sensitivity: 0.0507 - tn: 14502.0000 - auc: 0.5597 - prc: 0.0579 - val_loss: 0.2572 - val_Sensitivity: 0.1741 - val_tn: 4715.0000 - val_auc: 0.7444 - val_prc: 0.1105\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2118 - Sensitivity: 0.0350 - tn: 14501.0000 - auc: 0.5599 - prc: 0.0550 - val_loss: 0.2580 - val_Sensitivity: 0.1741 - val_tn: 4714.0000 - val_auc: 0.7444 - val_prc: 0.1105\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2155 - Sensitivity: 0.0402 - tn: 14487.0000 - auc: 0.5648 - prc: 0.0531 - val_loss: 0.2574 - val_Sensitivity: 0.1741 - val_tn: 4711.0000 - val_auc: 0.7465 - val_prc: 0.1113\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2122 - Sensitivity: 0.0297 - tn: 14494.0000 - auc: 0.5628 - prc: 0.0514 - val_loss: 0.2563 - val_Sensitivity: 0.1791 - val_tn: 4709.0000 - val_auc: 0.7467 - val_prc: 0.1114\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2081 - Sensitivity: 0.0350 - tn: 14490.0000 - auc: 0.5924 - prc: 0.0574 - val_loss: 0.2565 - val_Sensitivity: 0.1841 - val_tn: 4706.0000 - val_auc: 0.7475 - val_prc: 0.1116\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2090 - Sensitivity: 0.0437 - tn: 14472.0000 - auc: 0.5981 - prc: 0.0602 - val_loss: 0.2504 - val_Sensitivity: 0.1741 - val_tn: 4714.0000 - val_auc: 0.7482 - val_prc: 0.1118\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2162 - Sensitivity: 0.0315 - tn: 14501.0000 - auc: 0.5616 - prc: 0.0490 - val_loss: 0.2517 - val_Sensitivity: 0.1841 - val_tn: 4707.0000 - val_auc: 0.7471 - val_prc: 0.1116\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2053 - Sensitivity: 0.0367 - tn: 14537.0000 - auc: 0.5852 - prc: 0.0594 - val_loss: 0.2537 - val_Sensitivity: 0.1891 - val_tn: 4702.0000 - val_auc: 0.7476 - val_prc: 0.1116\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.2083 - Sensitivity: 0.0385 - tn: 14516.0000 - auc: 0.5733 - prc: 0.0579 - val_loss: 0.2579 - val_Sensitivity: 0.1940 - val_tn: 4698.0000 - val_auc: 0.7478 - val_prc: 0.1115\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2042 - Sensitivity: 0.0437 - tn: 14501.0000 - auc: 0.5937 - prc: 0.0651 - val_loss: 0.2571 - val_Sensitivity: 0.1940 - val_tn: 4698.0000 - val_auc: 0.7484 - val_prc: 0.1115\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2077 - Sensitivity: 0.0507 - tn: 14493.0000 - auc: 0.5854 - prc: 0.0590 - val_loss: 0.2529 - val_Sensitivity: 0.1841 - val_tn: 4701.0000 - val_auc: 0.7479 - val_prc: 0.1117\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.2007 - Sensitivity: 0.0420 - tn: 14508.0000 - auc: 0.6046 - prc: 0.0639 - val_loss: 0.2525 - val_Sensitivity: 0.1841 - val_tn: 4701.0000 - val_auc: 0.7476 - val_prc: 0.1118\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2085 - Sensitivity: 0.0350 - tn: 14498.0000 - auc: 0.5851 - prc: 0.0563 - val_loss: 0.2561 - val_Sensitivity: 0.1891 - val_tn: 4695.0000 - val_auc: 0.7478 - val_prc: 0.1117\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2041 - Sensitivity: 0.0455 - tn: 14525.0000 - auc: 0.5957 - prc: 0.0617 - val_loss: 0.2554 - val_Sensitivity: 0.1891 - val_tn: 4697.0000 - val_auc: 0.7481 - val_prc: 0.1115\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.2096 - Sensitivity: 0.0385 - tn: 14514.0000 - auc: 0.5691 - prc: 0.0560 - val_loss: 0.2538 - val_Sensitivity: 0.1791 - val_tn: 4699.0000 - val_auc: 0.7479 - val_prc: 0.1118\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2044 - Sensitivity: 0.0262 - tn: 14542.0000 - auc: 0.5860 - prc: 0.0565 - val_loss: 0.2541 - val_Sensitivity: 0.1841 - val_tn: 4696.0000 - val_auc: 0.7478 - val_prc: 0.1119\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2085 - Sensitivity: 0.0350 - tn: 14521.0000 - auc: 0.5760 - prc: 0.0549 - val_loss: 0.2548 - val_Sensitivity: 0.1891 - val_tn: 4693.0000 - val_auc: 0.7480 - val_prc: 0.1120\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2118 - Sensitivity: 0.0350 - tn: 14498.0000 - auc: 0.5656 - prc: 0.0527 - val_loss: 0.2551 - val_Sensitivity: 0.1891 - val_tn: 4693.0000 - val_auc: 0.7486 - val_prc: 0.1123\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2032 - Sensitivity: 0.0367 - tn: 14533.0000 - auc: 0.5958 - prc: 0.0626 - val_loss: 0.2565 - val_Sensitivity: 0.1940 - val_tn: 4693.0000 - val_auc: 0.7481 - val_prc: 0.1124\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2041 - Sensitivity: 0.0402 - tn: 14523.0000 - auc: 0.5954 - prc: 0.0634 - val_loss: 0.2549 - val_Sensitivity: 0.1940 - val_tn: 4693.0000 - val_auc: 0.7493 - val_prc: 0.1128\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1979 - Sensitivity: 0.0507 - tn: 14541.0000 - auc: 0.6115 - prc: 0.0809 - val_loss: 0.2521 - val_Sensitivity: 0.1940 - val_tn: 4696.0000 - val_auc: 0.7499 - val_prc: 0.1133\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2067 - Sensitivity: 0.0262 - tn: 14543.0000 - auc: 0.5868 - prc: 0.0574 - val_loss: 0.2522 - val_Sensitivity: 0.1940 - val_tn: 4695.0000 - val_auc: 0.7498 - val_prc: 0.1133\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2063 - Sensitivity: 0.0490 - tn: 14529.0000 - auc: 0.5922 - prc: 0.0632 - val_loss: 0.2509 - val_Sensitivity: 0.1940 - val_tn: 4696.0000 - val_auc: 0.7491 - val_prc: 0.1131\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2017 - Sensitivity: 0.0315 - tn: 14542.0000 - auc: 0.6002 - prc: 0.0589 - val_loss: 0.2536 - val_Sensitivity: 0.1940 - val_tn: 4688.0000 - val_auc: 0.7483 - val_prc: 0.1130\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1967 - Sensitivity: 0.0612 - tn: 14541.0000 - auc: 0.6107 - prc: 0.0767 - val_loss: 0.2570 - val_Sensitivity: 0.1990 - val_tn: 4683.0000 - val_auc: 0.7482 - val_prc: 0.1127\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2078 - Sensitivity: 0.0402 - tn: 14529.0000 - auc: 0.5931 - prc: 0.0604 - val_loss: 0.2565 - val_Sensitivity: 0.1940 - val_tn: 4683.0000 - val_auc: 0.7487 - val_prc: 0.1126\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1997 - Sensitivity: 0.0455 - tn: 14537.0000 - auc: 0.6061 - prc: 0.0650 - val_loss: 0.2571 - val_Sensitivity: 0.1940 - val_tn: 4682.0000 - val_auc: 0.7495 - val_prc: 0.1129\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2060 - Sensitivity: 0.0332 - tn: 14513.0000 - auc: 0.5984 - prc: 0.0599 - val_loss: 0.2557 - val_Sensitivity: 0.1940 - val_tn: 4687.0000 - val_auc: 0.7486 - val_prc: 0.1128\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2007 - Sensitivity: 0.0332 - tn: 14536.0000 - auc: 0.6045 - prc: 0.0629 - val_loss: 0.2523 - val_Sensitivity: 0.1891 - val_tn: 4693.0000 - val_auc: 0.7495 - val_prc: 0.1132\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.2011 - Sensitivity: 0.0420 - tn: 14541.0000 - auc: 0.6033 - prc: 0.0656 - val_loss: 0.2539 - val_Sensitivity: 0.1940 - val_tn: 4693.0000 - val_auc: 0.7493 - val_prc: 0.1132\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.1962 - Sensitivity: 0.0490 - tn: 14536.0000 - auc: 0.6147 - prc: 0.0690 - val_loss: 0.2524 - val_Sensitivity: 0.1940 - val_tn: 4695.0000 - val_auc: 0.7495 - val_prc: 0.1132\n"
     ]
    }
   ],
   "source": [
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    #reset the model with untrained weights\n",
    "    model4 = tf.keras.models.load_model('model4.h5')\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7361053399252054,\n",
       " 0.7592099293606845,\n",
       " 0.7332603435582304,\n",
       " 0.7502934188303323,\n",
       " 0.7504715375782849]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))\n",
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\KChen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lrpreds = []\n",
    "model3 = LogisticRegression(penalty='none', warm_start=False)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7133616851674398,\n",
       " 0.7414817296032156,\n",
       " 0.7070905558403423,\n",
       " 0.7230963823870246,\n",
       " 0.757753201631483]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.746 (0.732-0.759)\n"
     ]
    }
   ],
   "source": [
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print('Neural Network:', round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.729 (0.703-0.754)\n"
     ]
    }
   ],
   "source": [
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.116 (0.104-0.128)\n"
     ]
    }
   ],
   "source": [
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.115 (0.098-0.132)\n"
     ]
    }
   ],
   "source": [
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hep_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_ann_tpr = []\n",
    "hep_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x], drop_intermediate=False)\n",
    "    hep_ann_tpr.append(tpr)\n",
    "    hep_ann_fpr.append(fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5081\n",
      "5080\n",
      "5080\n",
      "5079\n",
      "5080\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(hep_ann_tpr[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_ann_tpr' (list)\n",
      "Stored 'mean_hep_ann_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "mean_hep_ann_tpr = [np.mean(k) for k in zip(*hep_ann_tpr)]\n",
    "mean_hep_ann_fpr = [np.mean(k) for k in zip(*hep_ann_fpr)]\n",
    "%store mean_hep_ann_tpr\n",
    "%store mean_hep_ann_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_lr_tpr' (list)\n",
      "Stored 'mean_hep_lr_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hep_lr_tpr = []\n",
    "hep_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_tpr.append(tpr)\n",
    "    hep_lr_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_tpr[x]) - 310\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_tpr[x]))\n",
    "        hep_lr_tpr[x] = np.delete(hep_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_fpr[x]) - 310\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_fpr[x]))\n",
    "        hep_lr_fpr[x] = np.delete(hep_lr_fpr[x],ind)\n",
    "\n",
    "\n",
    "mean_hep_lr_tpr = [np.mean(k) for k in zip(*hep_lr_tpr)]\n",
    "mean_hep_lr_fpr = [np.mean(k) for k in zip(*hep_lr_fpr)]\n",
    "%store mean_hep_lr_tpr\n",
    "%store mean_hep_lr_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hep_lr_rec = []\n",
    "hep_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_rec.append(rec)\n",
    "    hep_lr_prec.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4829\n",
      "5073\n",
      "4815\n",
      "4987\n",
      "4988\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(hep_lr_rec[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_lr_rec' (list)\n",
      "Stored 'mean_hep_lr_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_rec[x]) - 4800\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_rec[x]))\n",
    "        hep_lr_rec[x] = np.delete(hep_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_lr_prec[x]) - 4800\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_lr_prec[x]))\n",
    "        hep_lr_prec[x] = np.delete(hep_lr_prec[x],ind)\n",
    "\n",
    "mean_hep_lr_rec = [np.mean(k) for k in zip(*hep_lr_rec)]\n",
    "mean_hep_lr_prec = [np.mean(k) for k in zip(*hep_lr_prec)]\n",
    "%store mean_hep_lr_rec\n",
    "%store mean_hep_lr_prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24f2fc2e250>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mean_hep_lr_rec, mean_hep_lr_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_hep_ann_rec' (list)\n",
      "Stored 'mean_hep_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hep_ann_rec = []\n",
    "hep_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_rec.append(rec)\n",
    "    hep_ann_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_ann_rec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_ann_rec[x]))\n",
    "        hep_ann_rec[x] = np.delete(hep_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(hep_ann_prec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(hep_ann_prec[x]))\n",
    "        hep_ann_prec[x] = np.delete(hep_ann_prec[x],ind)\n",
    "\n",
    "mean_hep_ann_rec = [np.mean(k) for k in zip(*hep_ann_rec)]\n",
    "mean_hep_ann_prec = [np.mean(k) for k in zip(*hep_ann_prec)]\n",
    "%store mean_hep_ann_rec\n",
    "%store mean_hep_ann_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hep_ann_tpr = []\n",
    "hep_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_tpr.append(tpr)\n",
    "    hep_ann_fpr.append(fpr)\n",
    "hep_ann_tpr_array = [np.array(x) for x in hep_ann_tpr]\n",
    "mean_hep_ann_tpr = [np.mean(k) for k in zip(*hep_ann_tpr_array)]\n",
    "hep_ann_fpr_array = [np.array(x) for x in hep_ann_fpr]\n",
    "mean_hep_ann_fpr = [np.mean(k) for k in zip(*hep_ann_fpr_array)]\n",
    "%store mean_hep_ann_tpr\n",
    "%store mean_hep_ann_fpr\n",
    "\n",
    "hep_lr_tpr = []\n",
    "hep_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_tpr.append(tpr)\n",
    "    hep_lr_fpr.append(fpr)\n",
    "hep_lr_tpr_array = [np.array(x) for x in hep_lr_tpr]\n",
    "mean_hep_lr_tpr = [np.mean(k) for k in zip(*hep_lr_tpr_array)]\n",
    "hep_lr_fpr_array = [np.array(x) for x in hep_lr_fpr]\n",
    "mean_hep_lr_fpr = [np.mean(k) for k in zip(*hep_lr_fpr_array)]\n",
    "%store mean_hep_lr_tpr\n",
    "%store mean_hep_lr_fpr\n",
    "hep_ann_rec = []\n",
    "hep_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    hep_ann_rec.append(rec)\n",
    "    hep_ann_prec.append(prec)\n",
    "hep_ann_rec_array = [np.array(x) for x in hep_ann_rec]\n",
    "mean_hep_ann_rec = [np.mean(k) for k in zip(*hep_ann_rec_array)]\n",
    "hep_ann_prec_array = [np.array(x) for x in hep_ann_prec]\n",
    "mean_hep_ann_prec = [np.mean(k) for k in zip(*hep_ann_prec_array)]\n",
    "%store mean_hep_ann_rec\n",
    "%store mean_hep_ann_prec\n",
    "\n",
    "hep_lr_rec = []\n",
    "hep_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    hep_lr_rec.append(rec)\n",
    "    hep_lr_prec.append(prec)\n",
    "hep_lr_rec_array = [np.array(x) for x in hep_lr_rec]\n",
    "mean_hep_lr_rec = [np.mean(k) for k in zip(*hep_lr_rec_array)]\n",
    "hep_lr_prec_array = [np.array(x) for x in hep_lr_prec]\n",
    "mean_hep_lr_prec = [np.mean(k) for k in zip(*hep_lr_prec_array)]\n",
    "%store mean_hep_lr_rec\n",
    "%store mean_hep_lr_prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.metrics import specificity_score\n",
    "thresh = np.arange(0, 1, 0.01)\n",
    "#calculate recall at 10 thresholds\n",
    "annrecall_list = []\n",
    "for i in thresh:\n",
    "    annrecall_list.append(recall_score(dy_test[1], annpreds[1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "annspec_list = []\n",
    "for i in thresh:\n",
    "    annspec_list.append(specificity_score(dy_test[1], annpreds[1] > i))\n",
    "from matplotlib import pyplot as plt\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, annrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, annspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "thresh = np.arange(0, 1, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculate recall at 10 thresholds\n",
    "lrrecall_list = []\n",
    "for i in thresh:\n",
    "    lrrecall_list.append(recall_score(dy_test[1], lrpreds[1][:,1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "lrspec_list = []\n",
    "for i in thresh:\n",
    "    lrspec_list.append(specificity_score(dy_test[1], lrpreds[1][:,1] > i))\n",
    "from matplotlib import pyplot as plt\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, lrrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, lrspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7564766839378239\n",
      "0.5928805237315876\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(dy_test[1], lrpreds[1][:,1] > 0.03))\n",
    "print(specificity_score(dy_test[1], lrpreds[1][:,1] > 0.03))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7512953367875648\n",
      "0.6180441898527005\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(dy_test[1], annpreds[1] > 0.125))\n",
    "print(specificity_score(dy_test[1], annpreds[1] > 0.125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "\n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n",
    "\n",
    "\n",
    "p = []\n",
    "z = []\n",
    "\n",
    "for x in range(0,5):\n",
    "    preds_A = annpreds[x]\n",
    "    preds_B = lrpreds[x][:,1]\n",
    "    actual = dy_test[x]\n",
    "\n",
    "    actual = actual.array\n",
    "\n",
    "    def group_preds_by_label(preds, actual):\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "\n",
    "\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "    # Two tailed test\n",
    "    z.append(z_score(var_A, var_B, covar_AB, auc_A, auc_B))\n",
    "    p.append(st.norm.sf(abs(z[x-1]))*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02878554973814149,\n",
       " 0.02878554973814149,\n",
       " 0.041249800102458614,\n",
       " 0.025162249891738275,\n",
       " 0.005967036174264849]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025990037128948946"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p)/(len(p))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
