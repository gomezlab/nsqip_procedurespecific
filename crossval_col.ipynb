{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "print(keras.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_col.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COL_STEROID</th>\n",
       "      <th>COL_MECH_BOWEL_PREP</th>\n",
       "      <th>COL_ORAL_ANTIBIOTIC</th>\n",
       "      <th>COL_CHEMO</th>\n",
       "      <th>COL_INDICATION</th>\n",
       "      <th>COL_ICD9_INDICATION</th>\n",
       "      <th>COL_EMERGENT</th>\n",
       "      <th>COL_ICD9_EMERGENT</th>\n",
       "      <th>COL_APPROACH</th>\n",
       "      <th>COL_ANASTOMOTIC</th>\n",
       "      <th>SEX</th>\n",
       "      <th>PRNCPTX</th>\n",
       "      <th>CPT</th>\n",
       "      <th>WORKRVU</th>\n",
       "      <th>INOUT</th>\n",
       "      <th>TRANST</th>\n",
       "      <th>AGE</th>\n",
       "      <th>OPERYR</th>\n",
       "      <th>ANESTHES</th>\n",
       "      <th>SURGSPEC</th>\n",
       "      <th>ELECTSURG</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>DYSPNEA</th>\n",
       "      <th>FNSTATUS2</th>\n",
       "      <th>VENTILAT</th>\n",
       "      <th>HXCOPD</th>\n",
       "      <th>ASCITES</th>\n",
       "      <th>HXCHF</th>\n",
       "      <th>HYPERMED</th>\n",
       "      <th>RENAFAIL</th>\n",
       "      <th>DIALYSIS</th>\n",
       "      <th>DISCANCR</th>\n",
       "      <th>WNDINF</th>\n",
       "      <th>STEROID</th>\n",
       "      <th>WTLOSS</th>\n",
       "      <th>BLEEDDIS</th>\n",
       "      <th>TRANSFUS</th>\n",
       "      <th>PRSEPIS</th>\n",
       "      <th>PRSODM</th>\n",
       "      <th>PRBUN</th>\n",
       "      <th>PRCREAT</th>\n",
       "      <th>PRWBC</th>\n",
       "      <th>PRHCT</th>\n",
       "      <th>PRPLATE</th>\n",
       "      <th>EMERGNCY</th>\n",
       "      <th>WNDCLAS</th>\n",
       "      <th>ASACLAS</th>\n",
       "      <th>OPTIME</th>\n",
       "      <th>HTOODAY</th>\n",
       "      <th>SSSIPATOS</th>\n",
       "      <th>DSSIPATOS</th>\n",
       "      <th>OSSIPATOS</th>\n",
       "      <th>PNAPATOS</th>\n",
       "      <th>VENTPATOS</th>\n",
       "      <th>UTIPATOS</th>\n",
       "      <th>SEPSISPATOS</th>\n",
       "      <th>SEPSHOCKPATOS</th>\n",
       "      <th>COL_ICD10_INDICATION</th>\n",
       "      <th>COL_ICD10_EMERGENT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>RACE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASEID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6551967</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.8542</td>\n",
       "      <td>97.975872</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>8.4034</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>7.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>28.497449</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552344</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>83.914520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43.4</td>\n",
       "      <td>373.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>31.754826</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552431</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6764</td>\n",
       "      <td>81.646560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>4.7</td>\n",
       "      <td>37.7</td>\n",
       "      <td>309.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>29.052438</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552941</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.4986</td>\n",
       "      <td>47.627160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>6.7</td>\n",
       "      <td>37.2</td>\n",
       "      <td>453.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>21.207195</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>68.945984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>7.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>26.090451</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         COL_STEROID  COL_MECH_BOWEL_PREP  COL_ORAL_ANTIBIOTIC  COL_CHEMO  \\\n",
       "CASEID                                                                      \n",
       "6551967            0                    2                    0          0   \n",
       "6552344            0                    1                    1          0   \n",
       "6552431            0                    2                    2          0   \n",
       "6552941            2                    0                    2          0   \n",
       "6552982            0                    0                    0          0   \n",
       "\n",
       "         COL_INDICATION  COL_ICD9_INDICATION  COL_EMERGENT  COL_ICD9_EMERGENT  \\\n",
       "CASEID                                                                          \n",
       "6551967               3                  451             6                155   \n",
       "6552344               8                  451             6                155   \n",
       "6552431               2                  451             6                155   \n",
       "6552941               5                  451             6                155   \n",
       "6552982               3                  451             6                155   \n",
       "\n",
       "         COL_APPROACH  COL_ANASTOMOTIC  SEX  PRNCPTX  CPT  WORKRVU  INOUT  \\\n",
       "CASEID                                                                      \n",
       "6551967             0                0    2        6    4        3      0   \n",
       "6552344             0                0    1        8    5        2      0   \n",
       "6552431             0                0    1        7    6        5      0   \n",
       "6552941             0                0    1        6    4        3      0   \n",
       "6552982             0                1    1        6    4        3      0   \n",
       "\n",
       "         TRANST   AGE  OPERYR  ANESTHES  SURGSPEC  ELECTSURG  HEIGHT  \\\n",
       "CASEID                                                                 \n",
       "6551967       1  64.0       5         1         1          2  1.8542   \n",
       "6552344       1  44.0       5         1         1          2  1.6256   \n",
       "6552431       1  30.0       5         1         1          2  1.6764   \n",
       "6552941       1  45.0       5         1         1          2  1.4986   \n",
       "6552982       1  50.0       5         1         1          2  1.6256   \n",
       "\n",
       "            WEIGHT  DIABETES  SMOKE  DYSPNEA  FNSTATUS2  VENTILAT  HXCOPD  \\\n",
       "CASEID                                                                      \n",
       "6551967  97.975872         2      0        2          0         0       0   \n",
       "6552344  83.914520         1      0        2          0         0       0   \n",
       "6552431  81.646560         1      0        2          0         0       0   \n",
       "6552941  47.627160         1      0        2          0         0       0   \n",
       "6552982  68.945984         1      0        2          0         0       0   \n",
       "\n",
       "         ASCITES  HXCHF  HYPERMED  RENAFAIL  DIALYSIS  DISCANCR  WNDINF  \\\n",
       "CASEID                                                                    \n",
       "6551967        0      0         2         0         0         0       0   \n",
       "6552344        0      0         2         0         0         0       0   \n",
       "6552431        0      0         0         0         0         0       0   \n",
       "6552941        0      0         0         0         0         0       0   \n",
       "6552982        0      0         0         0         0         0       0   \n",
       "\n",
       "         STEROID  WTLOSS  BLEEDDIS  TRANSFUS  PRSEPIS  PRSODM    PRBUN  \\\n",
       "CASEID                                                                   \n",
       "6551967        0       0         0         0        0   139.0   8.4034   \n",
       "6552344        0       0         0         0        0   139.0  14.0000   \n",
       "6552431        0       0         0         0        0   138.0   3.0000   \n",
       "6552941        2       0         0         0        0   141.0  13.0000   \n",
       "6552982        0       0         0         0        0   138.0  11.0000   \n",
       "\n",
       "         PRCREAT  PRWBC  PRHCT  PRPLATE  EMERGNCY  WNDCLAS  ASACLAS  OPTIME  \\\n",
       "CASEID                                                                        \n",
       "6551967   0.8032    7.1   38.0    190.0         0        1        2   121.0   \n",
       "6552344   0.8600   11.0   43.4    373.0         0        1        1    64.0   \n",
       "6552431   0.6000    4.7   37.7    309.0         0        2        1    99.0   \n",
       "6552941   0.5700    6.7   37.2    453.0         0        1        1    77.0   \n",
       "6552982   0.7300    7.3   29.7    432.0         0        1        1   134.0   \n",
       "\n",
       "         HTOODAY  SSSIPATOS  DSSIPATOS  OSSIPATOS  PNAPATOS  VENTPATOS  \\\n",
       "CASEID                                                                   \n",
       "6551967      0.0          0          0          0         0          0   \n",
       "6552344      0.0          0          0          0         0          0   \n",
       "6552431      0.0          0          0          0         0          0   \n",
       "6552941      0.0          0          0          0         0          0   \n",
       "6552982      0.0          0          0          0         0          0   \n",
       "\n",
       "         UTIPATOS  SEPSISPATOS  SEPSHOCKPATOS  COL_ICD10_INDICATION  \\\n",
       "CASEID                                                                \n",
       "6551967         0            0              0                   445   \n",
       "6552344         0            0              0                   445   \n",
       "6552431         0            0              0                   445   \n",
       "6552941         0            0              0                   445   \n",
       "6552982         0            0              0                   445   \n",
       "\n",
       "         COL_ICD10_EMERGENT        BMI  RACE  \n",
       "CASEID                                        \n",
       "6551967                 123  28.497449     5  \n",
       "6552344                 123  31.754826     6  \n",
       "6552431                 123  29.052438     6  \n",
       "6552941                 123  21.207195     6  \n",
       "6552982                 123  26.090451     5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.drop('CASEID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define outcome \"y\" and predictors \"X\"\n",
    "y = data['COL_ANASTOMOTIC']\n",
    "X = data.drop(['COL_ANASTOMOTIC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 5 folds of cross-validation, with 5 train/test sets, export to csv\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv's back into a dictionary, naming them train or test 1-5\n",
    "d = {}\n",
    "for x in range(0,5):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of training data for X and y\n",
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['COL_ANASTOMOTIC'], axis=1))\n",
    "        dy_train.append(d[x]['COL_ANASTOMOTIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of test data for X and y\n",
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['COL_ANASTOMOTIC'], axis=1))\n",
    "        dy_test.append(d[x]['COL_ANASTOMOTIC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfpreds = []\n",
    "xgbpreds = []\n",
    "model = RandomForestClassifier(n_estimators=1250, min_samples_split=2, min_samples_leaf=8, max_features='auto', max_depth=20, bootstrap=True)\n",
    "model2 = XGBClassifier(n_estimators=50, subsample=0.6, min_child_weight=10, max_depth=6, learning_rate=0.1, colsample_bytree=0.8)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model.fit(X, y)\n",
    "    model2.fit(X, y)\n",
    "    rfpreds.append(model.predict_proba(X_test))\n",
    "    xgbpreds.append(model2.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store rfpreds\n",
    "%store xgbpreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], rfpreds[x][:,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], xgbpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define keras model with 2 layers, 1000 nodes, followed by batch norm and dropout\n",
    "from tensorflow import keras\n",
    "input_shape = [X.shape[1]]\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "for _ in range(2):\n",
    "    model4.add(keras.layers.Dense(1000))\n",
    "    model4.add(keras.layers.BatchNormalization())\n",
    "    model4.add(keras.layers.Dropout(0.8))\n",
    "    model4.add(keras.layers.Activation(\"relu\"))\n",
    "model4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "metrics = [keras.metrics.Recall(name='Sensitivity'), keras.metrics.TrueNegatives(name='tn'), keras.metrics.AUC(name='auc'), keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.00001,\n",
    "    restore_best_weights=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "229/229 [==============================] - 5s 15ms/step - loss: 0.2442 - Sensitivity: 0.0433 - tn: 109461.0000 - auc: 0.5131 - prc: 0.0368 - val_loss: 0.2100 - val_Sensitivity: 0.0662 - val_tn: 37459.0000 - val_auc: 0.6560 - val_prc: 0.0874\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1893 - Sensitivity: 0.0246 - tn: 112227.0000 - auc: 0.5447 - prc: 0.0466 - val_loss: 0.2058 - val_Sensitivity: 0.1002 - val_tn: 37329.0000 - val_auc: 0.6584 - val_prc: 0.0891\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1740 - Sensitivity: 0.0182 - tn: 112477.0000 - auc: 0.5610 - prc: 0.0489 - val_loss: 0.2124 - val_Sensitivity: 0.1033 - val_tn: 37270.0000 - val_auc: 0.6588 - val_prc: 0.0888\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1630 - Sensitivity: 0.0131 - tn: 112611.0000 - auc: 0.5869 - prc: 0.0534 - val_loss: 0.2109 - val_Sensitivity: 0.1057 - val_tn: 37264.0000 - val_auc: 0.6556 - val_prc: 0.0902\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1559 - Sensitivity: 0.0095 - tn: 112759.0000 - auc: 0.6000 - prc: 0.0584 - val_loss: 0.2132 - val_Sensitivity: 0.1057 - val_tn: 37269.0000 - val_auc: 0.6568 - val_prc: 0.0916\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1512 - Sensitivity: 0.0082 - tn: 112784.0000 - auc: 0.6106 - prc: 0.0629 - val_loss: 0.2131 - val_Sensitivity: 0.1025 - val_tn: 37313.0000 - val_auc: 0.6563 - val_prc: 0.0913\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1495 - Sensitivity: 0.0044 - tn: 112839.0000 - auc: 0.6122 - prc: 0.0638 - val_loss: 0.2124 - val_Sensitivity: 0.1033 - val_tn: 37290.0000 - val_auc: 0.6554 - val_prc: 0.0903\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1461 - Sensitivity: 0.0028 - tn: 112867.0000 - auc: 0.6287 - prc: 0.0724 - val_loss: 0.2056 - val_Sensitivity: 0.1002 - val_tn: 37322.0000 - val_auc: 0.6557 - val_prc: 0.0892\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1453 - Sensitivity: 0.0013 - tn: 112879.0000 - auc: 0.6305 - prc: 0.0719 - val_loss: 0.2041 - val_Sensitivity: 0.1017 - val_tn: 37308.0000 - val_auc: 0.6571 - val_prc: 0.0910\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1438 - Sensitivity: 2.5648e-04 - tn: 112880.0000 - auc: 0.6411 - prc: 0.0764 - val_loss: 0.2009 - val_Sensitivity: 0.0962 - val_tn: 37366.0000 - val_auc: 0.6581 - val_prc: 0.0928\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1433 - Sensitivity: 7.6943e-04 - tn: 112879.0000 - auc: 0.6412 - prc: 0.0790 - val_loss: 0.1943 - val_Sensitivity: 0.0954 - val_tn: 37379.0000 - val_auc: 0.6587 - val_prc: 0.0913\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1420 - Sensitivity: 5.1295e-04 - tn: 112884.0000 - auc: 0.6510 - prc: 0.0839 - val_loss: 0.2019 - val_Sensitivity: 0.1073 - val_tn: 37158.0000 - val_auc: 0.6583 - val_prc: 0.0868\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1417 - Sensitivity: 7.6943e-04 - tn: 112887.0000 - auc: 0.6527 - prc: 0.0856 - val_loss: 0.1924 - val_Sensitivity: 0.0923 - val_tn: 37386.0000 - val_auc: 0.6607 - val_prc: 0.0891\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1416 - Sensitivity: 5.1295e-04 - tn: 112883.0000 - auc: 0.6553 - prc: 0.0837 - val_loss: 0.1956 - val_Sensitivity: 0.1096 - val_tn: 37069.0000 - val_auc: 0.6592 - val_prc: 0.0741\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1412 - Sensitivity: 5.1295e-04 - tn: 112888.0000 - auc: 0.6539 - prc: 0.0876 - val_loss: 0.1871 - val_Sensitivity: 0.0844 - val_tn: 37392.0000 - val_auc: 0.6623 - val_prc: 0.0898\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1409 - Sensitivity: 2.5648e-04 - tn: 112885.0000 - auc: 0.6558 - prc: 0.0874 - val_loss: 0.1908 - val_Sensitivity: 0.0899 - val_tn: 37372.0000 - val_auc: 0.6625 - val_prc: 0.0879\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1401 - Sensitivity: 5.1295e-04 - tn: 112886.0000 - auc: 0.6654 - prc: 0.0900 - val_loss: 0.1856 - val_Sensitivity: 0.0024 - val_tn: 37647.0000 - val_auc: 0.6617 - val_prc: 0.0884\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1409 - Sensitivity: 0.0000e+00 - tn: 112889.0000 - auc: 0.6577 - prc: 0.0883 - val_loss: 0.1774 - val_Sensitivity: 0.0410 - val_tn: 37541.0000 - val_auc: 0.6646 - val_prc: 0.0905\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1399 - Sensitivity: 0.0015 - tn: 112885.0000 - auc: 0.6662 - prc: 0.0914 - val_loss: 0.1800 - val_Sensitivity: 0.0442 - val_tn: 37527.0000 - val_auc: 0.6653 - val_prc: 0.0910\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1398 - Sensitivity: 5.1295e-04 - tn: 112888.0000 - auc: 0.6654 - prc: 0.0936 - val_loss: 0.1767 - val_Sensitivity: 0.0726 - val_tn: 37443.0000 - val_auc: 0.6645 - val_prc: 0.0906\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1402 - Sensitivity: 0.0010 - tn: 112891.0000 - auc: 0.6631 - prc: 0.0928 - val_loss: 0.1753 - val_Sensitivity: 0.0142 - val_tn: 37615.0000 - val_auc: 0.6635 - val_prc: 0.0905\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1395 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6703 - prc: 0.0913 - val_loss: 0.1789 - val_Sensitivity: 0.0757 - val_tn: 37442.0000 - val_auc: 0.6638 - val_prc: 0.0906\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1393 - Sensitivity: 7.6943e-04 - tn: 112889.0000 - auc: 0.6701 - prc: 0.0960 - val_loss: 0.1709 - val_Sensitivity: 0.0024 - val_tn: 37661.0000 - val_auc: 0.6662 - val_prc: 0.0943\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1393 - Sensitivity: 0.0010 - tn: 112887.0000 - auc: 0.6737 - prc: 0.0959 - val_loss: 0.1721 - val_Sensitivity: 7.8864e-04 - val_tn: 37661.0000 - val_auc: 0.6663 - val_prc: 0.0921\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1393 - Sensitivity: 5.1295e-04 - tn: 112887.0000 - auc: 0.6705 - prc: 0.0939 - val_loss: 0.1687 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6665 - val_prc: 0.0927\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1388 - Sensitivity: 7.6943e-04 - tn: 112892.0000 - auc: 0.6757 - prc: 0.0967 - val_loss: 0.1698 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6686 - val_prc: 0.0960\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1387 - Sensitivity: 0.0000e+00 - tn: 112891.0000 - auc: 0.6774 - prc: 0.0950 - val_loss: 0.1668 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6681 - val_prc: 0.0958\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6806 - prc: 0.0969 - val_loss: 0.1679 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6697 - val_prc: 0.0968\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1389 - Sensitivity: 0.0000e+00 - tn: 112891.0000 - auc: 0.6749 - prc: 0.0959 - val_loss: 0.1675 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6685 - val_prc: 0.0955\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1378 - Sensitivity: 0.0010 - tn: 112889.0000 - auc: 0.6831 - prc: 0.1011 - val_loss: 0.1656 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6695 - val_prc: 0.0969\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 7.6943e-04 - tn: 112888.0000 - auc: 0.6796 - prc: 0.1012 - val_loss: 0.1639 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6699 - val_prc: 0.0964\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 0.0000e+00 - tn: 112889.0000 - auc: 0.6798 - prc: 0.0971 - val_loss: 0.1646 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6699 - val_prc: 0.0957\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1376 - Sensitivity: 0.0000e+00 - tn: 112891.0000 - auc: 0.6850 - prc: 0.1012 - val_loss: 0.1612 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6711 - val_prc: 0.0959\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1381 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6828 - prc: 0.1010 - val_loss: 0.1621 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6709 - val_prc: 0.0956\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1376 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.6869 - prc: 0.1026 - val_loss: 0.1606 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6703 - val_prc: 0.1000\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1377 - Sensitivity: 5.1295e-04 - tn: 112891.0000 - auc: 0.6842 - prc: 0.1013 - val_loss: 0.1611 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6710 - val_prc: 0.0983\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1371 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6907 - prc: 0.1025 - val_loss: 0.1609 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6713 - val_prc: 0.0945\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1375 - Sensitivity: 5.1295e-04 - tn: 112889.0000 - auc: 0.6883 - prc: 0.1018 - val_loss: 0.1553 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6732 - val_prc: 0.0983\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1369 - Sensitivity: 5.1295e-04 - tn: 112888.0000 - auc: 0.6918 - prc: 0.1038 - val_loss: 0.1551 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6741 - val_prc: 0.0977\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1372 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.6889 - prc: 0.1026 - val_loss: 0.1574 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6730 - val_prc: 0.0958\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1373 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6900 - prc: 0.1022 - val_loss: 0.1594 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6720 - val_prc: 0.0954\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1373 - Sensitivity: 5.1295e-04 - tn: 112891.0000 - auc: 0.6886 - prc: 0.1024 - val_loss: 0.1575 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6727 - val_prc: 0.0951\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1366 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6957 - prc: 0.1055 - val_loss: 0.1542 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6755 - val_prc: 0.0991\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1370 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6910 - prc: 0.1069 - val_loss: 0.1549 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6733 - val_prc: 0.0955\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1366 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6969 - prc: 0.1056 - val_loss: 0.1548 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6750 - val_prc: 0.0981\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1366 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.6968 - prc: 0.1030 - val_loss: 0.1555 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0988\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1366 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6969 - prc: 0.1036 - val_loss: 0.1546 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6730 - val_prc: 0.0975\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1365 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6955 - prc: 0.1060 - val_loss: 0.1557 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6743 - val_prc: 0.0976\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1364 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6970 - prc: 0.1098 - val_loss: 0.1535 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6738 - val_prc: 0.0981\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1361 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7006 - prc: 0.1082 - val_loss: 0.1540 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6746 - val_prc: 0.0991\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6955 - prc: 0.1086 - val_loss: 0.1561 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6738 - val_prc: 0.0989\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1363 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7005 - prc: 0.1051 - val_loss: 0.1530 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6742 - val_prc: 0.0989\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1360 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7004 - prc: 0.1099 - val_loss: 0.1530 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6746 - val_prc: 0.0995\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6988 - prc: 0.1078 - val_loss: 0.1508 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6743 - val_prc: 0.1001\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6998 - prc: 0.1101 - val_loss: 0.1527 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6752 - val_prc: 0.0999\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6995 - prc: 0.1098 - val_loss: 0.1527 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6748 - val_prc: 0.0984\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1359 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7016 - prc: 0.1106 - val_loss: 0.1513 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6747 - val_prc: 0.0989\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1357 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7039 - prc: 0.1131 - val_loss: 0.1521 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6761 - val_prc: 0.0995\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1358 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7032 - prc: 0.1077 - val_loss: 0.1523 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6749 - val_prc: 0.0999\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1352 - Sensitivity: 5.1295e-04 - tn: 112892.0000 - auc: 0.7083 - prc: 0.1150 - val_loss: 0.1491 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6736 - val_prc: 0.0983\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1358 - Sensitivity: 7.6943e-04 - tn: 112888.0000 - auc: 0.7036 - prc: 0.1125 - val_loss: 0.1503 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6745 - val_prc: 0.0989\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1358 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.7041 - prc: 0.1116 - val_loss: 0.1511 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6746 - val_prc: 0.0985\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7072 - prc: 0.1124 - val_loss: 0.1494 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6758 - val_prc: 0.0996\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1354 - Sensitivity: 5.1295e-04 - tn: 112889.0000 - auc: 0.7074 - prc: 0.1115 - val_loss: 0.1494 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6751 - val_prc: 0.0987\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1356 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7033 - prc: 0.1126 - val_loss: 0.1498 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6747 - val_prc: 0.0970\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7083 - prc: 0.1076 - val_loss: 0.1501 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6734 - val_prc: 0.0979\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7074 - prc: 0.1129 - val_loss: 0.1499 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0973\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1353 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7069 - prc: 0.1147 - val_loss: 0.1498 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6731 - val_prc: 0.0979\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112891.0000 - auc: 0.7045 - prc: 0.1147 - val_loss: 0.1485 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6745 - val_prc: 0.0979\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7113 - prc: 0.1145 - val_loss: 0.1466 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6762 - val_prc: 0.0998\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7101 - prc: 0.1130 - val_loss: 0.1473 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6744 - val_prc: 0.0975\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7118 - prc: 0.1139 - val_loss: 0.1492 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6764 - val_prc: 0.0997\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7098 - prc: 0.1142 - val_loss: 0.1505 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6757 - val_prc: 0.0964\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7107 - prc: 0.1138 - val_loss: 0.1474 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6763 - val_prc: 0.0983\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7092 - prc: 0.1145 - val_loss: 0.1495 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6750 - val_prc: 0.0971\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1344 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7171 - prc: 0.1165 - val_loss: 0.1506 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6757 - val_prc: 0.0980\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1346 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7135 - prc: 0.1167 - val_loss: 0.1478 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6764 - val_prc: 0.0994\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1346 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7120 - prc: 0.1196 - val_loss: 0.1483 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6747 - val_prc: 0.0992\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1345 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7176 - prc: 0.1162 - val_loss: 0.1491 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6743 - val_prc: 0.0982\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1350 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7101 - prc: 0.1142 - val_loss: 0.1470 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6751 - val_prc: 0.0988\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1376 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6881 - prc: 0.1043 - val_loss: 0.1452 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7053 - val_prc: 0.1119\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1373 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6879 - prc: 0.1096 - val_loss: 0.1439 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7009 - val_prc: 0.1097\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1371 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6901 - prc: 0.1091 - val_loss: 0.1453 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7010 - val_prc: 0.1116\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1365 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6954 - prc: 0.1127 - val_loss: 0.1433 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6980 - val_prc: 0.1108\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.6962 - prc: 0.1071 - val_loss: 0.1452 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6984 - val_prc: 0.1115\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1364 - Sensitivity: 2.5582e-04 - tn: 112883.0000 - auc: 0.6984 - prc: 0.1121 - val_loss: 0.1440 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6986 - val_prc: 0.1115\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.6945 - prc: 0.1100 - val_loss: 0.1467 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6975 - val_prc: 0.1120\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1363 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6992 - prc: 0.1103 - val_loss: 0.1440 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6968 - val_prc: 0.1097\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1362 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7000 - prc: 0.1102 - val_loss: 0.1421 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6990 - val_prc: 0.1122\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1363 - Sensitivity: 2.5582e-04 - tn: 112883.0000 - auc: 0.6983 - prc: 0.1131 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6958 - val_prc: 0.1115\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7019 - prc: 0.1093 - val_loss: 0.1432 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6941 - val_prc: 0.1099\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 2.5582e-04 - tn: 112882.0000 - auc: 0.7047 - prc: 0.1147 - val_loss: 0.1454 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6947 - val_prc: 0.1112\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1356 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7045 - prc: 0.1166 - val_loss: 0.1430 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6939 - val_prc: 0.1109\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1353 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7088 - prc: 0.1146 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6923 - val_prc: 0.1081\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 5.1164e-04 - tn: 112881.0000 - auc: 0.7038 - prc: 0.1156 - val_loss: 0.1437 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6916 - val_prc: 0.1076\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.7063 - prc: 0.1177 - val_loss: 0.1429 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6911 - val_prc: 0.1090\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 2.5582e-04 - tn: 112882.0000 - auc: 0.7048 - prc: 0.1159 - val_loss: 0.1443 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6924 - val_prc: 0.1097\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7089 - prc: 0.1165 - val_loss: 0.1412 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6930 - val_prc: 0.1091\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.7093 - prc: 0.1167 - val_loss: 0.1432 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6923 - val_prc: 0.1081\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 2.5582e-04 - tn: 112882.0000 - auc: 0.7043 - prc: 0.1154 - val_loss: 0.1431 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6928 - val_prc: 0.1091\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.7130 - prc: 0.1179 - val_loss: 0.1440 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6922 - val_prc: 0.1084\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.7076 - prc: 0.1155 - val_loss: 0.1421 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6930 - val_prc: 0.1086\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1350 - Sensitivity: 2.5582e-04 - tn: 112880.0000 - auc: 0.7112 - prc: 0.1153 - val_loss: 0.1432 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6914 - val_prc: 0.1081\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7074 - prc: 0.1149 - val_loss: 0.1446 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6887 - val_prc: 0.1063\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112880.0000 - auc: 0.7127 - prc: 0.1171 - val_loss: 0.1415 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6913 - val_prc: 0.1070\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1346 - Sensitivity: 5.1164e-04 - tn: 112883.0000 - auc: 0.7138 - prc: 0.1227 - val_loss: 0.1441 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6893 - val_prc: 0.1060\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1350 - Sensitivity: 0.0013 - tn: 112882.0000 - auc: 0.7111 - prc: 0.1178 - val_loss: 0.1436 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6887 - val_prc: 0.1053\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1350 - Sensitivity: 0.0000e+00 - tn: 112881.0000 - auc: 0.7114 - prc: 0.1172 - val_loss: 0.1432 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.6897 - val_prc: 0.1060\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1369 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.6846 - prc: 0.1038 - val_loss: 0.1423 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7122 - val_prc: 0.1159\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1366 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6898 - prc: 0.1029 - val_loss: 0.1425 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7119 - val_prc: 0.1170\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1362 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.6920 - prc: 0.1070 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7093 - val_prc: 0.1158\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1360 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6916 - prc: 0.1088 - val_loss: 0.1410 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7105 - val_prc: 0.1164\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1357 - Sensitivity: 0.0000e+00 - tn: 112914.0000 - auc: 0.6959 - prc: 0.1106 - val_loss: 0.1434 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7077 - val_prc: 0.1146\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1356 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6978 - prc: 0.1063 - val_loss: 0.1424 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7068 - val_prc: 0.1150\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1353 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6996 - prc: 0.1115 - val_loss: 0.1432 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7078 - val_prc: 0.1152\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6996 - prc: 0.1082 - val_loss: 0.1417 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7073 - val_prc: 0.1150\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1352 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.7032 - prc: 0.1072 - val_loss: 0.1416 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7057 - val_prc: 0.1134\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0000e+00 - tn: 112913.0000 - auc: 0.7038 - prc: 0.1099 - val_loss: 0.1420 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7060 - val_prc: 0.1138\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1353 - Sensitivity: 2.5800e-04 - tn: 112916.0000 - auc: 0.7006 - prc: 0.1110 - val_loss: 0.1433 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7047 - val_prc: 0.1129\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1352 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.7028 - prc: 0.1113 - val_loss: 0.1428 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7050 - val_prc: 0.1135\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 2.5800e-04 - tn: 112914.0000 - auc: 0.7041 - prc: 0.1118 - val_loss: 0.1425 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7045 - val_prc: 0.1132\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.7063 - prc: 0.1102 - val_loss: 0.1424 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7025 - val_prc: 0.1120\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1346 - Sensitivity: 2.6116e-04 - tn: 112960.0000 - auc: 0.6912 - prc: 0.1122 - val_loss: 0.1435 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7213 - val_prc: 0.1166\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1343 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.6969 - prc: 0.1097 - val_loss: 0.1451 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7183 - val_prc: 0.1151\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1343 - Sensitivity: 5.2233e-04 - tn: 112961.0000 - auc: 0.6946 - prc: 0.1091 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7164 - val_prc: 0.1164\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1344 - Sensitivity: 5.2233e-04 - tn: 112963.0000 - auc: 0.6958 - prc: 0.1120 - val_loss: 0.1451 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7157 - val_prc: 0.1137\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 2.6116e-04 - tn: 112961.0000 - auc: 0.6964 - prc: 0.1124 - val_loss: 0.1462 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7120 - val_prc: 0.1121\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1336 - Sensitivity: 5.2233e-04 - tn: 112960.0000 - auc: 0.7029 - prc: 0.1140 - val_loss: 0.1440 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7158 - val_prc: 0.1132\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1340 - Sensitivity: 2.6116e-04 - tn: 112961.0000 - auc: 0.7001 - prc: 0.1084 - val_loss: 0.1460 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7112 - val_prc: 0.1118\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1338 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7003 - prc: 0.1098 - val_loss: 0.1440 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7135 - val_prc: 0.1125\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1340 - Sensitivity: 5.2233e-04 - tn: 112960.0000 - auc: 0.6984 - prc: 0.1109 - val_loss: 0.1446 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7113 - val_prc: 0.1115\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1338 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.6995 - prc: 0.1136 - val_loss: 0.1433 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7118 - val_prc: 0.1119\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7029 - prc: 0.1123 - val_loss: 0.1430 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7134 - val_prc: 0.1110\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 2.6116e-04 - tn: 112959.0000 - auc: 0.7010 - prc: 0.1134 - val_loss: 0.1461 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7080 - val_prc: 0.1097\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1332 - Sensitivity: 2.6116e-04 - tn: 112962.0000 - auc: 0.7028 - prc: 0.1150 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7092 - val_prc: 0.1085\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1333 - Sensitivity: 5.2233e-04 - tn: 112958.0000 - auc: 0.7051 - prc: 0.1134 - val_loss: 0.1463 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7055 - val_prc: 0.1085\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1333 - Sensitivity: 2.6116e-04 - tn: 112959.0000 - auc: 0.7068 - prc: 0.1148 - val_loss: 0.1462 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7070 - val_prc: 0.1087\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1332 - Sensitivity: 2.6116e-04 - tn: 112962.0000 - auc: 0.7068 - prc: 0.1148 - val_loss: 0.1458 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7067 - val_prc: 0.1096\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 2.6116e-04 - tn: 112960.0000 - auc: 0.7095 - prc: 0.1156 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7085 - val_prc: 0.1086\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0018 - tn: 112959.0000 - auc: 0.7068 - prc: 0.1191 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7068 - val_prc: 0.1086\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1330 - Sensitivity: 2.6116e-04 - tn: 112959.0000 - auc: 0.7070 - prc: 0.1176 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7076 - val_prc: 0.1079\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 5.2233e-04 - tn: 112960.0000 - auc: 0.7067 - prc: 0.1167 - val_loss: 0.1444 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7052 - val_prc: 0.1078\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1323 - Sensitivity: 0.0010 - tn: 112960.0000 - auc: 0.7155 - prc: 0.1206 - val_loss: 0.1440 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7085 - val_prc: 0.1080\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 2.6042e-04 - tn: 112951.0000 - auc: 0.6959 - prc: 0.1063 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7125 - val_prc: 0.1267\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.6959 - prc: 0.1058 - val_loss: 0.1450 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7109 - val_prc: 0.1250\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.6930 - prc: 0.1078 - val_loss: 0.1420 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7109 - val_prc: 0.1252\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1344 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.7000 - prc: 0.1102 - val_loss: 0.1443 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7065 - val_prc: 0.1256\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1340 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.7047 - prc: 0.1085 - val_loss: 0.1431 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7057 - val_prc: 0.1237\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1343 - Sensitivity: 2.6042e-04 - tn: 112953.0000 - auc: 0.7023 - prc: 0.1078 - val_loss: 0.1424 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7051 - val_prc: 0.1238\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1342 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7030 - prc: 0.1084 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7048 - val_prc: 0.1221\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1341 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7024 - prc: 0.1118 - val_loss: 0.1434 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7051 - val_prc: 0.1232\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1338 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.7076 - prc: 0.1098 - val_loss: 0.1424 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7052 - val_prc: 0.1215\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1335 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7097 - prc: 0.1116 - val_loss: 0.1428 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7043 - val_prc: 0.1217\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7086 - prc: 0.1113 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7026 - val_prc: 0.1218\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1336 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.7089 - prc: 0.1112 - val_loss: 0.1437 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7026 - val_prc: 0.1212\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 5.2083e-04 - tn: 112952.0000 - auc: 0.7080 - prc: 0.1121 - val_loss: 0.1431 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7023 - val_prc: 0.1218\n"
     ]
    }
   ],
   "source": [
    "#split each training set into train/val, preserving 60/20/20 ratio, and fit the model\n",
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate auroc for each of the 5 folds\n",
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6799775889526353,\n",
       " 0.6981062743118449,\n",
       " 0.7158145366150336,\n",
       " 0.7079689529757069,\n",
       " 0.7306139953349061]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706 (0.683-0.73)\n"
     ]
    }
   ],
   "source": [
    "#find the mean auroc and 95% CI\n",
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print(round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(dX_train[0], dy_train[0])\n",
    "def rf_feat_importance(model, X):\n",
    "    return pd.DataFrame({'cols':X.columns, 'imp':model.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "fi = rf_feat_importance(model, X)\n",
    "fi[:10]\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fit(dX_train[0], dy_train[0])\n",
    "fi = rf_feat_importance(model2, X)\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'lrpreds' (list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#train the logistic regression model, max_iter set to max due to large dataset size, no validation set needed for LR (no early stopping)\n",
    "lrpreds = []\n",
    "model3 = LogisticRegression()\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))\n",
    "%store lrpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score the LR model on 5 folds\n",
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.631 (0.618-0.644)\n"
     ]
    }
   ],
   "source": [
    "#find the mean and 95% CI\n",
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.119 (0.101-0.136)\n",
      "Logistic Regression: 0.056 (0.051-0.06)\n"
     ]
    }
   ],
   "source": [
    "#write results to txt file\n",
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')\n",
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')\n",
    "with open('col_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_col_ann_tpr' (list)\n",
      "Stored 'mean_col_ann_fpr' (list)\n",
      "Stored 'mean_col_lr_tpr' (list)\n",
      "Stored 'mean_col_lr_fpr' (list)\n",
      "Stored 'mean_col_lr_rec' (list)\n",
      "Stored 'mean_col_lr_prec' (list)\n",
      "Stored 'mean_col_ann_rec' (list)\n",
      "Stored 'mean_col_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "#generate fpr and tpr to create ROC curves\n",
    "col_ann_tpr = []\n",
    "col_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_tpr.append(tpr)\n",
    "    col_ann_fpr.append(fpr)\n",
    "\n",
    "#sklearn's roc_curve function created fpr/tpr of different lengths (different number of thresholds)\n",
    "#averaging different length lists results in truncated ROC curves\n",
    "#randomly delete values to make each list the same length\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_tpr[x]))\n",
    "        col_ann_tpr[x] = np.delete(col_ann_tpr[x],ind)\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_fpr[x]))\n",
    "        col_ann_fpr[x] = np.delete(col_ann_fpr[x],ind)\n",
    "\n",
    "#find the mean tpr's and fpr's\n",
    "mean_col_ann_tpr = [np.mean(k) for k in zip(*col_ann_tpr)]\n",
    "mean_col_ann_fpr = [np.mean(k) for k in zip(*col_ann_fpr)]\n",
    "\n",
    "#save the means to use in the curves notebook\n",
    "%store mean_col_ann_tpr\n",
    "%store mean_col_ann_fpr\n",
    "\n",
    "#same process for LR\n",
    "col_lr_tpr = []\n",
    "col_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_tpr.append(tpr)\n",
    "    col_lr_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_tpr[x]))\n",
    "        col_lr_tpr[x] = np.delete(col_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_fpr[x]))\n",
    "        col_lr_fpr[x] = np.delete(col_lr_fpr[x],ind)\n",
    "\n",
    "mean_col_lr_tpr = [np.mean(k) for k in zip(*col_lr_tpr)]\n",
    "mean_col_lr_fpr = [np.mean(k) for k in zip(*col_lr_fpr)]\n",
    "%store mean_col_lr_tpr\n",
    "%store mean_col_lr_fpr\n",
    "\n",
    "#same process for precision-recall curves\n",
    "col_lr_rec = []\n",
    "col_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_rec.append(rec)\n",
    "    col_lr_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_rec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_rec[x]))\n",
    "        col_lr_rec[x] = np.delete(col_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_prec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_prec[x]))\n",
    "        col_lr_prec[x] = np.delete(col_lr_prec[x],ind)\n",
    "\n",
    "mean_col_lr_rec = [np.mean(k) for k in zip(*col_lr_rec)]\n",
    "\n",
    "mean_col_lr_prec = [np.mean(k) for k in zip(*col_lr_prec)]\n",
    "%store mean_col_lr_rec\n",
    "%store mean_col_lr_prec\n",
    "col_ann_rec = []\n",
    "col_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_rec.append(rec)\n",
    "    col_ann_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_rec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_rec[x]))\n",
    "        col_ann_rec[x] = np.delete(col_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_prec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_prec[x]))\n",
    "        col_ann_prec[x] = np.delete(col_ann_prec[x],ind)\n",
    "\n",
    "mean_col_ann_rec = [np.mean(k) for k in zip(*col_ann_rec)]\n",
    "\n",
    "mean_col_ann_prec = [np.mean(k) for k in zip(*col_ann_prec)]\n",
    "%store mean_col_ann_rec\n",
    "%store mean_col_ann_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import specificity_score\n",
    "thresh = np.arange(0, 1, 0.01)\n",
    "#calculate recall at 10 thresholds\n",
    "annrecall_list = []\n",
    "for i in thresh:\n",
    "    annrecall_list.append(recall_score(dy_test[1], annpreds[1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "annspec_list = []\n",
    "for i in thresh:\n",
    "    annspec_list.append(specificity_score(dy_test[1], annpreds[1] > i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, annrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, annspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "thresh = np.arange(0, 1, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculate recall at 10 thresholds\n",
    "lrrecall_list = []\n",
    "for i in thresh:\n",
    "    lrrecall_list.append(recall_score(dy_test[1], lrpreds[1][:,1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "lrspec_list = []\n",
    "for i in thresh:\n",
    "    lrspec_list.append(specificity_score(dy_test[1], lrpreds[1][:,1] > i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, lrrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, lrspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7089783281733746\n",
      "0.5000929886553841\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(recall_score(dy_test[1], lrpreds[1][:,1] > 0.028))\n",
    "print(specificity_score(dy_test[1], lrpreds[1][:,1] > 0.028))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7538699690402477\n",
      "0.5145726507080421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(recall_score(dy_test[1], annpreds[1] > 0.04))\n",
    "print(specificity_score(dy_test[1], annpreds[1] > 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6b501d5e1a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Two tailed test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_AB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "\n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from \n",
    "p = []\n",
    "z = []\n",
    "# Model A (random) vs. \"good\" model B\n",
    "for x in range(1,5):\n",
    "    preds_A = annpreds[x]\n",
    "    preds_B = lrpreds[x][:,1]\n",
    "    actual = dy_test[x]\n",
    "\n",
    "    actual = actual.array\n",
    "\n",
    "    def group_preds_by_label(preds, actual):\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "\n",
    "\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "    # Two tailed test\n",
    "    z.append(z_score(var_A, var_B, covar_AB, auc_A, auc_B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-27-6b501d5e1a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Two tailed test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_AB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "\n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for Delong test adapted from https://biasedml.com/roc-comparison/\n",
    "p = []\n",
    "z = []\n",
    "# Model A vs. Model B\n",
    "for x in range(1,5):\n",
    "    preds_A = annpreds[x]\n",
    "    preds_B = lrpreds[x][:,1]\n",
    "    actual = dy_test[x]\n",
    "\n",
    "    actual = actual.array\n",
    "\n",
    "    def group_preds_by_label(preds, actual):\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "\n",
    "\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "    # Two tailed test\n",
    "    z.append(z_score(var_A, var_B, covar_AB, auc_A, auc_B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    p.append(st.norm.sf(abs(z[x-1]))*2)\n",
    "    p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_ann_tpr = []\n",
    "col_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_tpr.append(tpr)\n",
    "    col_ann_fpr.append(fpr)\n",
    "col_ann_tpr_array = [np.array(x) for x in col_ann_tpr]\n",
    "mean_col_ann_tpr = [np.mean(k) for k in zip(*col_ann_tpr_array)]\n",
    "col_ann_fpr_array = [np.array(x) for x in col_ann_fpr]\n",
    "mean_col_ann_fpr = [np.mean(k) for k in zip(*col_ann_fpr_array)]\n",
    "%store mean_col_ann_tpr\n",
    "%store mean_col_ann_fpr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_lr_tpr = []\n",
    "col_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_tpr.append(tpr)\n",
    "    col_lr_fpr.append(fpr)\n",
    "col_lr_tpr_array = [np.array(x) for x in col_lr_tpr]\n",
    "mean_col_lr_tpr = [np.mean(k) for k in zip(*col_lr_tpr_array)]\n",
    "col_lr_fpr_array = [np.array(x) for x in col_lr_fpr]\n",
    "mean_col_lr_fpr = [np.mean(k) for k in zip(*col_lr_fpr_array)]\n",
    "%store mean_col_lr_tpr\n",
    "%store mean_col_lr_fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_ann_rec = []\n",
    "col_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_rec.append(rec)\n",
    "    col_ann_prec.append(prec)\n",
    "col_ann_rec_array = [np.array(x) for x in col_ann_rec]\n",
    "mean_col_ann_rec = [np.mean(k) for k in zip(*col_ann_rec_array)]\n",
    "col_ann_prec_array = [np.array(x) for x in col_ann_prec]\n",
    "mean_col_ann_prec = [np.mean(k) for k in zip(*col_ann_prec_array)]\n",
    "%store mean_col_ann_rec\n",
    "%store mean_col_ann_prec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_lr_rec = []\n",
    "col_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_rec.append(rec)\n",
    "    col_lr_prec.append(prec)\n",
    "col_lr_rec_array = [np.array(x) for x in col_lr_rec]\n",
    "mean_col_lr_rec = [np.mean(k) for k in zip(*col_lr_rec_array)]\n",
    "col_lr_prec_array = [np.array(x) for x in col_lr_prec]\n",
    "mean_col_lr_prec = [np.mean(k) for k in zip(*col_lr_prec_array)]\n",
    "%store mean_col_lr_rec\n",
    "%store mean_col_lr_prec\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
