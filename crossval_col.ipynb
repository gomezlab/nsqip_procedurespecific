{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "print(keras.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_col.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COL_STEROID</th>\n",
       "      <th>COL_MECH_BOWEL_PREP</th>\n",
       "      <th>COL_ORAL_ANTIBIOTIC</th>\n",
       "      <th>COL_CHEMO</th>\n",
       "      <th>COL_INDICATION</th>\n",
       "      <th>COL_ICD9_INDICATION</th>\n",
       "      <th>COL_EMERGENT</th>\n",
       "      <th>COL_ICD9_EMERGENT</th>\n",
       "      <th>COL_APPROACH</th>\n",
       "      <th>COL_ANASTOMOTIC</th>\n",
       "      <th>SEX</th>\n",
       "      <th>PRNCPTX</th>\n",
       "      <th>CPT</th>\n",
       "      <th>WORKRVU</th>\n",
       "      <th>INOUT</th>\n",
       "      <th>TRANST</th>\n",
       "      <th>AGE</th>\n",
       "      <th>OPERYR</th>\n",
       "      <th>ANESTHES</th>\n",
       "      <th>SURGSPEC</th>\n",
       "      <th>ELECTSURG</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>DYSPNEA</th>\n",
       "      <th>FNSTATUS2</th>\n",
       "      <th>VENTILAT</th>\n",
       "      <th>HXCOPD</th>\n",
       "      <th>ASCITES</th>\n",
       "      <th>HXCHF</th>\n",
       "      <th>HYPERMED</th>\n",
       "      <th>RENAFAIL</th>\n",
       "      <th>DIALYSIS</th>\n",
       "      <th>DISCANCR</th>\n",
       "      <th>WNDINF</th>\n",
       "      <th>STEROID</th>\n",
       "      <th>WTLOSS</th>\n",
       "      <th>BLEEDDIS</th>\n",
       "      <th>TRANSFUS</th>\n",
       "      <th>PRSEPIS</th>\n",
       "      <th>PRSODM</th>\n",
       "      <th>PRBUN</th>\n",
       "      <th>PRCREAT</th>\n",
       "      <th>PRWBC</th>\n",
       "      <th>PRHCT</th>\n",
       "      <th>PRPLATE</th>\n",
       "      <th>EMERGNCY</th>\n",
       "      <th>WNDCLAS</th>\n",
       "      <th>ASACLAS</th>\n",
       "      <th>OPTIME</th>\n",
       "      <th>HTOODAY</th>\n",
       "      <th>SSSIPATOS</th>\n",
       "      <th>DSSIPATOS</th>\n",
       "      <th>OSSIPATOS</th>\n",
       "      <th>PNAPATOS</th>\n",
       "      <th>VENTPATOS</th>\n",
       "      <th>UTIPATOS</th>\n",
       "      <th>SEPSISPATOS</th>\n",
       "      <th>SEPSHOCKPATOS</th>\n",
       "      <th>COL_ICD10_INDICATION</th>\n",
       "      <th>COL_ICD10_EMERGENT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>RACE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASEID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6551967</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.8542</td>\n",
       "      <td>97.975872</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>8.4034</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>7.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>28.497449</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552344</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>83.914520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43.4</td>\n",
       "      <td>373.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>31.754826</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552431</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6764</td>\n",
       "      <td>81.646560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>4.7</td>\n",
       "      <td>37.7</td>\n",
       "      <td>309.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>29.052438</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552941</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.4986</td>\n",
       "      <td>47.627160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>6.7</td>\n",
       "      <td>37.2</td>\n",
       "      <td>453.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>21.207195</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>68.945984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>7.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>26.090451</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         COL_STEROID  COL_MECH_BOWEL_PREP  COL_ORAL_ANTIBIOTIC  COL_CHEMO  \\\n",
       "CASEID                                                                      \n",
       "6551967            0                    2                    0          0   \n",
       "6552344            0                    1                    1          0   \n",
       "6552431            0                    2                    2          0   \n",
       "6552941            2                    0                    2          0   \n",
       "6552982            0                    0                    0          0   \n",
       "\n",
       "         COL_INDICATION  COL_ICD9_INDICATION  COL_EMERGENT  COL_ICD9_EMERGENT  \\\n",
       "CASEID                                                                          \n",
       "6551967               3                  451             6                155   \n",
       "6552344               8                  451             6                155   \n",
       "6552431               2                  451             6                155   \n",
       "6552941               5                  451             6                155   \n",
       "6552982               3                  451             6                155   \n",
       "\n",
       "         COL_APPROACH  COL_ANASTOMOTIC  SEX  PRNCPTX  CPT  WORKRVU  INOUT  \\\n",
       "CASEID                                                                      \n",
       "6551967             0                0    2        6    4        3      0   \n",
       "6552344             0                0    1        8    5        2      0   \n",
       "6552431             0                0    1        7    6        5      0   \n",
       "6552941             0                0    1        6    4        3      0   \n",
       "6552982             0                1    1        6    4        3      0   \n",
       "\n",
       "         TRANST   AGE  OPERYR  ANESTHES  SURGSPEC  ELECTSURG  HEIGHT  \\\n",
       "CASEID                                                                 \n",
       "6551967       1  64.0       5         1         1          2  1.8542   \n",
       "6552344       1  44.0       5         1         1          2  1.6256   \n",
       "6552431       1  30.0       5         1         1          2  1.6764   \n",
       "6552941       1  45.0       5         1         1          2  1.4986   \n",
       "6552982       1  50.0       5         1         1          2  1.6256   \n",
       "\n",
       "            WEIGHT  DIABETES  SMOKE  DYSPNEA  FNSTATUS2  VENTILAT  HXCOPD  \\\n",
       "CASEID                                                                      \n",
       "6551967  97.975872         2      0        2          0         0       0   \n",
       "6552344  83.914520         1      0        2          0         0       0   \n",
       "6552431  81.646560         1      0        2          0         0       0   \n",
       "6552941  47.627160         1      0        2          0         0       0   \n",
       "6552982  68.945984         1      0        2          0         0       0   \n",
       "\n",
       "         ASCITES  HXCHF  HYPERMED  RENAFAIL  DIALYSIS  DISCANCR  WNDINF  \\\n",
       "CASEID                                                                    \n",
       "6551967        0      0         2         0         0         0       0   \n",
       "6552344        0      0         2         0         0         0       0   \n",
       "6552431        0      0         0         0         0         0       0   \n",
       "6552941        0      0         0         0         0         0       0   \n",
       "6552982        0      0         0         0         0         0       0   \n",
       "\n",
       "         STEROID  WTLOSS  BLEEDDIS  TRANSFUS  PRSEPIS  PRSODM    PRBUN  \\\n",
       "CASEID                                                                   \n",
       "6551967        0       0         0         0        0   139.0   8.4034   \n",
       "6552344        0       0         0         0        0   139.0  14.0000   \n",
       "6552431        0       0         0         0        0   138.0   3.0000   \n",
       "6552941        2       0         0         0        0   141.0  13.0000   \n",
       "6552982        0       0         0         0        0   138.0  11.0000   \n",
       "\n",
       "         PRCREAT  PRWBC  PRHCT  PRPLATE  EMERGNCY  WNDCLAS  ASACLAS  OPTIME  \\\n",
       "CASEID                                                                        \n",
       "6551967   0.8032    7.1   38.0    190.0         0        1        2   121.0   \n",
       "6552344   0.8600   11.0   43.4    373.0         0        1        1    64.0   \n",
       "6552431   0.6000    4.7   37.7    309.0         0        2        1    99.0   \n",
       "6552941   0.5700    6.7   37.2    453.0         0        1        1    77.0   \n",
       "6552982   0.7300    7.3   29.7    432.0         0        1        1   134.0   \n",
       "\n",
       "         HTOODAY  SSSIPATOS  DSSIPATOS  OSSIPATOS  PNAPATOS  VENTPATOS  \\\n",
       "CASEID                                                                   \n",
       "6551967      0.0          0          0          0         0          0   \n",
       "6552344      0.0          0          0          0         0          0   \n",
       "6552431      0.0          0          0          0         0          0   \n",
       "6552941      0.0          0          0          0         0          0   \n",
       "6552982      0.0          0          0          0         0          0   \n",
       "\n",
       "         UTIPATOS  SEPSISPATOS  SEPSHOCKPATOS  COL_ICD10_INDICATION  \\\n",
       "CASEID                                                                \n",
       "6551967         0            0              0                   445   \n",
       "6552344         0            0              0                   445   \n",
       "6552431         0            0              0                   445   \n",
       "6552941         0            0              0                   445   \n",
       "6552982         0            0              0                   445   \n",
       "\n",
       "         COL_ICD10_EMERGENT        BMI  RACE  \n",
       "CASEID                                        \n",
       "6551967                 123  28.497449     5  \n",
       "6552344                 123  31.754826     6  \n",
       "6552431                 123  29.052438     6  \n",
       "6552941                 123  21.207195     6  \n",
       "6552982                 123  26.090451     5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.drop('CASEID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define outcome \"y\" and predictors \"X\"\n",
    "y = data['COL_ANASTOMOTIC']\n",
    "X = data.drop(['COL_ANASTOMOTIC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 5 folds of cross-validation, with 5 train/test sets, export to csv\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv's back into a dictionary, naming them train or test 1-5\n",
    "d = {}\n",
    "for x in range(0,5):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of training data for X and y\n",
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['COL_ANASTOMOTIC'], axis=1))\n",
    "        dy_train.append(d[x]['COL_ANASTOMOTIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of test data for X and y\n",
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['COL_ANASTOMOTIC'], axis=1))\n",
    "        dy_test.append(d[x]['COL_ANASTOMOTIC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfpreds = []\n",
    "xgbpreds = []\n",
    "model = RandomForestClassifier(n_estimators=1250, min_samples_split=2, min_samples_leaf=8, max_features='auto', max_depth=20, bootstrap=True)\n",
    "model2 = XGBClassifier(n_estimators=50, subsample=0.6, min_child_weight=10, max_depth=6, learning_rate=0.1, colsample_bytree=0.8)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model.fit(X, y)\n",
    "    model2.fit(X, y)\n",
    "    rfpreds.append(model.predict_proba(X_test))\n",
    "    xgbpreds.append(model2.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store rfpreds\n",
    "%store xgbpreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], rfpreds[x][:,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], xgbpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define keras model with 2 layers, 2000 nodes, followed by batch norm and dropout\n",
    "from tensorflow import keras\n",
    "input_shape = [X.shape[1]]\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "for _ in range(2):\n",
    "    model4.add(keras.layers.Dense(2000))\n",
    "    model4.add(keras.layers.BatchNormalization())\n",
    "    model4.add(keras.layers.Dropout(0.8))\n",
    "    model4.add(keras.layers.Activation(\"relu\"))\n",
    "model4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "metrics = [keras.metrics.Recall(name='Sensitivity'), keras.metrics.TrueNegatives(name='tn'), keras.metrics.AUC(name='auc'), keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=25,\n",
    "    min_delta=0.00001,\n",
    "    restore_best_weights=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save untrained model\n",
    "model4.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "229/229 [==============================] - 5s 15ms/step - loss: 0.2226 - Sensitivity: 0.0367 - tn: 110795.0000 - auc: 0.5280 - prc: 0.0403 - val_loss: 0.1899 - val_Sensitivity: 0.0055 - val_tn: 37639.0000 - val_auc: 0.6703 - val_prc: 0.0887\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1851 - Sensitivity: 0.0269 - tn: 112237.0000 - auc: 0.5598 - prc: 0.0488 - val_loss: 0.1785 - val_Sensitivity: 0.0970 - val_tn: 37367.0000 - val_auc: 0.6654 - val_prc: 0.0929\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1667 - Sensitivity: 0.0190 - tn: 112521.0000 - auc: 0.5806 - prc: 0.0550 - val_loss: 0.2030 - val_Sensitivity: 0.1088 - val_tn: 37179.0000 - val_auc: 0.6636 - val_prc: 0.0910\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1570 - Sensitivity: 0.0182 - tn: 112661.0000 - auc: 0.6023 - prc: 0.0622 - val_loss: 0.1997 - val_Sensitivity: 0.0994 - val_tn: 37313.0000 - val_auc: 0.6595 - val_prc: 0.0923\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1505 - Sensitivity: 0.0154 - tn: 112734.0000 - auc: 0.6197 - prc: 0.0689 - val_loss: 0.1837 - val_Sensitivity: 0.0860 - val_tn: 37418.0000 - val_auc: 0.6625 - val_prc: 0.0953\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1482 - Sensitivity: 0.0074 - tn: 112805.0000 - auc: 0.6213 - prc: 0.0730 - val_loss: 0.2001 - val_Sensitivity: 0.1136 - val_tn: 37128.0000 - val_auc: 0.6590 - val_prc: 0.0883\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1447 - Sensitivity: 0.0059 - tn: 112810.0000 - auc: 0.6425 - prc: 0.0769 - val_loss: 0.1930 - val_Sensitivity: 0.1025 - val_tn: 37298.0000 - val_auc: 0.6593 - val_prc: 0.0895\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1439 - Sensitivity: 0.0056 - tn: 112831.0000 - auc: 0.6406 - prc: 0.0816 - val_loss: 0.1815 - val_Sensitivity: 0.0978 - val_tn: 37345.0000 - val_auc: 0.6599 - val_prc: 0.0894\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1432 - Sensitivity: 0.0038 - tn: 112847.0000 - auc: 0.6463 - prc: 0.0790 - val_loss: 0.1873 - val_Sensitivity: 0.0994 - val_tn: 37342.0000 - val_auc: 0.6623 - val_prc: 0.0897\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1421 - Sensitivity: 0.0028 - tn: 112864.0000 - auc: 0.6537 - prc: 0.0825 - val_loss: 0.1816 - val_Sensitivity: 0.1002 - val_tn: 37324.0000 - val_auc: 0.6629 - val_prc: 0.0921\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1412 - Sensitivity: 0.0031 - tn: 112870.0000 - auc: 0.6554 - prc: 0.0891 - val_loss: 0.1904 - val_Sensitivity: 0.1041 - val_tn: 37294.0000 - val_auc: 0.6639 - val_prc: 0.0893\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1409 - Sensitivity: 0.0021 - tn: 112858.0000 - auc: 0.6631 - prc: 0.0864 - val_loss: 0.1774 - val_Sensitivity: 0.0962 - val_tn: 37381.0000 - val_auc: 0.6643 - val_prc: 0.0946\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1404 - Sensitivity: 0.0018 - tn: 112871.0000 - auc: 0.6666 - prc: 0.0911 - val_loss: 0.1714 - val_Sensitivity: 0.0489 - val_tn: 37527.0000 - val_auc: 0.6674 - val_prc: 0.0938\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1399 - Sensitivity: 0.0015 - tn: 112867.0000 - auc: 0.6682 - prc: 0.0940 - val_loss: 0.1672 - val_Sensitivity: 0.0276 - val_tn: 37590.0000 - val_auc: 0.6683 - val_prc: 0.0946\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1395 - Sensitivity: 0.0010 - tn: 112878.0000 - auc: 0.6747 - prc: 0.0923 - val_loss: 0.1687 - val_Sensitivity: 0.0852 - val_tn: 37405.0000 - val_auc: 0.6682 - val_prc: 0.0957\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1396 - Sensitivity: 0.0031 - tn: 112861.0000 - auc: 0.6721 - prc: 0.0927 - val_loss: 0.1660 - val_Sensitivity: 0.0860 - val_tn: 37403.0000 - val_auc: 0.6731 - val_prc: 0.0958\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1391 - Sensitivity: 0.0023 - tn: 112867.0000 - auc: 0.6740 - prc: 0.0941 - val_loss: 0.1638 - val_Sensitivity: 0.0371 - val_tn: 37562.0000 - val_auc: 0.6706 - val_prc: 0.0942\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1391 - Sensitivity: 0.0013 - tn: 112868.0000 - auc: 0.6775 - prc: 0.0944 - val_loss: 0.1663 - val_Sensitivity: 0.0521 - val_tn: 37520.0000 - val_auc: 0.6686 - val_prc: 0.0944\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1390 - Sensitivity: 0.0023 - tn: 112866.0000 - auc: 0.6788 - prc: 0.0958 - val_loss: 0.1630 - val_Sensitivity: 0.0505 - val_tn: 37517.0000 - val_auc: 0.6687 - val_prc: 0.0936\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1387 - Sensitivity: 0.0018 - tn: 112874.0000 - auc: 0.6774 - prc: 0.0981 - val_loss: 0.1637 - val_Sensitivity: 0.0812 - val_tn: 37423.0000 - val_auc: 0.6704 - val_prc: 0.0944\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 0.0031 - tn: 112864.0000 - auc: 0.6854 - prc: 0.0991 - val_loss: 0.1585 - val_Sensitivity: 0.0465 - val_tn: 37539.0000 - val_auc: 0.6732 - val_prc: 0.0961\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 0.0033 - tn: 112863.0000 - auc: 0.6870 - prc: 0.0985 - val_loss: 0.1578 - val_Sensitivity: 0.0134 - val_tn: 37628.0000 - val_auc: 0.6711 - val_prc: 0.0960\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 0.0023 - tn: 112879.0000 - auc: 0.6825 - prc: 0.0996 - val_loss: 0.1586 - val_Sensitivity: 0.0300 - val_tn: 37588.0000 - val_auc: 0.6757 - val_prc: 0.0976\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1378 - Sensitivity: 0.0018 - tn: 112880.0000 - auc: 0.6855 - prc: 0.1022 - val_loss: 0.1545 - val_Sensitivity: 0.0205 - val_tn: 37616.0000 - val_auc: 0.6745 - val_prc: 0.1003\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1381 - Sensitivity: 0.0023 - tn: 112872.0000 - auc: 0.6866 - prc: 0.1003 - val_loss: 0.1578 - val_Sensitivity: 0.0032 - val_tn: 37644.0000 - val_auc: 0.6727 - val_prc: 0.0976\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1379 - Sensitivity: 0.0033 - tn: 112879.0000 - auc: 0.6889 - prc: 0.1016 - val_loss: 0.1527 - val_Sensitivity: 7.8864e-04 - val_tn: 37659.0000 - val_auc: 0.6749 - val_prc: 0.1000\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1377 - Sensitivity: 0.0028 - tn: 112874.0000 - auc: 0.6895 - prc: 0.1046 - val_loss: 0.1556 - val_Sensitivity: 0.0032 - val_tn: 37655.0000 - val_auc: 0.6712 - val_prc: 0.0991\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1368 - Sensitivity: 0.0033 - tn: 112877.0000 - auc: 0.6959 - prc: 0.1100 - val_loss: 0.1503 - val_Sensitivity: 0.0024 - val_tn: 37653.0000 - val_auc: 0.6748 - val_prc: 0.0997\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1381 - Sensitivity: 0.0021 - tn: 112870.0000 - auc: 0.6876 - prc: 0.1022 - val_loss: 0.1517 - val_Sensitivity: 0.0276 - val_tn: 37602.0000 - val_auc: 0.6771 - val_prc: 0.1014\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1374 - Sensitivity: 0.0018 - tn: 112865.0000 - auc: 0.6922 - prc: 0.1029 - val_loss: 0.1542 - val_Sensitivity: 0.0252 - val_tn: 37606.0000 - val_auc: 0.6779 - val_prc: 0.1010\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1372 - Sensitivity: 0.0023 - tn: 112874.0000 - auc: 0.6925 - prc: 0.1066 - val_loss: 0.1530 - val_Sensitivity: 0.0315 - val_tn: 37592.0000 - val_auc: 0.6766 - val_prc: 0.0994\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1373 - Sensitivity: 0.0021 - tn: 112870.0000 - auc: 0.6935 - prc: 0.1035 - val_loss: 0.1506 - val_Sensitivity: 0.0126 - val_tn: 37632.0000 - val_auc: 0.6772 - val_prc: 0.0998\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1370 - Sensitivity: 0.0031 - tn: 112870.0000 - auc: 0.6928 - prc: 0.1058 - val_loss: 0.1477 - val_Sensitivity: 7.8864e-04 - val_tn: 37662.0000 - val_auc: 0.6752 - val_prc: 0.1004\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1365 - Sensitivity: 0.0033 - tn: 112872.0000 - auc: 0.7005 - prc: 0.1077 - val_loss: 0.1502 - val_Sensitivity: 0.0237 - val_tn: 37608.0000 - val_auc: 0.6782 - val_prc: 0.1019\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1368 - Sensitivity: 0.0018 - tn: 112878.0000 - auc: 0.6947 - prc: 0.1089 - val_loss: 0.1497 - val_Sensitivity: 0.0647 - val_tn: 37505.0000 - val_auc: 0.6762 - val_prc: 0.1015\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1369 - Sensitivity: 0.0028 - tn: 112866.0000 - auc: 0.6950 - prc: 0.1057 - val_loss: 0.1503 - val_Sensitivity: 0.0047 - val_tn: 37650.0000 - val_auc: 0.6744 - val_prc: 0.1006\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 0.0021 - tn: 112873.0000 - auc: 0.7028 - prc: 0.1101 - val_loss: 0.1485 - val_Sensitivity: 7.8864e-04 - val_tn: 37660.0000 - val_auc: 0.6764 - val_prc: 0.1007\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1363 - Sensitivity: 0.0015 - tn: 112878.0000 - auc: 0.7034 - prc: 0.1083 - val_loss: 0.1500 - val_Sensitivity: 0.0047 - val_tn: 37652.0000 - val_auc: 0.6743 - val_prc: 0.0993\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1366 - Sensitivity: 2.5648e-04 - tn: 112885.0000 - auc: 0.7012 - prc: 0.1069 - val_loss: 0.1488 - val_Sensitivity: 7.8864e-04 - val_tn: 37651.0000 - val_auc: 0.6767 - val_prc: 0.0989\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1361 - Sensitivity: 0.0015 - tn: 112881.0000 - auc: 0.7026 - prc: 0.1095 - val_loss: 0.1477 - val_Sensitivity: 0.0024 - val_tn: 37653.0000 - val_auc: 0.6790 - val_prc: 0.0992\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 0.0013 - tn: 112877.0000 - auc: 0.6985 - prc: 0.1065 - val_loss: 0.1491 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6760 - val_prc: 0.0988\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 0.0015 - tn: 112879.0000 - auc: 0.7025 - prc: 0.1115 - val_loss: 0.1474 - val_Sensitivity: 0.0039 - val_tn: 37649.0000 - val_auc: 0.6791 - val_prc: 0.0995\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0015 - tn: 112877.0000 - auc: 0.7019 - prc: 0.1096 - val_loss: 0.1480 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6750 - val_prc: 0.0974\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1356 - Sensitivity: 0.0015 - tn: 112880.0000 - auc: 0.7087 - prc: 0.1128 - val_loss: 0.1465 - val_Sensitivity: 0.0024 - val_tn: 37656.0000 - val_auc: 0.6733 - val_prc: 0.0998\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1353 - Sensitivity: 0.0015 - tn: 112880.0000 - auc: 0.7122 - prc: 0.1140 - val_loss: 0.1462 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6737 - val_prc: 0.1028\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1360 - Sensitivity: 0.0015 - tn: 112884.0000 - auc: 0.7049 - prc: 0.1123 - val_loss: 0.1469 - val_Sensitivity: 0.0024 - val_tn: 37654.0000 - val_auc: 0.6769 - val_prc: 0.1005\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1360 - Sensitivity: 0.0013 - tn: 112876.0000 - auc: 0.7053 - prc: 0.1126 - val_loss: 0.1469 - val_Sensitivity: 0.0039 - val_tn: 37652.0000 - val_auc: 0.6779 - val_prc: 0.1014\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 0.0023 - tn: 112869.0000 - auc: 0.7072 - prc: 0.1153 - val_loss: 0.1511 - val_Sensitivity: 0.0071 - val_tn: 37643.0000 - val_auc: 0.6757 - val_prc: 0.1010\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1351 - Sensitivity: 0.0023 - tn: 112876.0000 - auc: 0.7131 - prc: 0.1175 - val_loss: 0.1444 - val_Sensitivity: 0.0039 - val_tn: 37649.0000 - val_auc: 0.6751 - val_prc: 0.1005\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0023 - tn: 112873.0000 - auc: 0.7105 - prc: 0.1153 - val_loss: 0.1452 - val_Sensitivity: 0.0032 - val_tn: 37652.0000 - val_auc: 0.6735 - val_prc: 0.0993\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1349 - Sensitivity: 0.0038 - tn: 112870.0000 - auc: 0.7142 - prc: 0.1162 - val_loss: 0.1454 - val_Sensitivity: 0.0221 - val_tn: 37615.0000 - val_auc: 0.6797 - val_prc: 0.1007\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1356 - Sensitivity: 0.0015 - tn: 112881.0000 - auc: 0.7080 - prc: 0.1153 - val_loss: 0.1457 - val_Sensitivity: 0.0024 - val_tn: 37655.0000 - val_auc: 0.6780 - val_prc: 0.1001\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0031 - tn: 112878.0000 - auc: 0.7095 - prc: 0.1184 - val_loss: 0.1449 - val_Sensitivity: 0.0095 - val_tn: 37638.0000 - val_auc: 0.6779 - val_prc: 0.1011\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0044 - tn: 112872.0000 - auc: 0.7149 - prc: 0.1172 - val_loss: 0.1431 - val_Sensitivity: 0.0016 - val_tn: 37661.0000 - val_auc: 0.6720 - val_prc: 0.1011\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0046 - tn: 112873.0000 - auc: 0.7138 - prc: 0.1203 - val_loss: 0.1461 - val_Sensitivity: 0.0142 - val_tn: 37629.0000 - val_auc: 0.6789 - val_prc: 0.1004\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0031 - tn: 112869.0000 - auc: 0.7119 - prc: 0.1200 - val_loss: 0.1444 - val_Sensitivity: 0.0016 - val_tn: 37660.0000 - val_auc: 0.6775 - val_prc: 0.1005\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1347 - Sensitivity: 0.0041 - tn: 112873.0000 - auc: 0.7134 - prc: 0.1225 - val_loss: 0.1436 - val_Sensitivity: 0.0055 - val_tn: 37643.0000 - val_auc: 0.6727 - val_prc: 0.0988\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1346 - Sensitivity: 0.0028 - tn: 112879.0000 - auc: 0.7185 - prc: 0.1196 - val_loss: 0.1463 - val_Sensitivity: 0.0047 - val_tn: 37647.0000 - val_auc: 0.6739 - val_prc: 0.0994\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1343 - Sensitivity: 0.0031 - tn: 112880.0000 - auc: 0.7174 - prc: 0.1213 - val_loss: 0.1448 - val_Sensitivity: 0.0032 - val_tn: 37655.0000 - val_auc: 0.6778 - val_prc: 0.1016\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 0.0015 - tn: 112881.0000 - auc: 0.7150 - prc: 0.1179 - val_loss: 0.1429 - val_Sensitivity: 0.0016 - val_tn: 37660.0000 - val_auc: 0.6764 - val_prc: 0.1009\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0013 - tn: 112883.0000 - auc: 0.7207 - prc: 0.1222 - val_loss: 0.1475 - val_Sensitivity: 0.0260 - val_tn: 37603.0000 - val_auc: 0.6779 - val_prc: 0.0996\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0036 - tn: 112870.0000 - auc: 0.7206 - prc: 0.1232 - val_loss: 0.1460 - val_Sensitivity: 0.0055 - val_tn: 37649.0000 - val_auc: 0.6752 - val_prc: 0.0994\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1341 - Sensitivity: 0.0021 - tn: 112876.0000 - auc: 0.7212 - prc: 0.1190 - val_loss: 0.1461 - val_Sensitivity: 0.0016 - val_tn: 37655.0000 - val_auc: 0.6754 - val_prc: 0.0996\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1340 - Sensitivity: 0.0033 - tn: 112878.0000 - auc: 0.7220 - prc: 0.1242 - val_loss: 0.1445 - val_Sensitivity: 0.0032 - val_tn: 37650.0000 - val_auc: 0.6770 - val_prc: 0.0996\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1344 - Sensitivity: 0.0036 - tn: 112877.0000 - auc: 0.7177 - prc: 0.1245 - val_loss: 0.1447 - val_Sensitivity: 0.0032 - val_tn: 37656.0000 - val_auc: 0.6753 - val_prc: 0.0989\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1341 - Sensitivity: 0.0023 - tn: 112881.0000 - auc: 0.7208 - prc: 0.1227 - val_loss: 0.1460 - val_Sensitivity: 0.0024 - val_tn: 37654.0000 - val_auc: 0.6745 - val_prc: 0.0992\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0028 - tn: 112877.0000 - auc: 0.7206 - prc: 0.1228 - val_loss: 0.1469 - val_Sensitivity: 0.0047 - val_tn: 37643.0000 - val_auc: 0.6743 - val_prc: 0.0978\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0018 - tn: 112882.0000 - auc: 0.7234 - prc: 0.1255 - val_loss: 0.1456 - val_Sensitivity: 0.0016 - val_tn: 37657.0000 - val_auc: 0.6719 - val_prc: 0.0985\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0028 - tn: 112881.0000 - auc: 0.7236 - prc: 0.1255 - val_loss: 0.1432 - val_Sensitivity: 0.0024 - val_tn: 37652.0000 - val_auc: 0.6749 - val_prc: 0.0984\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0010 - tn: 112875.0000 - auc: 0.7206 - prc: 0.1206 - val_loss: 0.1452 - val_Sensitivity: 0.0032 - val_tn: 37650.0000 - val_auc: 0.6740 - val_prc: 0.0993\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1338 - Sensitivity: 0.0023 - tn: 112875.0000 - auc: 0.7263 - prc: 0.1247 - val_loss: 0.1433 - val_Sensitivity: 0.0032 - val_tn: 37651.0000 - val_auc: 0.6751 - val_prc: 0.0997\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1337 - Sensitivity: 0.0028 - tn: 112881.0000 - auc: 0.7228 - prc: 0.1279 - val_loss: 0.1455 - val_Sensitivity: 0.0047 - val_tn: 37646.0000 - val_auc: 0.6751 - val_prc: 0.0995\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1332 - Sensitivity: 0.0049 - tn: 112879.0000 - auc: 0.7267 - prc: 0.1331 - val_loss: 0.1430 - val_Sensitivity: 0.0032 - val_tn: 37647.0000 - val_auc: 0.6748 - val_prc: 0.0997\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1328 - Sensitivity: 0.0077 - tn: 112876.0000 - auc: 0.7301 - prc: 0.1324 - val_loss: 0.1441 - val_Sensitivity: 0.0055 - val_tn: 37644.0000 - val_auc: 0.6737 - val_prc: 0.1000\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1328 - Sensitivity: 0.0051 - tn: 112864.0000 - auc: 0.7298 - prc: 0.1315 - val_loss: 0.1423 - val_Sensitivity: 0.0039 - val_tn: 37649.0000 - val_auc: 0.6777 - val_prc: 0.1008\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1333 - Sensitivity: 0.0033 - tn: 112865.0000 - auc: 0.7279 - prc: 0.1261 - val_loss: 0.1471 - val_Sensitivity: 0.0103 - val_tn: 37638.0000 - val_auc: 0.6733 - val_prc: 0.0995\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1332 - Sensitivity: 0.0046 - tn: 112883.0000 - auc: 0.7260 - prc: 0.1335 - val_loss: 0.1448 - val_Sensitivity: 0.0166 - val_tn: 37623.0000 - val_auc: 0.6730 - val_prc: 0.0982\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 0.0031 - tn: 112858.0000 - auc: 0.7253 - prc: 0.1250 - val_loss: 0.1437 - val_Sensitivity: 0.0024 - val_tn: 37653.0000 - val_auc: 0.6752 - val_prc: 0.0997\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1332 - Sensitivity: 0.0038 - tn: 112883.0000 - auc: 0.7275 - prc: 0.1300 - val_loss: 0.1443 - val_Sensitivity: 0.0103 - val_tn: 37633.0000 - val_auc: 0.6755 - val_prc: 0.0995\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1330 - Sensitivity: 0.0051 - tn: 112871.0000 - auc: 0.7322 - prc: 0.1318 - val_loss: 0.1437 - val_Sensitivity: 0.0024 - val_tn: 37654.0000 - val_auc: 0.6749 - val_prc: 0.0992\n",
      "Epoch 81/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1329 - Sensitivity: 0.0036 - tn: 112880.0000 - auc: 0.7300 - prc: 0.1312 - val_loss: 0.1466 - val_Sensitivity: 0.0197 - val_tn: 37610.0000 - val_auc: 0.6755 - val_prc: 0.0990\n",
      "Epoch 82/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1324 - Sensitivity: 0.0064 - tn: 112870.0000 - auc: 0.7352 - prc: 0.1345 - val_loss: 0.1448 - val_Sensitivity: 0.0110 - val_tn: 37631.0000 - val_auc: 0.6766 - val_prc: 0.0989\n",
      "Epoch 83/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1329 - Sensitivity: 0.0038 - tn: 112874.0000 - auc: 0.7319 - prc: 0.1283 - val_loss: 0.1446 - val_Sensitivity: 0.0110 - val_tn: 37627.0000 - val_auc: 0.6766 - val_prc: 0.0991\n",
      "Epoch 84/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1323 - Sensitivity: 0.0080 - tn: 112865.0000 - auc: 0.7353 - prc: 0.1345 - val_loss: 0.1437 - val_Sensitivity: 0.0087 - val_tn: 37641.0000 - val_auc: 0.6751 - val_prc: 0.0987\n",
      "Epoch 85/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1324 - Sensitivity: 0.0023 - tn: 112870.0000 - auc: 0.7341 - prc: 0.1327 - val_loss: 0.1448 - val_Sensitivity: 0.0032 - val_tn: 37653.0000 - val_auc: 0.6731 - val_prc: 0.0991\n",
      "Epoch 86/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1326 - Sensitivity: 0.0036 - tn: 112877.0000 - auc: 0.7310 - prc: 0.1323 - val_loss: 0.1447 - val_Sensitivity: 0.0024 - val_tn: 37653.0000 - val_auc: 0.6741 - val_prc: 0.0985\n",
      "Epoch 87/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1323 - Sensitivity: 0.0044 - tn: 112882.0000 - auc: 0.7337 - prc: 0.1375 - val_loss: 0.1460 - val_Sensitivity: 0.0134 - val_tn: 37625.0000 - val_auc: 0.6732 - val_prc: 0.0986\n",
      "Epoch 88/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1327 - Sensitivity: 0.0023 - tn: 112882.0000 - auc: 0.7320 - prc: 0.1289 - val_loss: 0.1443 - val_Sensitivity: 0.0134 - val_tn: 37627.0000 - val_auc: 0.6736 - val_prc: 0.0989\n",
      "Epoch 89/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1323 - Sensitivity: 0.0062 - tn: 112872.0000 - auc: 0.7354 - prc: 0.1359 - val_loss: 0.1437 - val_Sensitivity: 0.0181 - val_tn: 37625.0000 - val_auc: 0.6749 - val_prc: 0.0997\n",
      "Epoch 90/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1319 - Sensitivity: 0.0041 - tn: 112866.0000 - auc: 0.7381 - prc: 0.1350 - val_loss: 0.1429 - val_Sensitivity: 0.0095 - val_tn: 37637.0000 - val_auc: 0.6757 - val_prc: 0.0995\n",
      "Epoch 91/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1325 - Sensitivity: 0.0044 - tn: 112874.0000 - auc: 0.7339 - prc: 0.1325 - val_loss: 0.1443 - val_Sensitivity: 0.0071 - val_tn: 37640.0000 - val_auc: 0.6771 - val_prc: 0.0995\n",
      "Epoch 92/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1322 - Sensitivity: 0.0044 - tn: 112872.0000 - auc: 0.7370 - prc: 0.1336 - val_loss: 0.1456 - val_Sensitivity: 0.0032 - val_tn: 37654.0000 - val_auc: 0.6723 - val_prc: 0.0989\n",
      "Epoch 93/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1313 - Sensitivity: 0.0054 - tn: 112872.0000 - auc: 0.7438 - prc: 0.1389 - val_loss: 0.1436 - val_Sensitivity: 0.0079 - val_tn: 37637.0000 - val_auc: 0.6723 - val_prc: 0.0978\n",
      "Epoch 94/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1324 - Sensitivity: 0.0041 - tn: 112867.0000 - auc: 0.7350 - prc: 0.1343 - val_loss: 0.1453 - val_Sensitivity: 0.0150 - val_tn: 37628.0000 - val_auc: 0.6731 - val_prc: 0.0984\n",
      "Epoch 95/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1320 - Sensitivity: 0.0077 - tn: 112870.0000 - auc: 0.7388 - prc: 0.1371 - val_loss: 0.1423 - val_Sensitivity: 0.0055 - val_tn: 37646.0000 - val_auc: 0.6743 - val_prc: 0.0984\n",
      "Epoch 96/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1319 - Sensitivity: 0.0082 - tn: 112861.0000 - auc: 0.7404 - prc: 0.1382 - val_loss: 0.1455 - val_Sensitivity: 0.0079 - val_tn: 37637.0000 - val_auc: 0.6707 - val_prc: 0.0975\n",
      "Epoch 97/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1324 - Sensitivity: 0.0041 - tn: 112869.0000 - auc: 0.7356 - prc: 0.1340 - val_loss: 0.1423 - val_Sensitivity: 0.0079 - val_tn: 37638.0000 - val_auc: 0.6739 - val_prc: 0.0974\n",
      "Epoch 98/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1315 - Sensitivity: 0.0054 - tn: 112872.0000 - auc: 0.7406 - prc: 0.1409 - val_loss: 0.1441 - val_Sensitivity: 0.0079 - val_tn: 37644.0000 - val_auc: 0.6717 - val_prc: 0.0987\n",
      "Epoch 99/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1320 - Sensitivity: 0.0062 - tn: 112864.0000 - auc: 0.7361 - prc: 0.1384 - val_loss: 0.1430 - val_Sensitivity: 0.0047 - val_tn: 37651.0000 - val_auc: 0.6718 - val_prc: 0.0988\n",
      "Epoch 100/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1315 - Sensitivity: 0.0049 - tn: 112877.0000 - auc: 0.7416 - prc: 0.1404 - val_loss: 0.1444 - val_Sensitivity: 0.0103 - val_tn: 37636.0000 - val_auc: 0.6740 - val_prc: 0.0977\n",
      "Epoch 101/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1317 - Sensitivity: 0.0036 - tn: 112867.0000 - auc: 0.7395 - prc: 0.1385 - val_loss: 0.1442 - val_Sensitivity: 0.0118 - val_tn: 37628.0000 - val_auc: 0.6736 - val_prc: 0.0994\n",
      "Epoch 102/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1317 - Sensitivity: 0.0092 - tn: 112865.0000 - auc: 0.7385 - prc: 0.1405 - val_loss: 0.1438 - val_Sensitivity: 0.0103 - val_tn: 37641.0000 - val_auc: 0.6745 - val_prc: 0.0986\n",
      "Epoch 103/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1312 - Sensitivity: 0.0077 - tn: 112867.0000 - auc: 0.7427 - prc: 0.1418 - val_loss: 0.1449 - val_Sensitivity: 0.0110 - val_tn: 37642.0000 - val_auc: 0.6733 - val_prc: 0.0987\n",
      "Epoch 104/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1312 - Sensitivity: 0.0064 - tn: 112865.0000 - auc: 0.7445 - prc: 0.1390 - val_loss: 0.1464 - val_Sensitivity: 0.0205 - val_tn: 37603.0000 - val_auc: 0.6717 - val_prc: 0.0955\n",
      "Epoch 105/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1314 - Sensitivity: 0.0049 - tn: 112873.0000 - auc: 0.7428 - prc: 0.1407 - val_loss: 0.1439 - val_Sensitivity: 0.0032 - val_tn: 37647.0000 - val_auc: 0.6741 - val_prc: 0.0986\n",
      "Epoch 106/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1314 - Sensitivity: 0.0038 - tn: 112865.0000 - auc: 0.7434 - prc: 0.1383 - val_loss: 0.1435 - val_Sensitivity: 0.0024 - val_tn: 37657.0000 - val_auc: 0.6716 - val_prc: 0.0976\n",
      "Epoch 107/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1312 - Sensitivity: 0.0059 - tn: 112865.0000 - auc: 0.7442 - prc: 0.1430 - val_loss: 0.1431 - val_Sensitivity: 0.0055 - val_tn: 37648.0000 - val_auc: 0.6733 - val_prc: 0.0994\n",
      "Epoch 108/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1312 - Sensitivity: 0.0044 - tn: 112875.0000 - auc: 0.7430 - prc: 0.1396 - val_loss: 0.1453 - val_Sensitivity: 0.0118 - val_tn: 37636.0000 - val_auc: 0.6733 - val_prc: 0.0978\n",
      "Epoch 109/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1308 - Sensitivity: 0.0051 - tn: 112876.0000 - auc: 0.7455 - prc: 0.1464 - val_loss: 0.1453 - val_Sensitivity: 0.0174 - val_tn: 37616.0000 - val_auc: 0.6748 - val_prc: 0.0995\n",
      "Epoch 110/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1312 - Sensitivity: 0.0077 - tn: 112863.0000 - auc: 0.7425 - prc: 0.1436 - val_loss: 0.1448 - val_Sensitivity: 0.0205 - val_tn: 37612.0000 - val_auc: 0.6741 - val_prc: 0.0988\n",
      "Epoch 111/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1308 - Sensitivity: 0.0097 - tn: 112869.0000 - auc: 0.7456 - prc: 0.1505 - val_loss: 0.1436 - val_Sensitivity: 0.0158 - val_tn: 37622.0000 - val_auc: 0.6717 - val_prc: 0.0985\n",
      "Epoch 112/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1307 - Sensitivity: 0.0087 - tn: 112860.0000 - auc: 0.7459 - prc: 0.1472 - val_loss: 0.1450 - val_Sensitivity: 0.0110 - val_tn: 37630.0000 - val_auc: 0.6717 - val_prc: 0.0989\n",
      "Epoch 113/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1304 - Sensitivity: 0.0067 - tn: 112872.0000 - auc: 0.7503 - prc: 0.1487 - val_loss: 0.1427 - val_Sensitivity: 0.0110 - val_tn: 37633.0000 - val_auc: 0.6738 - val_prc: 0.0992\n",
      "Epoch 114/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1306 - Sensitivity: 0.0082 - tn: 112865.0000 - auc: 0.7462 - prc: 0.1507 - val_loss: 0.1460 - val_Sensitivity: 0.0213 - val_tn: 37604.0000 - val_auc: 0.6731 - val_prc: 0.0983\n",
      "Epoch 115/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1315 - Sensitivity: 0.0095 - tn: 112864.0000 - auc: 0.7405 - prc: 0.1454 - val_loss: 0.1473 - val_Sensitivity: 0.0174 - val_tn: 37610.0000 - val_auc: 0.6697 - val_prc: 0.0978\n",
      "Epoch 116/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1307 - Sensitivity: 0.0100 - tn: 112847.0000 - auc: 0.7463 - prc: 0.1435 - val_loss: 0.1446 - val_Sensitivity: 0.0150 - val_tn: 37619.0000 - val_auc: 0.6712 - val_prc: 0.0983\n",
      "Epoch 117/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1308 - Sensitivity: 0.0069 - tn: 112860.0000 - auc: 0.7453 - prc: 0.1469 - val_loss: 0.1445 - val_Sensitivity: 0.0087 - val_tn: 37642.0000 - val_auc: 0.6691 - val_prc: 0.0982\n",
      "Epoch 118/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1307 - Sensitivity: 0.0069 - tn: 112876.0000 - auc: 0.7476 - prc: 0.1472 - val_loss: 0.1441 - val_Sensitivity: 0.0134 - val_tn: 37628.0000 - val_auc: 0.6716 - val_prc: 0.0978\n",
      "Epoch 119/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1297 - Sensitivity: 0.0095 - tn: 112864.0000 - auc: 0.7537 - prc: 0.1515 - val_loss: 0.1444 - val_Sensitivity: 0.0134 - val_tn: 37631.0000 - val_auc: 0.6728 - val_prc: 0.0985\n",
      "Epoch 120/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1301 - Sensitivity: 0.0090 - tn: 112861.0000 - auc: 0.7506 - prc: 0.1523 - val_loss: 0.1455 - val_Sensitivity: 0.0134 - val_tn: 37634.0000 - val_auc: 0.6686 - val_prc: 0.0975\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 5s 14ms/step - loss: 0.2246 - Sensitivity: 0.0427 - tn: 110698.0000 - auc: 0.5274 - prc: 0.0409 - val_loss: 0.1806 - val_Sensitivity: 0.0072 - val_tn: 37640.0000 - val_auc: 0.6611 - val_prc: 0.0814\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1833 - Sensitivity: 0.0266 - tn: 112249.0000 - auc: 0.5638 - prc: 0.0496 - val_loss: 0.1748 - val_Sensitivity: 0.0874 - val_tn: 37403.0000 - val_auc: 0.6636 - val_prc: 0.0936\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1676 - Sensitivity: 0.0246 - tn: 112471.0000 - auc: 0.5809 - prc: 0.0555 - val_loss: 0.1783 - val_Sensitivity: 0.0882 - val_tn: 37410.0000 - val_auc: 0.6615 - val_prc: 0.0956\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1574 - Sensitivity: 0.0171 - tn: 112640.0000 - auc: 0.6039 - prc: 0.0634 - val_loss: 0.2111 - val_Sensitivity: 0.1161 - val_tn: 37015.0000 - val_auc: 0.6561 - val_prc: 0.0871\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1508 - Sensitivity: 0.0141 - tn: 112728.0000 - auc: 0.6218 - prc: 0.0664 - val_loss: 0.2012 - val_Sensitivity: 0.0986 - val_tn: 37297.0000 - val_auc: 0.6592 - val_prc: 0.0925\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1473 - Sensitivity: 0.0115 - tn: 112758.0000 - auc: 0.6336 - prc: 0.0773 - val_loss: 0.1845 - val_Sensitivity: 0.1017 - val_tn: 37340.0000 - val_auc: 0.6600 - val_prc: 0.0931\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1448 - Sensitivity: 0.0059 - tn: 112802.0000 - auc: 0.6416 - prc: 0.0797 - val_loss: 0.1919 - val_Sensitivity: 0.1033 - val_tn: 37293.0000 - val_auc: 0.6601 - val_prc: 0.0910\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1433 - Sensitivity: 0.0074 - tn: 112817.0000 - auc: 0.6458 - prc: 0.0854 - val_loss: 0.1883 - val_Sensitivity: 0.1033 - val_tn: 37311.0000 - val_auc: 0.6601 - val_prc: 0.0900\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1420 - Sensitivity: 0.0020 - tn: 112834.0000 - auc: 0.6580 - prc: 0.0851 - val_loss: 0.1837 - val_Sensitivity: 0.0978 - val_tn: 37366.0000 - val_auc: 0.6611 - val_prc: 0.0931\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1421 - Sensitivity: 0.0031 - tn: 112845.0000 - auc: 0.6530 - prc: 0.0878 - val_loss: 0.1827 - val_Sensitivity: 0.1002 - val_tn: 37357.0000 - val_auc: 0.6629 - val_prc: 0.0941\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1409 - Sensitivity: 0.0036 - tn: 112842.0000 - auc: 0.6637 - prc: 0.0917 - val_loss: 0.1756 - val_Sensitivity: 0.0914 - val_tn: 37403.0000 - val_auc: 0.6651 - val_prc: 0.0932\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1405 - Sensitivity: 0.0046 - tn: 112852.0000 - auc: 0.6696 - prc: 0.0910 - val_loss: 0.1728 - val_Sensitivity: 0.0882 - val_tn: 37419.0000 - val_auc: 0.6646 - val_prc: 0.0934\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1405 - Sensitivity: 0.0028 - tn: 112853.0000 - auc: 0.6693 - prc: 0.0921 - val_loss: 0.1696 - val_Sensitivity: 0.0509 - val_tn: 37548.0000 - val_auc: 0.6644 - val_prc: 0.0921\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1398 - Sensitivity: 0.0036 - tn: 112861.0000 - auc: 0.6728 - prc: 0.0949 - val_loss: 0.1782 - val_Sensitivity: 0.0946 - val_tn: 37402.0000 - val_auc: 0.6657 - val_prc: 0.0960\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1390 - Sensitivity: 0.0028 - tn: 112858.0000 - auc: 0.6748 - prc: 0.0991 - val_loss: 0.1654 - val_Sensitivity: 0.0914 - val_tn: 37410.0000 - val_auc: 0.6682 - val_prc: 0.0963\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1398 - Sensitivity: 0.0023 - tn: 112861.0000 - auc: 0.6728 - prc: 0.0961 - val_loss: 0.1711 - val_Sensitivity: 0.0604 - val_tn: 37531.0000 - val_auc: 0.6669 - val_prc: 0.0972\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1389 - Sensitivity: 0.0026 - tn: 112852.0000 - auc: 0.6795 - prc: 0.0997 - val_loss: 0.1644 - val_Sensitivity: 0.0707 - val_tn: 37504.0000 - val_auc: 0.6687 - val_prc: 0.0981\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1383 - Sensitivity: 0.0018 - tn: 112863.0000 - auc: 0.6847 - prc: 0.1015 - val_loss: 0.1657 - val_Sensitivity: 0.0803 - val_tn: 37440.0000 - val_auc: 0.6692 - val_prc: 0.0956\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1384 - Sensitivity: 0.0046 - tn: 112860.0000 - auc: 0.6835 - prc: 0.1049 - val_loss: 0.1598 - val_Sensitivity: 0.0501 - val_tn: 37565.0000 - val_auc: 0.6716 - val_prc: 0.0996\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1381 - Sensitivity: 0.0015 - tn: 112863.0000 - auc: 0.6870 - prc: 0.1014 - val_loss: 0.1638 - val_Sensitivity: 0.0477 - val_tn: 37574.0000 - val_auc: 0.6695 - val_prc: 0.0970\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1379 - Sensitivity: 0.0031 - tn: 112866.0000 - auc: 0.6891 - prc: 0.1043 - val_loss: 0.1562 - val_Sensitivity: 0.0262 - val_tn: 37628.0000 - val_auc: 0.6699 - val_prc: 0.1006\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1382 - Sensitivity: 0.0036 - tn: 112854.0000 - auc: 0.6852 - prc: 0.1046 - val_loss: 0.1526 - val_Sensitivity: 0.0223 - val_tn: 37633.0000 - val_auc: 0.6718 - val_prc: 0.1019\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1377 - Sensitivity: 0.0038 - tn: 112850.0000 - auc: 0.6892 - prc: 0.1069 - val_loss: 0.1598 - val_Sensitivity: 0.0612 - val_tn: 37518.0000 - val_auc: 0.6696 - val_prc: 0.0981\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1381 - Sensitivity: 0.0018 - tn: 112858.0000 - auc: 0.6884 - prc: 0.1029 - val_loss: 0.1603 - val_Sensitivity: 0.0421 - val_tn: 37580.0000 - val_auc: 0.6713 - val_prc: 0.0985\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1377 - Sensitivity: 0.0043 - tn: 112858.0000 - auc: 0.6882 - prc: 0.1062 - val_loss: 0.1537 - val_Sensitivity: 0.0238 - val_tn: 37618.0000 - val_auc: 0.6723 - val_prc: 0.0986\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1380 - Sensitivity: 0.0026 - tn: 112859.0000 - auc: 0.6864 - prc: 0.1065 - val_loss: 0.1565 - val_Sensitivity: 0.0445 - val_tn: 37565.0000 - val_auc: 0.6720 - val_prc: 0.0994\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1376 - Sensitivity: 0.0026 - tn: 112860.0000 - auc: 0.6913 - prc: 0.1050 - val_loss: 0.1530 - val_Sensitivity: 0.0135 - val_tn: 37644.0000 - val_auc: 0.6718 - val_prc: 0.0999\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1372 - Sensitivity: 0.0038 - tn: 112860.0000 - auc: 0.6926 - prc: 0.1094 - val_loss: 0.1532 - val_Sensitivity: 0.0151 - val_tn: 37639.0000 - val_auc: 0.6724 - val_prc: 0.0970\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1373 - Sensitivity: 0.0026 - tn: 112855.0000 - auc: 0.6933 - prc: 0.1084 - val_loss: 0.1547 - val_Sensitivity: 0.0191 - val_tn: 37625.0000 - val_auc: 0.6731 - val_prc: 0.1013\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1372 - Sensitivity: 0.0018 - tn: 112866.0000 - auc: 0.6946 - prc: 0.1081 - val_loss: 0.1496 - val_Sensitivity: 0.0215 - val_tn: 37626.0000 - val_auc: 0.6765 - val_prc: 0.1033\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1365 - Sensitivity: 0.0038 - tn: 112857.0000 - auc: 0.7001 - prc: 0.1125 - val_loss: 0.1529 - val_Sensitivity: 0.0095 - val_tn: 37652.0000 - val_auc: 0.6741 - val_prc: 0.1008\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1370 - Sensitivity: 0.0046 - tn: 112858.0000 - auc: 0.6975 - prc: 0.1112 - val_loss: 0.1539 - val_Sensitivity: 0.0103 - val_tn: 37653.0000 - val_auc: 0.6733 - val_prc: 0.1011\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1363 - Sensitivity: 0.0033 - tn: 112855.0000 - auc: 0.7023 - prc: 0.1135 - val_loss: 0.1492 - val_Sensitivity: 0.0278 - val_tn: 37626.0000 - val_auc: 0.6741 - val_prc: 0.1020\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 0.0028 - tn: 112861.0000 - auc: 0.6975 - prc: 0.1117 - val_loss: 0.1499 - val_Sensitivity: 0.0207 - val_tn: 37635.0000 - val_auc: 0.6741 - val_prc: 0.0999\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1367 - Sensitivity: 0.0033 - tn: 112866.0000 - auc: 0.6981 - prc: 0.1136 - val_loss: 0.1463 - val_Sensitivity: 0.0087 - val_tn: 37653.0000 - val_auc: 0.6750 - val_prc: 0.1014\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 0.0028 - tn: 112864.0000 - auc: 0.7008 - prc: 0.1135 - val_loss: 0.1473 - val_Sensitivity: 0.0095 - val_tn: 37651.0000 - val_auc: 0.6747 - val_prc: 0.1004\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1359 - Sensitivity: 0.0026 - tn: 112863.0000 - auc: 0.7045 - prc: 0.1141 - val_loss: 0.1452 - val_Sensitivity: 0.0048 - val_tn: 37662.0000 - val_auc: 0.6765 - val_prc: 0.1014\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1365 - Sensitivity: 0.0020 - tn: 112866.0000 - auc: 0.7009 - prc: 0.1129 - val_loss: 0.1501 - val_Sensitivity: 0.0087 - val_tn: 37662.0000 - val_auc: 0.6740 - val_prc: 0.1016\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1359 - Sensitivity: 0.0023 - tn: 112867.0000 - auc: 0.7050 - prc: 0.1132 - val_loss: 0.1482 - val_Sensitivity: 0.0127 - val_tn: 37647.0000 - val_auc: 0.6735 - val_prc: 0.1013\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 0.0020 - tn: 112860.0000 - auc: 0.7018 - prc: 0.1119 - val_loss: 0.1462 - val_Sensitivity: 0.0079 - val_tn: 37662.0000 - val_auc: 0.6755 - val_prc: 0.1021\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1360 - Sensitivity: 0.0015 - tn: 112858.0000 - auc: 0.7072 - prc: 0.1164 - val_loss: 0.1472 - val_Sensitivity: 0.0032 - val_tn: 37666.0000 - val_auc: 0.6739 - val_prc: 0.1000\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0041 - tn: 112855.0000 - auc: 0.7020 - prc: 0.1141 - val_loss: 0.1468 - val_Sensitivity: 0.0151 - val_tn: 37646.0000 - val_auc: 0.6738 - val_prc: 0.0989\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0031 - tn: 112860.0000 - auc: 0.7106 - prc: 0.1208 - val_loss: 0.1464 - val_Sensitivity: 0.0095 - val_tn: 37656.0000 - val_auc: 0.6719 - val_prc: 0.0990\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 0.0031 - tn: 112867.0000 - auc: 0.7044 - prc: 0.1143 - val_loss: 0.1463 - val_Sensitivity: 0.0111 - val_tn: 37654.0000 - val_auc: 0.6742 - val_prc: 0.1009\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1358 - Sensitivity: 0.0018 - tn: 112853.0000 - auc: 0.7091 - prc: 0.1144 - val_loss: 0.1459 - val_Sensitivity: 0.0111 - val_tn: 37652.0000 - val_auc: 0.6718 - val_prc: 0.1002\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0038 - tn: 112868.0000 - auc: 0.7115 - prc: 0.1197 - val_loss: 0.1452 - val_Sensitivity: 0.0064 - val_tn: 37661.0000 - val_auc: 0.6742 - val_prc: 0.1006\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 0.0013 - tn: 112861.0000 - auc: 0.7090 - prc: 0.1157 - val_loss: 0.1445 - val_Sensitivity: 0.0119 - val_tn: 37650.0000 - val_auc: 0.6740 - val_prc: 0.1017\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1353 - Sensitivity: 0.0033 - tn: 112869.0000 - auc: 0.7124 - prc: 0.1194 - val_loss: 0.1518 - val_Sensitivity: 0.0167 - val_tn: 37646.0000 - val_auc: 0.6733 - val_prc: 0.0993\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0023 - tn: 112872.0000 - auc: 0.7092 - prc: 0.1194 - val_loss: 0.1459 - val_Sensitivity: 0.0135 - val_tn: 37649.0000 - val_auc: 0.6729 - val_prc: 0.0993\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0038 - tn: 112864.0000 - auc: 0.7113 - prc: 0.1216 - val_loss: 0.1452 - val_Sensitivity: 0.0040 - val_tn: 37659.0000 - val_auc: 0.6716 - val_prc: 0.0998\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 0.0023 - tn: 112868.0000 - auc: 0.7112 - prc: 0.1229 - val_loss: 0.1424 - val_Sensitivity: 0.0040 - val_tn: 37667.0000 - val_auc: 0.6723 - val_prc: 0.1007\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1343 - Sensitivity: 0.0043 - tn: 112863.0000 - auc: 0.7182 - prc: 0.1265 - val_loss: 0.1426 - val_Sensitivity: 0.0056 - val_tn: 37659.0000 - val_auc: 0.6742 - val_prc: 0.1003\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1346 - Sensitivity: 0.0046 - tn: 112859.0000 - auc: 0.7171 - prc: 0.1218 - val_loss: 0.1452 - val_Sensitivity: 0.0024 - val_tn: 37668.0000 - val_auc: 0.6729 - val_prc: 0.1002\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1349 - Sensitivity: 0.0018 - tn: 112862.0000 - auc: 0.7148 - prc: 0.1195 - val_loss: 0.1437 - val_Sensitivity: 0.0000e+00 - val_tn: 37671.0000 - val_auc: 0.6727 - val_prc: 0.0990\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1347 - Sensitivity: 0.0023 - tn: 112861.0000 - auc: 0.7142 - prc: 0.1222 - val_loss: 0.1457 - val_Sensitivity: 0.0000e+00 - val_tn: 37672.0000 - val_auc: 0.6735 - val_prc: 0.0996\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1344 - Sensitivity: 0.0026 - tn: 112861.0000 - auc: 0.7172 - prc: 0.1245 - val_loss: 0.1458 - val_Sensitivity: 0.0175 - val_tn: 37638.0000 - val_auc: 0.6732 - val_prc: 0.1002\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1345 - Sensitivity: 0.0046 - tn: 112858.0000 - auc: 0.7151 - prc: 0.1260 - val_loss: 0.1417 - val_Sensitivity: 0.0056 - val_tn: 37660.0000 - val_auc: 0.6713 - val_prc: 0.0987\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1341 - Sensitivity: 0.0064 - tn: 112852.0000 - auc: 0.7232 - prc: 0.1278 - val_loss: 0.1439 - val_Sensitivity: 0.0111 - val_tn: 37647.0000 - val_auc: 0.6717 - val_prc: 0.0981\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1341 - Sensitivity: 0.0020 - tn: 112865.0000 - auc: 0.7217 - prc: 0.1243 - val_loss: 0.1418 - val_Sensitivity: 0.0095 - val_tn: 37648.0000 - val_auc: 0.6718 - val_prc: 0.0985\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1344 - Sensitivity: 0.0059 - tn: 112853.0000 - auc: 0.7162 - prc: 0.1280 - val_loss: 0.1454 - val_Sensitivity: 0.0159 - val_tn: 37628.0000 - val_auc: 0.6735 - val_prc: 0.0984\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1341 - Sensitivity: 0.0043 - tn: 112861.0000 - auc: 0.7208 - prc: 0.1279 - val_loss: 0.1435 - val_Sensitivity: 0.0119 - val_tn: 37651.0000 - val_auc: 0.6719 - val_prc: 0.0985\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1331 - Sensitivity: 0.0043 - tn: 112866.0000 - auc: 0.7289 - prc: 0.1332 - val_loss: 0.1428 - val_Sensitivity: 0.0151 - val_tn: 37640.0000 - val_auc: 0.6738 - val_prc: 0.0986\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1338 - Sensitivity: 0.0038 - tn: 112862.0000 - auc: 0.7225 - prc: 0.1282 - val_loss: 0.1439 - val_Sensitivity: 0.0072 - val_tn: 37655.0000 - val_auc: 0.6720 - val_prc: 0.0992\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1340 - Sensitivity: 0.0041 - tn: 112869.0000 - auc: 0.7205 - prc: 0.1292 - val_loss: 0.1430 - val_Sensitivity: 0.0095 - val_tn: 37653.0000 - val_auc: 0.6718 - val_prc: 0.0983\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1335 - Sensitivity: 0.0036 - tn: 112863.0000 - auc: 0.7259 - prc: 0.1297 - val_loss: 0.1440 - val_Sensitivity: 0.0064 - val_tn: 37656.0000 - val_auc: 0.6717 - val_prc: 0.0998\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1332 - Sensitivity: 0.0036 - tn: 112865.0000 - auc: 0.7281 - prc: 0.1315 - val_loss: 0.1425 - val_Sensitivity: 0.0016 - val_tn: 37669.0000 - val_auc: 0.6736 - val_prc: 0.0992\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1338 - Sensitivity: 0.0054 - tn: 112862.0000 - auc: 0.7209 - prc: 0.1308 - val_loss: 0.1467 - val_Sensitivity: 0.0278 - val_tn: 37609.0000 - val_auc: 0.6706 - val_prc: 0.0974\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1335 - Sensitivity: 0.0049 - tn: 112851.0000 - auc: 0.7264 - prc: 0.1297 - val_loss: 0.1433 - val_Sensitivity: 0.0143 - val_tn: 37648.0000 - val_auc: 0.6737 - val_prc: 0.1001\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1335 - Sensitivity: 0.0077 - tn: 112862.0000 - auc: 0.7259 - prc: 0.1316 - val_loss: 0.1432 - val_Sensitivity: 0.0175 - val_tn: 37635.0000 - val_auc: 0.6691 - val_prc: 0.0990\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1339 - Sensitivity: 0.0038 - tn: 112863.0000 - auc: 0.7213 - prc: 0.1300 - val_loss: 0.1449 - val_Sensitivity: 0.0127 - val_tn: 37642.0000 - val_auc: 0.6719 - val_prc: 0.0990\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1335 - Sensitivity: 0.0059 - tn: 112851.0000 - auc: 0.7254 - prc: 0.1314 - val_loss: 0.1436 - val_Sensitivity: 0.0215 - val_tn: 37634.0000 - val_auc: 0.6714 - val_prc: 0.0998\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1329 - Sensitivity: 0.0072 - tn: 112849.0000 - auc: 0.7313 - prc: 0.1338 - val_loss: 0.1433 - val_Sensitivity: 0.0135 - val_tn: 37644.0000 - val_auc: 0.6746 - val_prc: 0.0996\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1334 - Sensitivity: 0.0036 - tn: 112861.0000 - auc: 0.7265 - prc: 0.1350 - val_loss: 0.1439 - val_Sensitivity: 0.0072 - val_tn: 37656.0000 - val_auc: 0.6715 - val_prc: 0.0990\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1327 - Sensitivity: 0.0051 - tn: 112862.0000 - auc: 0.7307 - prc: 0.1367 - val_loss: 0.1435 - val_Sensitivity: 0.0111 - val_tn: 37648.0000 - val_auc: 0.6728 - val_prc: 0.0984\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1330 - Sensitivity: 0.0049 - tn: 112863.0000 - auc: 0.7297 - prc: 0.1357 - val_loss: 0.1419 - val_Sensitivity: 0.0072 - val_tn: 37654.0000 - val_auc: 0.6721 - val_prc: 0.0975\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1332 - Sensitivity: 0.0056 - tn: 112858.0000 - auc: 0.7284 - prc: 0.1335 - val_loss: 0.1428 - val_Sensitivity: 0.0167 - val_tn: 37638.0000 - val_auc: 0.6699 - val_prc: 0.0997\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1335 - Sensitivity: 0.0061 - tn: 112858.0000 - auc: 0.7245 - prc: 0.1334 - val_loss: 0.1430 - val_Sensitivity: 0.0127 - val_tn: 37644.0000 - val_auc: 0.6714 - val_prc: 0.0997\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1326 - Sensitivity: 0.0059 - tn: 112857.0000 - auc: 0.7335 - prc: 0.1347 - val_loss: 0.1439 - val_Sensitivity: 0.0103 - val_tn: 37651.0000 - val_auc: 0.6726 - val_prc: 0.0999\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1325 - Sensitivity: 0.0059 - tn: 112856.0000 - auc: 0.7315 - prc: 0.1378 - val_loss: 0.1430 - val_Sensitivity: 0.0079 - val_tn: 37649.0000 - val_auc: 0.6714 - val_prc: 0.0983\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1325 - Sensitivity: 0.0074 - tn: 112849.0000 - auc: 0.7327 - prc: 0.1370 - val_loss: 0.1416 - val_Sensitivity: 0.0087 - val_tn: 37649.0000 - val_auc: 0.6705 - val_prc: 0.0980\n",
      "Epoch 81/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1327 - Sensitivity: 0.0082 - tn: 112850.0000 - auc: 0.7304 - prc: 0.1373 - val_loss: 0.1425 - val_Sensitivity: 0.0056 - val_tn: 37659.0000 - val_auc: 0.6695 - val_prc: 0.0973\n",
      "Epoch 82/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1326 - Sensitivity: 0.0084 - tn: 112853.0000 - auc: 0.7325 - prc: 0.1374 - val_loss: 0.1412 - val_Sensitivity: 0.0087 - val_tn: 37657.0000 - val_auc: 0.6690 - val_prc: 0.0989\n",
      "Epoch 83/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1325 - Sensitivity: 0.0084 - tn: 112839.0000 - auc: 0.7315 - prc: 0.1407 - val_loss: 0.1437 - val_Sensitivity: 0.0191 - val_tn: 37636.0000 - val_auc: 0.6704 - val_prc: 0.0994\n",
      "Epoch 84/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1325 - Sensitivity: 0.0067 - tn: 112862.0000 - auc: 0.7320 - prc: 0.1411 - val_loss: 0.1435 - val_Sensitivity: 0.0262 - val_tn: 37622.0000 - val_auc: 0.6700 - val_prc: 0.0994\n",
      "Epoch 85/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1323 - Sensitivity: 0.0115 - tn: 112857.0000 - auc: 0.7348 - prc: 0.1431 - val_loss: 0.1464 - val_Sensitivity: 0.0318 - val_tn: 37606.0000 - val_auc: 0.6696 - val_prc: 0.0987\n",
      "Epoch 86/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1319 - Sensitivity: 0.0110 - tn: 112849.0000 - auc: 0.7356 - prc: 0.1453 - val_loss: 0.1436 - val_Sensitivity: 0.0207 - val_tn: 37634.0000 - val_auc: 0.6696 - val_prc: 0.0984\n",
      "Epoch 87/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1321 - Sensitivity: 0.0079 - tn: 112848.0000 - auc: 0.7371 - prc: 0.1400 - val_loss: 0.1441 - val_Sensitivity: 0.0278 - val_tn: 37617.0000 - val_auc: 0.6698 - val_prc: 0.0983\n",
      "Epoch 88/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1319 - Sensitivity: 0.0090 - tn: 112839.0000 - auc: 0.7400 - prc: 0.1388 - val_loss: 0.1450 - val_Sensitivity: 0.0191 - val_tn: 37628.0000 - val_auc: 0.6695 - val_prc: 0.0978\n",
      "Epoch 89/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1319 - Sensitivity: 0.0067 - tn: 112847.0000 - auc: 0.7378 - prc: 0.1428 - val_loss: 0.1430 - val_Sensitivity: 0.0143 - val_tn: 37638.0000 - val_auc: 0.6703 - val_prc: 0.0990\n",
      "Epoch 90/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1323 - Sensitivity: 0.0056 - tn: 112856.0000 - auc: 0.7349 - prc: 0.1385 - val_loss: 0.1430 - val_Sensitivity: 0.0056 - val_tn: 37659.0000 - val_auc: 0.6712 - val_prc: 0.0983\n",
      "Epoch 91/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1315 - Sensitivity: 0.0110 - tn: 112857.0000 - auc: 0.7394 - prc: 0.1477 - val_loss: 0.1417 - val_Sensitivity: 0.0159 - val_tn: 37640.0000 - val_auc: 0.6698 - val_prc: 0.0994\n",
      "Epoch 92/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1323 - Sensitivity: 0.0059 - tn: 112851.0000 - auc: 0.7355 - prc: 0.1398 - val_loss: 0.1432 - val_Sensitivity: 0.0135 - val_tn: 37646.0000 - val_auc: 0.6700 - val_prc: 0.0992\n",
      "Epoch 93/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1321 - Sensitivity: 0.0090 - tn: 112849.0000 - auc: 0.7349 - prc: 0.1419 - val_loss: 0.1444 - val_Sensitivity: 0.0246 - val_tn: 37621.0000 - val_auc: 0.6694 - val_prc: 0.0978\n",
      "Epoch 94/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1315 - Sensitivity: 0.0100 - tn: 112858.0000 - auc: 0.7396 - prc: 0.1459 - val_loss: 0.1425 - val_Sensitivity: 0.0103 - val_tn: 37646.0000 - val_auc: 0.6667 - val_prc: 0.0974\n",
      "Epoch 95/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1312 - Sensitivity: 0.0100 - tn: 112853.0000 - auc: 0.7408 - prc: 0.1485 - val_loss: 0.1422 - val_Sensitivity: 0.0326 - val_tn: 37607.0000 - val_auc: 0.6699 - val_prc: 0.0976\n",
      "Epoch 96/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1314 - Sensitivity: 0.0090 - tn: 112848.0000 - auc: 0.7391 - prc: 0.1486 - val_loss: 0.1442 - val_Sensitivity: 0.0318 - val_tn: 37606.0000 - val_auc: 0.6687 - val_prc: 0.0975\n",
      "Epoch 97/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1320 - Sensitivity: 0.0095 - tn: 112845.0000 - auc: 0.7336 - prc: 0.1453 - val_loss: 0.1424 - val_Sensitivity: 0.0215 - val_tn: 37631.0000 - val_auc: 0.6687 - val_prc: 0.0974\n",
      "Epoch 98/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1312 - Sensitivity: 0.0133 - tn: 112844.0000 - auc: 0.7433 - prc: 0.1488 - val_loss: 0.1434 - val_Sensitivity: 0.0143 - val_tn: 37640.0000 - val_auc: 0.6678 - val_prc: 0.0969\n",
      "Epoch 99/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1316 - Sensitivity: 0.0090 - tn: 112857.0000 - auc: 0.7397 - prc: 0.1436 - val_loss: 0.1421 - val_Sensitivity: 0.0278 - val_tn: 37618.0000 - val_auc: 0.6688 - val_prc: 0.0984\n",
      "Epoch 100/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1314 - Sensitivity: 0.0128 - tn: 112835.0000 - auc: 0.7407 - prc: 0.1466 - val_loss: 0.1437 - val_Sensitivity: 0.0254 - val_tn: 37619.0000 - val_auc: 0.6686 - val_prc: 0.0980\n",
      "Epoch 101/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1312 - Sensitivity: 0.0102 - tn: 112847.0000 - auc: 0.7408 - prc: 0.1491 - val_loss: 0.1425 - val_Sensitivity: 0.0302 - val_tn: 37610.0000 - val_auc: 0.6689 - val_prc: 0.0984\n",
      "Epoch 102/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1309 - Sensitivity: 0.0092 - tn: 112845.0000 - auc: 0.7446 - prc: 0.1469 - val_loss: 0.1446 - val_Sensitivity: 0.0246 - val_tn: 37627.0000 - val_auc: 0.6674 - val_prc: 0.0976\n",
      "Epoch 103/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1310 - Sensitivity: 0.0084 - tn: 112851.0000 - auc: 0.7447 - prc: 0.1484 - val_loss: 0.1438 - val_Sensitivity: 0.0278 - val_tn: 37612.0000 - val_auc: 0.6662 - val_prc: 0.0975\n",
      "Epoch 104/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1312 - Sensitivity: 0.0092 - tn: 112843.0000 - auc: 0.7439 - prc: 0.1481 - val_loss: 0.1437 - val_Sensitivity: 0.0270 - val_tn: 37618.0000 - val_auc: 0.6684 - val_prc: 0.0981\n",
      "Epoch 105/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1309 - Sensitivity: 0.0110 - tn: 112857.0000 - auc: 0.7433 - prc: 0.1529 - val_loss: 0.1439 - val_Sensitivity: 0.0437 - val_tn: 37581.0000 - val_auc: 0.6689 - val_prc: 0.0983\n",
      "Epoch 106/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1309 - Sensitivity: 0.0113 - tn: 112843.0000 - auc: 0.7446 - prc: 0.1469 - val_loss: 0.1439 - val_Sensitivity: 0.0413 - val_tn: 37587.0000 - val_auc: 0.6694 - val_prc: 0.0969\n",
      "Epoch 107/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1306 - Sensitivity: 0.0133 - tn: 112826.0000 - auc: 0.7457 - prc: 0.1515 - val_loss: 0.1433 - val_Sensitivity: 0.0231 - val_tn: 37618.0000 - val_auc: 0.6672 - val_prc: 0.0974\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 5s 15ms/step - loss: 0.2255 - Sensitivity: 0.0421 - tn: 110713.0000 - auc: 0.5224 - prc: 0.0402 - val_loss: 0.3006 - val_Sensitivity: 0.0604 - val_tn: 37368.0000 - val_auc: 0.6498 - val_prc: 0.0764\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1844 - Sensitivity: 0.0250 - tn: 112246.0000 - auc: 0.5537 - prc: 0.0475 - val_loss: 0.1988 - val_Sensitivity: 0.1084 - val_tn: 37103.0000 - val_auc: 0.6694 - val_prc: 0.0892\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1677 - Sensitivity: 0.0255 - tn: 112514.0000 - auc: 0.5782 - prc: 0.0559 - val_loss: 0.1890 - val_Sensitivity: 0.0914 - val_tn: 37327.0000 - val_auc: 0.6721 - val_prc: 0.0918\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1577 - Sensitivity: 0.0157 - tn: 112690.0000 - auc: 0.5909 - prc: 0.0593 - val_loss: 0.1895 - val_Sensitivity: 0.0899 - val_tn: 37329.0000 - val_auc: 0.6730 - val_prc: 0.0951\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1514 - Sensitivity: 0.0111 - tn: 112775.0000 - auc: 0.6147 - prc: 0.0659 - val_loss: 0.1932 - val_Sensitivity: 0.0999 - val_tn: 37251.0000 - val_auc: 0.6685 - val_prc: 0.0940\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1476 - Sensitivity: 0.0062 - tn: 112812.0000 - auc: 0.6240 - prc: 0.0686 - val_loss: 0.1946 - val_Sensitivity: 0.0999 - val_tn: 37258.0000 - val_auc: 0.6704 - val_prc: 0.0912\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1447 - Sensitivity: 0.0064 - tn: 112853.0000 - auc: 0.6341 - prc: 0.0774 - val_loss: 0.2024 - val_Sensitivity: 0.1077 - val_tn: 37126.0000 - val_auc: 0.6676 - val_prc: 0.0875\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1426 - Sensitivity: 0.0067 - tn: 112868.0000 - auc: 0.6474 - prc: 0.0819 - val_loss: 0.1864 - val_Sensitivity: 0.1015 - val_tn: 37243.0000 - val_auc: 0.6715 - val_prc: 0.0900\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1424 - Sensitivity: 0.0046 - tn: 112886.0000 - auc: 0.6437 - prc: 0.0798 - val_loss: 0.1928 - val_Sensitivity: 0.0999 - val_tn: 37282.0000 - val_auc: 0.6699 - val_prc: 0.0922\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1420 - Sensitivity: 0.0013 - tn: 112889.0000 - auc: 0.6473 - prc: 0.0808 - val_loss: 0.1773 - val_Sensitivity: 0.0945 - val_tn: 37326.0000 - val_auc: 0.6711 - val_prc: 0.0900\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1413 - Sensitivity: 0.0023 - tn: 112893.0000 - auc: 0.6510 - prc: 0.0825 - val_loss: 0.1846 - val_Sensitivity: 0.0991 - val_tn: 37279.0000 - val_auc: 0.6703 - val_prc: 0.0890\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1402 - Sensitivity: 0.0023 - tn: 112892.0000 - auc: 0.6580 - prc: 0.0892 - val_loss: 0.1759 - val_Sensitivity: 0.0829 - val_tn: 37362.0000 - val_auc: 0.6747 - val_prc: 0.0933\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1403 - Sensitivity: 0.0013 - tn: 112893.0000 - auc: 0.6558 - prc: 0.0880 - val_loss: 0.1753 - val_Sensitivity: 0.0782 - val_tn: 37393.0000 - val_auc: 0.6757 - val_prc: 0.0967\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1398 - Sensitivity: 0.0031 - tn: 112895.0000 - auc: 0.6632 - prc: 0.0932 - val_loss: 0.1648 - val_Sensitivity: 0.0511 - val_tn: 37509.0000 - val_auc: 0.6744 - val_prc: 0.0964\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1394 - Sensitivity: 0.0041 - tn: 112890.0000 - auc: 0.6677 - prc: 0.0939 - val_loss: 0.1670 - val_Sensitivity: 0.0829 - val_tn: 37361.0000 - val_auc: 0.6764 - val_prc: 0.0940\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1391 - Sensitivity: 0.0039 - tn: 112885.0000 - auc: 0.6664 - prc: 0.0943 - val_loss: 0.1635 - val_Sensitivity: 0.0225 - val_tn: 37581.0000 - val_auc: 0.6761 - val_prc: 0.0959\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1388 - Sensitivity: 0.0018 - tn: 112891.0000 - auc: 0.6708 - prc: 0.0957 - val_loss: 0.1641 - val_Sensitivity: 0.0449 - val_tn: 37527.0000 - val_auc: 0.6777 - val_prc: 0.0970\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1381 - Sensitivity: 0.0021 - tn: 112900.0000 - auc: 0.6755 - prc: 0.0978 - val_loss: 0.1643 - val_Sensitivity: 0.0697 - val_tn: 37431.0000 - val_auc: 0.6764 - val_prc: 0.0960\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1385 - Sensitivity: 0.0036 - tn: 112899.0000 - auc: 0.6730 - prc: 0.0969 - val_loss: 0.1753 - val_Sensitivity: 0.1038 - val_tn: 37040.0000 - val_auc: 0.6736 - val_prc: 0.0794\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1386 - Sensitivity: 0.0028 - tn: 112887.0000 - auc: 0.6758 - prc: 0.0946 - val_loss: 0.1646 - val_Sensitivity: 0.0116 - val_tn: 37622.0000 - val_auc: 0.6801 - val_prc: 0.0968\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1378 - Sensitivity: 0.0026 - tn: 112900.0000 - auc: 0.6804 - prc: 0.0987 - val_loss: 0.1597 - val_Sensitivity: 0.0364 - val_tn: 37535.0000 - val_auc: 0.6772 - val_prc: 0.0951\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1383 - Sensitivity: 0.0015 - tn: 112897.0000 - auc: 0.6779 - prc: 0.0962 - val_loss: 0.1599 - val_Sensitivity: 0.0186 - val_tn: 37607.0000 - val_auc: 0.6820 - val_prc: 0.0962\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1382 - Sensitivity: 2.5800e-04 - tn: 112896.0000 - auc: 0.6777 - prc: 0.0957 - val_loss: 0.1621 - val_Sensitivity: 0.0139 - val_tn: 37617.0000 - val_auc: 0.6806 - val_prc: 0.0967\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1379 - Sensitivity: 7.7399e-04 - tn: 112896.0000 - auc: 0.6804 - prc: 0.0978 - val_loss: 0.1604 - val_Sensitivity: 0.0318 - val_tn: 37569.0000 - val_auc: 0.6779 - val_prc: 0.0964\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1369 - Sensitivity: 0.0015 - tn: 112902.0000 - auc: 0.6889 - prc: 0.1022 - val_loss: 0.1616 - val_Sensitivity: 0.0356 - val_tn: 37550.0000 - val_auc: 0.6737 - val_prc: 0.0923\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1367 - Sensitivity: 0.0015 - tn: 112898.0000 - auc: 0.6902 - prc: 0.1042 - val_loss: 0.1532 - val_Sensitivity: 0.0279 - val_tn: 37577.0000 - val_auc: 0.6820 - val_prc: 0.0990\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1375 - Sensitivity: 0.0021 - tn: 112898.0000 - auc: 0.6849 - prc: 0.1022 - val_loss: 0.1569 - val_Sensitivity: 0.0356 - val_tn: 37525.0000 - val_auc: 0.6797 - val_prc: 0.0923\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1365 - Sensitivity: 2.5800e-04 - tn: 112907.0000 - auc: 0.6929 - prc: 0.1032 - val_loss: 0.1538 - val_Sensitivity: 0.0225 - val_tn: 37573.0000 - val_auc: 0.6825 - val_prc: 0.0961\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1372 - Sensitivity: 0.0013 - tn: 112902.0000 - auc: 0.6866 - prc: 0.1026 - val_loss: 0.1509 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.6837 - val_prc: 0.0999\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1368 - Sensitivity: 0.0023 - tn: 112903.0000 - auc: 0.6877 - prc: 0.1057 - val_loss: 0.1508 - val_Sensitivity: 0.0046 - val_tn: 37630.0000 - val_auc: 0.6816 - val_prc: 0.0976\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1366 - Sensitivity: 0.0026 - tn: 112902.0000 - auc: 0.6919 - prc: 0.1042 - val_loss: 0.1544 - val_Sensitivity: 0.0023 - val_tn: 37636.0000 - val_auc: 0.6768 - val_prc: 0.0961\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1364 - Sensitivity: 7.7399e-04 - tn: 112904.0000 - auc: 0.6940 - prc: 0.1056 - val_loss: 0.1504 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.6811 - val_prc: 0.1009\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1364 - Sensitivity: 0.0021 - tn: 112901.0000 - auc: 0.6941 - prc: 0.1070 - val_loss: 0.1488 - val_Sensitivity: 0.0046 - val_tn: 37637.0000 - val_auc: 0.6820 - val_prc: 0.1012\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0010 - tn: 112907.0000 - auc: 0.6963 - prc: 0.1084 - val_loss: 0.1580 - val_Sensitivity: 0.0349 - val_tn: 37528.0000 - val_auc: 0.6758 - val_prc: 0.0918\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1363 - Sensitivity: 0.0039 - tn: 112891.0000 - auc: 0.6930 - prc: 0.1084 - val_loss: 0.1523 - val_Sensitivity: 0.0271 - val_tn: 37576.0000 - val_auc: 0.6828 - val_prc: 0.0995\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 0.0021 - tn: 112905.0000 - auc: 0.6929 - prc: 0.1061 - val_loss: 0.1561 - val_Sensitivity: 0.0550 - val_tn: 37473.0000 - val_auc: 0.6811 - val_prc: 0.0920\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 0.0013 - tn: 112899.0000 - auc: 0.6960 - prc: 0.1026 - val_loss: 0.1512 - val_Sensitivity: 0.0031 - val_tn: 37638.0000 - val_auc: 0.6838 - val_prc: 0.1010\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 0.0013 - tn: 112903.0000 - auc: 0.7018 - prc: 0.1088 - val_loss: 0.1488 - val_Sensitivity: 0.0015 - val_tn: 37640.0000 - val_auc: 0.6801 - val_prc: 0.1006\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 0.0023 - tn: 112906.0000 - auc: 0.7000 - prc: 0.1121 - val_loss: 0.1456 - val_Sensitivity: 7.7459e-04 - val_tn: 37640.0000 - val_auc: 0.6800 - val_prc: 0.1006\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 2.5800e-04 - tn: 112906.0000 - auc: 0.6967 - prc: 0.1075 - val_loss: 0.1463 - val_Sensitivity: 0.0031 - val_tn: 37639.0000 - val_auc: 0.6805 - val_prc: 0.1001\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1363 - Sensitivity: 0.0026 - tn: 112906.0000 - auc: 0.6937 - prc: 0.1090 - val_loss: 0.1525 - val_Sensitivity: 0.0031 - val_tn: 37638.0000 - val_auc: 0.6769 - val_prc: 0.0987\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1356 - Sensitivity: 0.0018 - tn: 112903.0000 - auc: 0.7002 - prc: 0.1112 - val_loss: 0.1465 - val_Sensitivity: 0.0132 - val_tn: 37616.0000 - val_auc: 0.6820 - val_prc: 0.0984\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1353 - Sensitivity: 0.0023 - tn: 112900.0000 - auc: 0.7018 - prc: 0.1142 - val_loss: 0.1490 - val_Sensitivity: 0.0077 - val_tn: 37623.0000 - val_auc: 0.6813 - val_prc: 0.0981\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 0.0021 - tn: 112901.0000 - auc: 0.7006 - prc: 0.1144 - val_loss: 0.1503 - val_Sensitivity: 7.7459e-04 - val_tn: 37640.0000 - val_auc: 0.6785 - val_prc: 0.1012\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 0.0013 - tn: 112903.0000 - auc: 0.7008 - prc: 0.1119 - val_loss: 0.1460 - val_Sensitivity: 0.0039 - val_tn: 37634.0000 - val_auc: 0.6813 - val_prc: 0.0983\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0015 - tn: 112899.0000 - auc: 0.7055 - prc: 0.1141 - val_loss: 0.1451 - val_Sensitivity: 0.0031 - val_tn: 37638.0000 - val_auc: 0.6809 - val_prc: 0.1001\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0028 - tn: 112898.0000 - auc: 0.7089 - prc: 0.1154 - val_loss: 0.1467 - val_Sensitivity: 0.0031 - val_tn: 37637.0000 - val_auc: 0.6818 - val_prc: 0.0993\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1345 - Sensitivity: 0.0021 - tn: 112908.0000 - auc: 0.7090 - prc: 0.1156 - val_loss: 0.1444 - val_Sensitivity: 0.0108 - val_tn: 37627.0000 - val_auc: 0.6823 - val_prc: 0.1011\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0021 - tn: 112898.0000 - auc: 0.7035 - prc: 0.1156 - val_loss: 0.1460 - val_Sensitivity: 0.0155 - val_tn: 37613.0000 - val_auc: 0.6818 - val_prc: 0.0997\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1347 - Sensitivity: 0.0021 - tn: 112900.0000 - auc: 0.7082 - prc: 0.1164 - val_loss: 0.1491 - val_Sensitivity: 0.0062 - val_tn: 37629.0000 - val_auc: 0.6778 - val_prc: 0.1001\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1348 - Sensitivity: 7.7399e-04 - tn: 112908.0000 - auc: 0.7051 - prc: 0.1188 - val_loss: 0.1471 - val_Sensitivity: 0.0116 - val_tn: 37627.0000 - val_auc: 0.6831 - val_prc: 0.1009\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1347 - Sensitivity: 0.0026 - tn: 112903.0000 - auc: 0.7102 - prc: 0.1165 - val_loss: 0.1504 - val_Sensitivity: 0.0302 - val_tn: 37556.0000 - val_auc: 0.6787 - val_prc: 0.0964\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1346 - Sensitivity: 0.0028 - tn: 112905.0000 - auc: 0.7067 - prc: 0.1186 - val_loss: 0.1478 - val_Sensitivity: 0.0062 - val_tn: 37631.0000 - val_auc: 0.6770 - val_prc: 0.0990\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1338 - Sensitivity: 0.0023 - tn: 112909.0000 - auc: 0.7162 - prc: 0.1218 - val_loss: 0.1474 - val_Sensitivity: 0.0046 - val_tn: 37631.0000 - val_auc: 0.6788 - val_prc: 0.0992\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1341 - Sensitivity: 0.0023 - tn: 112899.0000 - auc: 0.7117 - prc: 0.1207 - val_loss: 0.1511 - val_Sensitivity: 0.0101 - val_tn: 37625.0000 - val_auc: 0.6803 - val_prc: 0.0986\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1345 - Sensitivity: 0.0036 - tn: 112900.0000 - auc: 0.7087 - prc: 0.1205 - val_loss: 0.1464 - val_Sensitivity: 0.0046 - val_tn: 37635.0000 - val_auc: 0.6791 - val_prc: 0.0996\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1343 - Sensitivity: 0.0026 - tn: 112902.0000 - auc: 0.7096 - prc: 0.1203 - val_loss: 0.1476 - val_Sensitivity: 0.0194 - val_tn: 37595.0000 - val_auc: 0.6783 - val_prc: 0.0958\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1346 - Sensitivity: 0.0018 - tn: 112905.0000 - auc: 0.7085 - prc: 0.1192 - val_loss: 0.1479 - val_Sensitivity: 0.0054 - val_tn: 37630.0000 - val_auc: 0.6786 - val_prc: 0.0984\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1343 - Sensitivity: 0.0015 - tn: 112904.0000 - auc: 0.7113 - prc: 0.1203 - val_loss: 0.1446 - val_Sensitivity: 0.0093 - val_tn: 37625.0000 - val_auc: 0.6792 - val_prc: 0.0997\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1342 - Sensitivity: 0.0021 - tn: 112901.0000 - auc: 0.7122 - prc: 0.1196 - val_loss: 0.1456 - val_Sensitivity: 0.0108 - val_tn: 37616.0000 - val_auc: 0.6780 - val_prc: 0.0971\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1335 - Sensitivity: 0.0044 - tn: 112899.0000 - auc: 0.7182 - prc: 0.1234 - val_loss: 0.1463 - val_Sensitivity: 0.0124 - val_tn: 37621.0000 - val_auc: 0.6790 - val_prc: 0.0977\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1336 - Sensitivity: 0.0028 - tn: 112902.0000 - auc: 0.7152 - prc: 0.1228 - val_loss: 0.1462 - val_Sensitivity: 0.0093 - val_tn: 37630.0000 - val_auc: 0.6794 - val_prc: 0.0993\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1339 - Sensitivity: 0.0028 - tn: 112907.0000 - auc: 0.7151 - prc: 0.1219 - val_loss: 0.1489 - val_Sensitivity: 0.0108 - val_tn: 37620.0000 - val_auc: 0.6812 - val_prc: 0.0979\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1334 - Sensitivity: 0.0015 - tn: 112896.0000 - auc: 0.7191 - prc: 0.1228 - val_loss: 0.1458 - val_Sensitivity: 0.0054 - val_tn: 37629.0000 - val_auc: 0.6790 - val_prc: 0.0974\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1337 - Sensitivity: 0.0031 - tn: 112903.0000 - auc: 0.7162 - prc: 0.1247 - val_loss: 0.1453 - val_Sensitivity: 0.0039 - val_tn: 37632.0000 - val_auc: 0.6790 - val_prc: 0.1006\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1335 - Sensitivity: 0.0026 - tn: 112905.0000 - auc: 0.7168 - prc: 0.1269 - val_loss: 0.1486 - val_Sensitivity: 0.0178 - val_tn: 37614.0000 - val_auc: 0.6790 - val_prc: 0.0995\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1338 - Sensitivity: 0.0046 - tn: 112894.0000 - auc: 0.7158 - prc: 0.1232 - val_loss: 0.1450 - val_Sensitivity: 0.0062 - val_tn: 37632.0000 - val_auc: 0.6803 - val_prc: 0.1005\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1327 - Sensitivity: 0.0036 - tn: 112902.0000 - auc: 0.7270 - prc: 0.1267 - val_loss: 0.1441 - val_Sensitivity: 0.0116 - val_tn: 37619.0000 - val_auc: 0.6783 - val_prc: 0.0995\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1332 - Sensitivity: 0.0015 - tn: 112904.0000 - auc: 0.7201 - prc: 0.1268 - val_loss: 0.1437 - val_Sensitivity: 0.0031 - val_tn: 37635.0000 - val_auc: 0.6786 - val_prc: 0.0992\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1332 - Sensitivity: 0.0059 - tn: 112896.0000 - auc: 0.7187 - prc: 0.1296 - val_loss: 0.1451 - val_Sensitivity: 0.0170 - val_tn: 37612.0000 - val_auc: 0.6827 - val_prc: 0.0988\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1330 - Sensitivity: 0.0028 - tn: 112903.0000 - auc: 0.7232 - prc: 0.1300 - val_loss: 0.1445 - val_Sensitivity: 0.0163 - val_tn: 37611.0000 - val_auc: 0.6798 - val_prc: 0.0980\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1330 - Sensitivity: 0.0054 - tn: 112890.0000 - auc: 0.7245 - prc: 0.1282 - val_loss: 0.1466 - val_Sensitivity: 0.0170 - val_tn: 37604.0000 - val_auc: 0.6722 - val_prc: 0.0964\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1331 - Sensitivity: 0.0039 - tn: 112902.0000 - auc: 0.7200 - prc: 0.1292 - val_loss: 0.1462 - val_Sensitivity: 0.0155 - val_tn: 37607.0000 - val_auc: 0.6761 - val_prc: 0.0978\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1330 - Sensitivity: 0.0031 - tn: 112895.0000 - auc: 0.7229 - prc: 0.1263 - val_loss: 0.1442 - val_Sensitivity: 0.0015 - val_tn: 37639.0000 - val_auc: 0.6785 - val_prc: 0.0996\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1328 - Sensitivity: 0.0028 - tn: 112906.0000 - auc: 0.7245 - prc: 0.1287 - val_loss: 0.1438 - val_Sensitivity: 0.0085 - val_tn: 37627.0000 - val_auc: 0.6780 - val_prc: 0.0992\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1325 - Sensitivity: 0.0044 - tn: 112895.0000 - auc: 0.7246 - prc: 0.1293 - val_loss: 0.1466 - val_Sensitivity: 0.0101 - val_tn: 37624.0000 - val_auc: 0.6807 - val_prc: 0.0985\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1326 - Sensitivity: 0.0034 - tn: 112902.0000 - auc: 0.7268 - prc: 0.1290 - val_loss: 0.1458 - val_Sensitivity: 0.0085 - val_tn: 37627.0000 - val_auc: 0.6738 - val_prc: 0.0961\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1324 - Sensitivity: 0.0039 - tn: 112902.0000 - auc: 0.7269 - prc: 0.1286 - val_loss: 0.1440 - val_Sensitivity: 0.0132 - val_tn: 37616.0000 - val_auc: 0.6766 - val_prc: 0.0965\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1322 - Sensitivity: 0.0034 - tn: 112904.0000 - auc: 0.7303 - prc: 0.1307 - val_loss: 0.1442 - val_Sensitivity: 0.0101 - val_tn: 37625.0000 - val_auc: 0.6769 - val_prc: 0.0968\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1327 - Sensitivity: 0.0075 - tn: 112904.0000 - auc: 0.7230 - prc: 0.1336 - val_loss: 0.1475 - val_Sensitivity: 0.0155 - val_tn: 37598.0000 - val_auc: 0.6741 - val_prc: 0.0944\n",
      "Epoch 81/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1327 - Sensitivity: 0.0034 - tn: 112898.0000 - auc: 0.7239 - prc: 0.1289 - val_loss: 0.1431 - val_Sensitivity: 0.0031 - val_tn: 37633.0000 - val_auc: 0.6797 - val_prc: 0.0984\n",
      "Epoch 82/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1317 - Sensitivity: 0.0034 - tn: 112903.0000 - auc: 0.7332 - prc: 0.1340 - val_loss: 0.1459 - val_Sensitivity: 0.0124 - val_tn: 37615.0000 - val_auc: 0.6770 - val_prc: 0.0975\n",
      "Epoch 83/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1324 - Sensitivity: 0.0062 - tn: 112896.0000 - auc: 0.7278 - prc: 0.1317 - val_loss: 0.1479 - val_Sensitivity: 0.0132 - val_tn: 37621.0000 - val_auc: 0.6704 - val_prc: 0.0960\n",
      "Epoch 84/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1319 - Sensitivity: 0.0049 - tn: 112905.0000 - auc: 0.7309 - prc: 0.1341 - val_loss: 0.1461 - val_Sensitivity: 0.0155 - val_tn: 37613.0000 - val_auc: 0.6754 - val_prc: 0.0976\n",
      "Epoch 85/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1323 - Sensitivity: 0.0059 - tn: 112899.0000 - auc: 0.7287 - prc: 0.1320 - val_loss: 0.1459 - val_Sensitivity: 0.0093 - val_tn: 37621.0000 - val_auc: 0.6757 - val_prc: 0.0959\n",
      "Epoch 86/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1320 - Sensitivity: 0.0077 - tn: 112891.0000 - auc: 0.7302 - prc: 0.1355 - val_loss: 0.1450 - val_Sensitivity: 0.0147 - val_tn: 37611.0000 - val_auc: 0.6756 - val_prc: 0.0961\n",
      "Epoch 87/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1322 - Sensitivity: 0.0067 - tn: 112894.0000 - auc: 0.7277 - prc: 0.1345 - val_loss: 0.1452 - val_Sensitivity: 0.0108 - val_tn: 37623.0000 - val_auc: 0.6773 - val_prc: 0.0972\n",
      "Epoch 88/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1313 - Sensitivity: 0.0018 - tn: 112898.0000 - auc: 0.7390 - prc: 0.1343 - val_loss: 0.1441 - val_Sensitivity: 0.0108 - val_tn: 37616.0000 - val_auc: 0.6782 - val_prc: 0.0976\n",
      "Epoch 89/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1314 - Sensitivity: 0.0067 - tn: 112897.0000 - auc: 0.7348 - prc: 0.1407 - val_loss: 0.1460 - val_Sensitivity: 0.0178 - val_tn: 37606.0000 - val_auc: 0.6715 - val_prc: 0.0958\n",
      "Epoch 90/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1315 - Sensitivity: 0.0064 - tn: 112888.0000 - auc: 0.7334 - prc: 0.1350 - val_loss: 0.1476 - val_Sensitivity: 0.0147 - val_tn: 37606.0000 - val_auc: 0.6713 - val_prc: 0.0954\n",
      "Epoch 91/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1319 - Sensitivity: 0.0062 - tn: 112904.0000 - auc: 0.7297 - prc: 0.1387 - val_loss: 0.1451 - val_Sensitivity: 0.0124 - val_tn: 37617.0000 - val_auc: 0.6693 - val_prc: 0.0966\n",
      "Epoch 92/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1314 - Sensitivity: 0.0054 - tn: 112901.0000 - auc: 0.7341 - prc: 0.1398 - val_loss: 0.1462 - val_Sensitivity: 0.0139 - val_tn: 37612.0000 - val_auc: 0.6719 - val_prc: 0.0970\n",
      "Epoch 93/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1316 - Sensitivity: 0.0064 - tn: 112897.0000 - auc: 0.7334 - prc: 0.1362 - val_loss: 0.1480 - val_Sensitivity: 0.0132 - val_tn: 37612.0000 - val_auc: 0.6749 - val_prc: 0.0958\n",
      "Epoch 94/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1315 - Sensitivity: 0.0052 - tn: 112903.0000 - auc: 0.7341 - prc: 0.1383 - val_loss: 0.1443 - val_Sensitivity: 0.0085 - val_tn: 37623.0000 - val_auc: 0.6772 - val_prc: 0.0968\n",
      "Epoch 95/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1312 - Sensitivity: 0.0052 - tn: 112896.0000 - auc: 0.7377 - prc: 0.1373 - val_loss: 0.1463 - val_Sensitivity: 0.0155 - val_tn: 37609.0000 - val_auc: 0.6758 - val_prc: 0.0966\n",
      "Epoch 96/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1313 - Sensitivity: 0.0067 - tn: 112894.0000 - auc: 0.7348 - prc: 0.1401 - val_loss: 0.1449 - val_Sensitivity: 0.0108 - val_tn: 37624.0000 - val_auc: 0.6746 - val_prc: 0.0968\n",
      "Epoch 97/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1312 - Sensitivity: 0.0036 - tn: 112898.0000 - auc: 0.7356 - prc: 0.1390 - val_loss: 0.1453 - val_Sensitivity: 0.0116 - val_tn: 37618.0000 - val_auc: 0.6750 - val_prc: 0.0966\n",
      "Epoch 98/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1305 - Sensitivity: 0.0046 - tn: 112894.0000 - auc: 0.7412 - prc: 0.1420 - val_loss: 0.1469 - val_Sensitivity: 0.0294 - val_tn: 37572.0000 - val_auc: 0.6728 - val_prc: 0.0976\n",
      "Epoch 99/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1315 - Sensitivity: 0.0067 - tn: 112888.0000 - auc: 0.7334 - prc: 0.1384 - val_loss: 0.1481 - val_Sensitivity: 0.0155 - val_tn: 37606.0000 - val_auc: 0.6745 - val_prc: 0.0962\n",
      "Epoch 100/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1309 - Sensitivity: 0.0052 - tn: 112898.0000 - auc: 0.7390 - prc: 0.1406 - val_loss: 0.1456 - val_Sensitivity: 0.0225 - val_tn: 37590.0000 - val_auc: 0.6741 - val_prc: 0.0986\n",
      "Epoch 101/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1304 - Sensitivity: 0.0090 - tn: 112885.0000 - auc: 0.7439 - prc: 0.1431 - val_loss: 0.1453 - val_Sensitivity: 0.0155 - val_tn: 37616.0000 - val_auc: 0.6738 - val_prc: 0.0980\n",
      "Epoch 102/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1308 - Sensitivity: 0.0075 - tn: 112883.0000 - auc: 0.7373 - prc: 0.1426 - val_loss: 0.1452 - val_Sensitivity: 0.0186 - val_tn: 37605.0000 - val_auc: 0.6737 - val_prc: 0.0971\n",
      "Epoch 103/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1306 - Sensitivity: 0.0077 - tn: 112902.0000 - auc: 0.7401 - prc: 0.1456 - val_loss: 0.1453 - val_Sensitivity: 0.0186 - val_tn: 37597.0000 - val_auc: 0.6734 - val_prc: 0.0965\n",
      "Epoch 104/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1301 - Sensitivity: 0.0085 - tn: 112887.0000 - auc: 0.7464 - prc: 0.1445 - val_loss: 0.1458 - val_Sensitivity: 0.0294 - val_tn: 37565.0000 - val_auc: 0.6746 - val_prc: 0.0966\n",
      "Epoch 105/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1307 - Sensitivity: 0.0067 - tn: 112889.0000 - auc: 0.7409 - prc: 0.1418 - val_loss: 0.1462 - val_Sensitivity: 0.0225 - val_tn: 37592.0000 - val_auc: 0.6734 - val_prc: 0.0978\n",
      "Epoch 106/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1307 - Sensitivity: 0.0052 - tn: 112892.0000 - auc: 0.7393 - prc: 0.1412 - val_loss: 0.1452 - val_Sensitivity: 0.0147 - val_tn: 37607.0000 - val_auc: 0.6733 - val_prc: 0.0963\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 5s 14ms/step - loss: 0.2188 - Sensitivity: 0.0402 - tn: 110896.0000 - auc: 0.5314 - prc: 0.0405 - val_loss: 0.2038 - val_Sensitivity: 0.0239 - val_tn: 37508.0000 - val_auc: 0.6705 - val_prc: 0.0878\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1831 - Sensitivity: 0.0293 - tn: 112323.0000 - auc: 0.5589 - prc: 0.0501 - val_loss: 0.2029 - val_Sensitivity: 0.0904 - val_tn: 37221.0000 - val_auc: 0.6699 - val_prc: 0.0918\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1646 - Sensitivity: 0.0269 - tn: 112573.0000 - auc: 0.5823 - prc: 0.0577 - val_loss: 0.2086 - val_Sensitivity: 0.0927 - val_tn: 37130.0000 - val_auc: 0.6688 - val_prc: 0.0907\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1558 - Sensitivity: 0.0185 - tn: 112722.0000 - auc: 0.5992 - prc: 0.0610 - val_loss: 0.1940 - val_Sensitivity: 0.0859 - val_tn: 37251.0000 - val_auc: 0.6684 - val_prc: 0.0920\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1492 - Sensitivity: 0.0146 - tn: 112813.0000 - auc: 0.6097 - prc: 0.0682 - val_loss: 0.2004 - val_Sensitivity: 0.0964 - val_tn: 37166.0000 - val_auc: 0.6679 - val_prc: 0.0937\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1458 - Sensitivity: 0.0094 - tn: 112823.0000 - auc: 0.6231 - prc: 0.0722 - val_loss: 0.1942 - val_Sensitivity: 0.0927 - val_tn: 37218.0000 - val_auc: 0.6654 - val_prc: 0.0897\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1435 - Sensitivity: 0.0050 - tn: 112890.0000 - auc: 0.6330 - prc: 0.0741 - val_loss: 0.1906 - val_Sensitivity: 0.0942 - val_tn: 37170.0000 - val_auc: 0.6691 - val_prc: 0.0916\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1411 - Sensitivity: 0.0068 - tn: 112910.0000 - auc: 0.6478 - prc: 0.0877 - val_loss: 0.1888 - val_Sensitivity: 0.0934 - val_tn: 37203.0000 - val_auc: 0.6702 - val_prc: 0.0920\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1404 - Sensitivity: 0.0034 - tn: 112909.0000 - auc: 0.6492 - prc: 0.0854 - val_loss: 0.1818 - val_Sensitivity: 0.0740 - val_tn: 37319.0000 - val_auc: 0.6702 - val_prc: 0.0923\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1394 - Sensitivity: 0.0031 - tn: 112913.0000 - auc: 0.6557 - prc: 0.0909 - val_loss: 0.1806 - val_Sensitivity: 0.0643 - val_tn: 37359.0000 - val_auc: 0.6719 - val_prc: 0.0904\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1389 - Sensitivity: 0.0029 - tn: 112920.0000 - auc: 0.6594 - prc: 0.0918 - val_loss: 0.1817 - val_Sensitivity: 0.0725 - val_tn: 37315.0000 - val_auc: 0.6731 - val_prc: 0.0913\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1388 - Sensitivity: 0.0047 - tn: 112928.0000 - auc: 0.6595 - prc: 0.0917 - val_loss: 0.1777 - val_Sensitivity: 0.0807 - val_tn: 37295.0000 - val_auc: 0.6747 - val_prc: 0.0922\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1377 - Sensitivity: 0.0034 - tn: 112928.0000 - auc: 0.6674 - prc: 0.0960 - val_loss: 0.1755 - val_Sensitivity: 0.0830 - val_tn: 37289.0000 - val_auc: 0.6731 - val_prc: 0.0913\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1385 - Sensitivity: 0.0031 - tn: 112933.0000 - auc: 0.6601 - prc: 0.0924 - val_loss: 0.1678 - val_Sensitivity: 0.0157 - val_tn: 37545.0000 - val_auc: 0.6752 - val_prc: 0.0931\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1382 - Sensitivity: 0.0029 - tn: 112941.0000 - auc: 0.6615 - prc: 0.0930 - val_loss: 0.1722 - val_Sensitivity: 0.0575 - val_tn: 37381.0000 - val_auc: 0.6757 - val_prc: 0.0925\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1373 - Sensitivity: 0.0031 - tn: 112946.0000 - auc: 0.6676 - prc: 0.0983 - val_loss: 0.1748 - val_Sensitivity: 0.0845 - val_tn: 37271.0000 - val_auc: 0.6763 - val_prc: 0.0943\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1375 - Sensitivity: 0.0026 - tn: 112942.0000 - auc: 0.6705 - prc: 0.0961 - val_loss: 0.1656 - val_Sensitivity: 0.0531 - val_tn: 37405.0000 - val_auc: 0.6781 - val_prc: 0.0932\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1375 - Sensitivity: 0.0013 - tn: 112946.0000 - auc: 0.6707 - prc: 0.0954 - val_loss: 0.1664 - val_Sensitivity: 0.0583 - val_tn: 37397.0000 - val_auc: 0.6782 - val_prc: 0.0943\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1370 - Sensitivity: 0.0016 - tn: 112940.0000 - auc: 0.6738 - prc: 0.0965 - val_loss: 0.1578 - val_Sensitivity: 0.0135 - val_tn: 37539.0000 - val_auc: 0.6780 - val_prc: 0.0931\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1369 - Sensitivity: 0.0024 - tn: 112947.0000 - auc: 0.6745 - prc: 0.0972 - val_loss: 0.1643 - val_Sensitivity: 0.0381 - val_tn: 37456.0000 - val_auc: 0.6780 - val_prc: 0.0931\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1368 - Sensitivity: 0.0026 - tn: 112946.0000 - auc: 0.6750 - prc: 0.1001 - val_loss: 0.1647 - val_Sensitivity: 0.0448 - val_tn: 37441.0000 - val_auc: 0.6782 - val_prc: 0.0928\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1358 - Sensitivity: 0.0018 - tn: 112935.0000 - auc: 0.6852 - prc: 0.1034 - val_loss: 0.1679 - val_Sensitivity: 0.0762 - val_tn: 37293.0000 - val_auc: 0.6759 - val_prc: 0.0940\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1361 - Sensitivity: 0.0029 - tn: 112937.0000 - auc: 0.6835 - prc: 0.1017 - val_loss: 0.1628 - val_Sensitivity: 0.0845 - val_tn: 37278.0000 - val_auc: 0.6797 - val_prc: 0.0958\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1359 - Sensitivity: 0.0052 - tn: 112940.0000 - auc: 0.6840 - prc: 0.1060 - val_loss: 0.1605 - val_Sensitivity: 0.0120 - val_tn: 37553.0000 - val_auc: 0.6803 - val_prc: 0.0948\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1359 - Sensitivity: 0.0037 - tn: 112941.0000 - auc: 0.6850 - prc: 0.1040 - val_loss: 0.1577 - val_Sensitivity: 0.0478 - val_tn: 37424.0000 - val_auc: 0.6796 - val_prc: 0.0951\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1355 - Sensitivity: 0.0029 - tn: 112942.0000 - auc: 0.6856 - prc: 0.1082 - val_loss: 0.1656 - val_Sensitivity: 0.0478 - val_tn: 37422.0000 - val_auc: 0.6813 - val_prc: 0.0938\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1357 - Sensitivity: 0.0055 - tn: 112942.0000 - auc: 0.6842 - prc: 0.1080 - val_loss: 0.1555 - val_Sensitivity: 0.0164 - val_tn: 37541.0000 - val_auc: 0.6821 - val_prc: 0.0965\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1358 - Sensitivity: 0.0034 - tn: 112940.0000 - auc: 0.6850 - prc: 0.1038 - val_loss: 0.1533 - val_Sensitivity: 0.0075 - val_tn: 37574.0000 - val_auc: 0.6836 - val_prc: 0.0994\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1354 - Sensitivity: 0.0031 - tn: 112947.0000 - auc: 0.6868 - prc: 0.1068 - val_loss: 0.1535 - val_Sensitivity: 0.0097 - val_tn: 37565.0000 - val_auc: 0.6824 - val_prc: 0.0966\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1355 - Sensitivity: 0.0047 - tn: 112939.0000 - auc: 0.6862 - prc: 0.1081 - val_loss: 0.1554 - val_Sensitivity: 0.0209 - val_tn: 37521.0000 - val_auc: 0.6799 - val_prc: 0.0962\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1355 - Sensitivity: 0.0039 - tn: 112925.0000 - auc: 0.6866 - prc: 0.1078 - val_loss: 0.1547 - val_Sensitivity: 0.0075 - val_tn: 37574.0000 - val_auc: 0.6845 - val_prc: 0.0971\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 2s 10ms/step - loss: 0.1347 - Sensitivity: 0.0029 - tn: 112944.0000 - auc: 0.6952 - prc: 0.1094 - val_loss: 0.1534 - val_Sensitivity: 0.0277 - val_tn: 37506.0000 - val_auc: 0.6831 - val_prc: 0.0962\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1352 - Sensitivity: 0.0039 - tn: 112920.0000 - auc: 0.6896 - prc: 0.1094 - val_loss: 0.1509 - val_Sensitivity: 0.0000e+00 - val_tn: 37592.0000 - val_auc: 0.6814 - val_prc: 0.0966\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1352 - Sensitivity: 0.0050 - tn: 112933.0000 - auc: 0.6893 - prc: 0.1070 - val_loss: 0.1541 - val_Sensitivity: 0.0000e+00 - val_tn: 37590.0000 - val_auc: 0.6831 - val_prc: 0.0964\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1348 - Sensitivity: 0.0021 - tn: 112949.0000 - auc: 0.6945 - prc: 0.1117 - val_loss: 0.1529 - val_Sensitivity: 0.0404 - val_tn: 37486.0000 - val_auc: 0.6852 - val_prc: 0.0991\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1351 - Sensitivity: 0.0024 - tn: 112942.0000 - auc: 0.6898 - prc: 0.1086 - val_loss: 0.1546 - val_Sensitivity: 0.0127 - val_tn: 37560.0000 - val_auc: 0.6828 - val_prc: 0.0973\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1343 - Sensitivity: 0.0024 - tn: 112949.0000 - auc: 0.7001 - prc: 0.1108 - val_loss: 0.1519 - val_Sensitivity: 0.0067 - val_tn: 37579.0000 - val_auc: 0.6835 - val_prc: 0.0972\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1348 - Sensitivity: 0.0050 - tn: 112945.0000 - auc: 0.6918 - prc: 0.1150 - val_loss: 0.1534 - val_Sensitivity: 0.0112 - val_tn: 37566.0000 - val_auc: 0.6834 - val_prc: 0.0965\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1345 - Sensitivity: 0.0037 - tn: 112944.0000 - auc: 0.6974 - prc: 0.1114 - val_loss: 0.1503 - val_Sensitivity: 0.0209 - val_tn: 37533.0000 - val_auc: 0.6852 - val_prc: 0.0976\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1343 - Sensitivity: 0.0039 - tn: 112943.0000 - auc: 0.6992 - prc: 0.1141 - val_loss: 0.1503 - val_Sensitivity: 0.0082 - val_tn: 37571.0000 - val_auc: 0.6813 - val_prc: 0.0965\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1342 - Sensitivity: 0.0042 - tn: 112934.0000 - auc: 0.6998 - prc: 0.1153 - val_loss: 0.1518 - val_Sensitivity: 0.0105 - val_tn: 37557.0000 - val_auc: 0.6828 - val_prc: 0.0966\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1344 - Sensitivity: 0.0050 - tn: 112944.0000 - auc: 0.6957 - prc: 0.1149 - val_loss: 0.1524 - val_Sensitivity: 0.0015 - val_tn: 37587.0000 - val_auc: 0.6815 - val_prc: 0.0978\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1340 - Sensitivity: 0.0060 - tn: 112939.0000 - auc: 0.7005 - prc: 0.1168 - val_loss: 0.1495 - val_Sensitivity: 0.0000e+00 - val_tn: 37592.0000 - val_auc: 0.6839 - val_prc: 0.0971\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1342 - Sensitivity: 0.0016 - tn: 112939.0000 - auc: 0.7001 - prc: 0.1120 - val_loss: 0.1511 - val_Sensitivity: 0.0075 - val_tn: 37575.0000 - val_auc: 0.6853 - val_prc: 0.0977\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1338 - Sensitivity: 0.0029 - tn: 112950.0000 - auc: 0.7009 - prc: 0.1166 - val_loss: 0.1483 - val_Sensitivity: 0.0015 - val_tn: 37590.0000 - val_auc: 0.6850 - val_prc: 0.0975\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0042 - tn: 112945.0000 - auc: 0.6989 - prc: 0.1149 - val_loss: 0.1481 - val_Sensitivity: 0.0037 - val_tn: 37590.0000 - val_auc: 0.6848 - val_prc: 0.0968\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1334 - Sensitivity: 0.0029 - tn: 112944.0000 - auc: 0.7070 - prc: 0.1171 - val_loss: 0.1505 - val_Sensitivity: 0.0037 - val_tn: 37585.0000 - val_auc: 0.6816 - val_prc: 0.0957\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1338 - Sensitivity: 0.0039 - tn: 112940.0000 - auc: 0.7059 - prc: 0.1174 - val_loss: 0.1504 - val_Sensitivity: 0.0052 - val_tn: 37576.0000 - val_auc: 0.6824 - val_prc: 0.0958\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0026 - tn: 112951.0000 - auc: 0.7025 - prc: 0.1142 - val_loss: 0.1479 - val_Sensitivity: 0.0030 - val_tn: 37583.0000 - val_auc: 0.6873 - val_prc: 0.0970\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1338 - Sensitivity: 0.0013 - tn: 112941.0000 - auc: 0.7028 - prc: 0.1136 - val_loss: 0.1460 - val_Sensitivity: 0.0000e+00 - val_tn: 37591.0000 - val_auc: 0.6869 - val_prc: 0.0966\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 0.0013 - tn: 112946.0000 - auc: 0.7036 - prc: 0.1185 - val_loss: 0.1476 - val_Sensitivity: 0.0037 - val_tn: 37581.0000 - val_auc: 0.6851 - val_prc: 0.0965\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0044 - tn: 112943.0000 - auc: 0.7105 - prc: 0.1209 - val_loss: 0.1485 - val_Sensitivity: 0.0022 - val_tn: 37587.0000 - val_auc: 0.6857 - val_prc: 0.0961\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1332 - Sensitivity: 0.0021 - tn: 112948.0000 - auc: 0.7079 - prc: 0.1177 - val_loss: 0.1477 - val_Sensitivity: 0.0000e+00 - val_tn: 37592.0000 - val_auc: 0.6834 - val_prc: 0.0956\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 0.0031 - tn: 112938.0000 - auc: 0.7104 - prc: 0.1209 - val_loss: 0.1474 - val_Sensitivity: 0.0060 - val_tn: 37580.0000 - val_auc: 0.6841 - val_prc: 0.0979\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0029 - tn: 112949.0000 - auc: 0.7105 - prc: 0.1207 - val_loss: 0.1521 - val_Sensitivity: 0.0045 - val_tn: 37585.0000 - val_auc: 0.6826 - val_prc: 0.0967\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0031 - tn: 112947.0000 - auc: 0.7108 - prc: 0.1211 - val_loss: 0.1481 - val_Sensitivity: 0.0082 - val_tn: 37565.0000 - val_auc: 0.6869 - val_prc: 0.0979\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1332 - Sensitivity: 0.0031 - tn: 112942.0000 - auc: 0.7065 - prc: 0.1222 - val_loss: 0.1482 - val_Sensitivity: 0.0052 - val_tn: 37574.0000 - val_auc: 0.6866 - val_prc: 0.0965\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1328 - Sensitivity: 0.0050 - tn: 112938.0000 - auc: 0.7077 - prc: 0.1259 - val_loss: 0.1500 - val_Sensitivity: 0.0015 - val_tn: 37590.0000 - val_auc: 0.6824 - val_prc: 0.0962\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1317 - Sensitivity: 0.0055 - tn: 112940.0000 - auc: 0.7196 - prc: 0.1294 - val_loss: 0.1506 - val_Sensitivity: 0.0060 - val_tn: 37577.0000 - val_auc: 0.6842 - val_prc: 0.0953\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1331 - Sensitivity: 0.0029 - tn: 112940.0000 - auc: 0.7069 - prc: 0.1219 - val_loss: 0.1469 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.6858 - val_prc: 0.0978\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1323 - Sensitivity: 0.0026 - tn: 112942.0000 - auc: 0.7147 - prc: 0.1242 - val_loss: 0.1471 - val_Sensitivity: 0.0037 - val_tn: 37574.0000 - val_auc: 0.6838 - val_prc: 0.0957\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1320 - Sensitivity: 0.0050 - tn: 112943.0000 - auc: 0.7180 - prc: 0.1291 - val_loss: 0.1490 - val_Sensitivity: 0.0120 - val_tn: 37561.0000 - val_auc: 0.6842 - val_prc: 0.0967\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1321 - Sensitivity: 0.0063 - tn: 112942.0000 - auc: 0.7185 - prc: 0.1265 - val_loss: 0.1467 - val_Sensitivity: 0.0052 - val_tn: 37579.0000 - val_auc: 0.6842 - val_prc: 0.0967\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1323 - Sensitivity: 0.0044 - tn: 112944.0000 - auc: 0.7143 - prc: 0.1261 - val_loss: 0.1461 - val_Sensitivity: 0.0105 - val_tn: 37558.0000 - val_auc: 0.6847 - val_prc: 0.0968\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1324 - Sensitivity: 0.0042 - tn: 112935.0000 - auc: 0.7132 - prc: 0.1254 - val_loss: 0.1479 - val_Sensitivity: 0.0090 - val_tn: 37565.0000 - val_auc: 0.6854 - val_prc: 0.0964\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1319 - Sensitivity: 0.0034 - tn: 112942.0000 - auc: 0.7192 - prc: 0.1269 - val_loss: 0.1487 - val_Sensitivity: 0.0060 - val_tn: 37571.0000 - val_auc: 0.6826 - val_prc: 0.0965\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1318 - Sensitivity: 0.0031 - tn: 112941.0000 - auc: 0.7215 - prc: 0.1268 - val_loss: 0.1477 - val_Sensitivity: 0.0037 - val_tn: 37588.0000 - val_auc: 0.6822 - val_prc: 0.0966\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1322 - Sensitivity: 0.0055 - tn: 112944.0000 - auc: 0.7161 - prc: 0.1287 - val_loss: 0.1483 - val_Sensitivity: 0.0157 - val_tn: 37541.0000 - val_auc: 0.6861 - val_prc: 0.0962\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1318 - Sensitivity: 0.0055 - tn: 112932.0000 - auc: 0.7213 - prc: 0.1305 - val_loss: 0.1483 - val_Sensitivity: 0.0120 - val_tn: 37556.0000 - val_auc: 0.6864 - val_prc: 0.0957\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1322 - Sensitivity: 0.0060 - tn: 112939.0000 - auc: 0.7163 - prc: 0.1282 - val_loss: 0.1479 - val_Sensitivity: 0.0082 - val_tn: 37563.0000 - val_auc: 0.6831 - val_prc: 0.0953\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1316 - Sensitivity: 0.0044 - tn: 112941.0000 - auc: 0.7205 - prc: 0.1311 - val_loss: 0.1467 - val_Sensitivity: 0.0112 - val_tn: 37556.0000 - val_auc: 0.6839 - val_prc: 0.0959\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1317 - Sensitivity: 0.0055 - tn: 112947.0000 - auc: 0.7212 - prc: 0.1312 - val_loss: 0.1485 - val_Sensitivity: 0.0045 - val_tn: 37582.0000 - val_auc: 0.6849 - val_prc: 0.0951\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1317 - Sensitivity: 0.0018 - tn: 112948.0000 - auc: 0.7203 - prc: 0.1309 - val_loss: 0.1471 - val_Sensitivity: 0.0082 - val_tn: 37559.0000 - val_auc: 0.6808 - val_prc: 0.0948\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1318 - Sensitivity: 0.0037 - tn: 112929.0000 - auc: 0.7178 - prc: 0.1294 - val_loss: 0.1467 - val_Sensitivity: 0.0000e+00 - val_tn: 37592.0000 - val_auc: 0.6835 - val_prc: 0.0949\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1310 - Sensitivity: 0.0052 - tn: 112942.0000 - auc: 0.7256 - prc: 0.1339 - val_loss: 0.1477 - val_Sensitivity: 0.0030 - val_tn: 37587.0000 - val_auc: 0.6811 - val_prc: 0.0961\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 5s 15ms/step - loss: 0.2202 - Sensitivity: 0.0362 - tn: 110854.0000 - auc: 0.5281 - prc: 0.0398 - val_loss: 0.2076 - val_Sensitivity: 0.0128 - val_tn: 37559.0000 - val_auc: 0.6614 - val_prc: 0.0874\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1820 - Sensitivity: 0.0281 - tn: 112306.0000 - auc: 0.5559 - prc: 0.0488 - val_loss: 0.1942 - val_Sensitivity: 0.1024 - val_tn: 37261.0000 - val_auc: 0.6627 - val_prc: 0.0983\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1658 - Sensitivity: 0.0203 - tn: 112575.0000 - auc: 0.5768 - prc: 0.0534 - val_loss: 0.2023 - val_Sensitivity: 0.1039 - val_tn: 37242.0000 - val_auc: 0.6633 - val_prc: 0.0994\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1552 - Sensitivity: 0.0177 - tn: 112720.0000 - auc: 0.6011 - prc: 0.0621 - val_loss: 0.2028 - val_Sensitivity: 0.1002 - val_tn: 37252.0000 - val_auc: 0.6613 - val_prc: 0.0953\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1493 - Sensitivity: 0.0089 - tn: 112785.0000 - auc: 0.6192 - prc: 0.0635 - val_loss: 0.1930 - val_Sensitivity: 0.0919 - val_tn: 37340.0000 - val_auc: 0.6622 - val_prc: 0.0970\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1455 - Sensitivity: 0.0086 - tn: 112869.0000 - auc: 0.6328 - prc: 0.0714 - val_loss: 0.1926 - val_Sensitivity: 0.1024 - val_tn: 37259.0000 - val_auc: 0.6600 - val_prc: 0.0940\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1440 - Sensitivity: 0.0060 - tn: 112882.0000 - auc: 0.6380 - prc: 0.0749 - val_loss: 0.1903 - val_Sensitivity: 0.0851 - val_tn: 37342.0000 - val_auc: 0.6594 - val_prc: 0.0972\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1418 - Sensitivity: 0.0039 - tn: 112905.0000 - auc: 0.6448 - prc: 0.0819 - val_loss: 0.1846 - val_Sensitivity: 0.0843 - val_tn: 37354.0000 - val_auc: 0.6620 - val_prc: 0.0982\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1407 - Sensitivity: 0.0060 - tn: 112917.0000 - auc: 0.6521 - prc: 0.0872 - val_loss: 0.1938 - val_Sensitivity: 0.1017 - val_tn: 37269.0000 - val_auc: 0.6616 - val_prc: 0.0951\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1406 - Sensitivity: 0.0036 - tn: 112915.0000 - auc: 0.6530 - prc: 0.0820 - val_loss: 0.1874 - val_Sensitivity: 0.1032 - val_tn: 37206.0000 - val_auc: 0.6607 - val_prc: 0.0914\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1397 - Sensitivity: 0.0018 - tn: 112920.0000 - auc: 0.6573 - prc: 0.0855 - val_loss: 0.1805 - val_Sensitivity: 0.0715 - val_tn: 37415.0000 - val_auc: 0.6630 - val_prc: 0.1002\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1389 - Sensitivity: 0.0018 - tn: 112930.0000 - auc: 0.6647 - prc: 0.0889 - val_loss: 0.1817 - val_Sensitivity: 0.0896 - val_tn: 37346.0000 - val_auc: 0.6631 - val_prc: 0.0980\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1390 - Sensitivity: 0.0034 - tn: 112926.0000 - auc: 0.6629 - prc: 0.0901 - val_loss: 0.1838 - val_Sensitivity: 0.0896 - val_tn: 37322.0000 - val_auc: 0.6612 - val_prc: 0.0940\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 0.0031 - tn: 112932.0000 - auc: 0.6678 - prc: 0.0928 - val_loss: 0.1742 - val_Sensitivity: 0.0873 - val_tn: 37353.0000 - val_auc: 0.6650 - val_prc: 0.1003\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1385 - Sensitivity: 0.0026 - tn: 112930.0000 - auc: 0.6698 - prc: 0.0884 - val_loss: 0.1764 - val_Sensitivity: 0.0783 - val_tn: 37372.0000 - val_auc: 0.6667 - val_prc: 0.0964\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1378 - Sensitivity: 0.0018 - tn: 112929.0000 - auc: 0.6743 - prc: 0.0934 - val_loss: 0.1750 - val_Sensitivity: 0.0791 - val_tn: 37372.0000 - val_auc: 0.6694 - val_prc: 0.0970\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1380 - Sensitivity: 0.0018 - tn: 112932.0000 - auc: 0.6716 - prc: 0.0922 - val_loss: 0.1714 - val_Sensitivity: 0.0700 - val_tn: 37410.0000 - val_auc: 0.6691 - val_prc: 0.0996\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1381 - Sensitivity: 0.0026 - tn: 112927.0000 - auc: 0.6716 - prc: 0.0938 - val_loss: 0.1678 - val_Sensitivity: 0.0414 - val_tn: 37502.0000 - val_auc: 0.6708 - val_prc: 0.1029\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1375 - Sensitivity: 0.0029 - tn: 112933.0000 - auc: 0.6753 - prc: 0.0954 - val_loss: 0.1692 - val_Sensitivity: 0.0512 - val_tn: 37468.0000 - val_auc: 0.6686 - val_prc: 0.0980\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1370 - Sensitivity: 0.0018 - tn: 112933.0000 - auc: 0.6818 - prc: 0.0971 - val_loss: 0.1627 - val_Sensitivity: 0.0474 - val_tn: 37491.0000 - val_auc: 0.6702 - val_prc: 0.1033\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1371 - Sensitivity: 7.8125e-04 - tn: 112932.0000 - auc: 0.6769 - prc: 0.0983 - val_loss: 0.1629 - val_Sensitivity: 0.0535 - val_tn: 37469.0000 - val_auc: 0.6701 - val_prc: 0.1007\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1369 - Sensitivity: 0.0018 - tn: 112931.0000 - auc: 0.6805 - prc: 0.0979 - val_loss: 0.1614 - val_Sensitivity: 0.0136 - val_tn: 37562.0000 - val_auc: 0.6722 - val_prc: 0.1028\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 0.0016 - tn: 112931.0000 - auc: 0.6889 - prc: 0.0979 - val_loss: 0.1630 - val_Sensitivity: 0.0120 - val_tn: 37567.0000 - val_auc: 0.6720 - val_prc: 0.1010\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1367 - Sensitivity: 0.0010 - tn: 112933.0000 - auc: 0.6852 - prc: 0.0979 - val_loss: 0.1582 - val_Sensitivity: 0.0211 - val_tn: 37548.0000 - val_auc: 0.6710 - val_prc: 0.1025\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0026 - tn: 112926.0000 - auc: 0.6921 - prc: 0.0996 - val_loss: 0.1570 - val_Sensitivity: 7.5301e-04 - val_tn: 37601.0000 - val_auc: 0.6728 - val_prc: 0.1040\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1363 - Sensitivity: 5.2083e-04 - tn: 112944.0000 - auc: 0.6850 - prc: 0.0988 - val_loss: 0.1581 - val_Sensitivity: 0.0083 - val_tn: 37593.0000 - val_auc: 0.6724 - val_prc: 0.1075\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1359 - Sensitivity: 0.0026 - tn: 112938.0000 - auc: 0.6926 - prc: 0.1033 - val_loss: 0.1584 - val_Sensitivity: 0.0218 - val_tn: 37548.0000 - val_auc: 0.6718 - val_prc: 0.1036\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0018 - tn: 112935.0000 - auc: 0.6895 - prc: 0.0992 - val_loss: 0.1540 - val_Sensitivity: 0.0053 - val_tn: 37592.0000 - val_auc: 0.6728 - val_prc: 0.1042\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1358 - Sensitivity: 0.0010 - tn: 112937.0000 - auc: 0.6927 - prc: 0.1010 - val_loss: 0.1596 - val_Sensitivity: 0.0241 - val_tn: 37545.0000 - val_auc: 0.6719 - val_prc: 0.1027\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1359 - Sensitivity: 0.0023 - tn: 112926.0000 - auc: 0.6926 - prc: 0.1035 - val_loss: 0.1510 - val_Sensitivity: 0.0053 - val_tn: 37590.0000 - val_auc: 0.6732 - val_prc: 0.1037\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1361 - Sensitivity: 0.0021 - tn: 112932.0000 - auc: 0.6887 - prc: 0.1025 - val_loss: 0.1541 - val_Sensitivity: 0.0256 - val_tn: 37531.0000 - val_auc: 0.6749 - val_prc: 0.1018\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0026 - tn: 112933.0000 - auc: 0.6988 - prc: 0.1078 - val_loss: 0.1547 - val_Sensitivity: 0.0361 - val_tn: 37517.0000 - val_auc: 0.6703 - val_prc: 0.0994\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 0.0023 - tn: 112928.0000 - auc: 0.6971 - prc: 0.1068 - val_loss: 0.1513 - val_Sensitivity: 0.0038 - val_tn: 37595.0000 - val_auc: 0.6741 - val_prc: 0.1050\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1358 - Sensitivity: 7.8125e-04 - tn: 112937.0000 - auc: 0.6904 - prc: 0.1035 - val_loss: 0.1561 - val_Sensitivity: 0.0090 - val_tn: 37575.0000 - val_auc: 0.6712 - val_prc: 0.1031\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 0.0013 - tn: 112938.0000 - auc: 0.6957 - prc: 0.1042 - val_loss: 0.1524 - val_Sensitivity: 0.0023 - val_tn: 37597.0000 - val_auc: 0.6766 - val_prc: 0.1072\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1353 - Sensitivity: 0.0023 - tn: 112929.0000 - auc: 0.6955 - prc: 0.1095 - val_loss: 0.1520 - val_Sensitivity: 0.0068 - val_tn: 37595.0000 - val_auc: 0.6749 - val_prc: 0.1063\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1353 - Sensitivity: 0.0010 - tn: 112937.0000 - auc: 0.6969 - prc: 0.1076 - val_loss: 0.1482 - val_Sensitivity: 7.5301e-04 - val_tn: 37600.0000 - val_auc: 0.6760 - val_prc: 0.1071\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1345 - Sensitivity: 0.0021 - tn: 112937.0000 - auc: 0.7042 - prc: 0.1128 - val_loss: 0.1468 - val_Sensitivity: 0.0075 - val_tn: 37589.0000 - val_auc: 0.6754 - val_prc: 0.1076\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0021 - tn: 112936.0000 - auc: 0.6998 - prc: 0.1079 - val_loss: 0.1557 - val_Sensitivity: 0.0068 - val_tn: 37586.0000 - val_auc: 0.6751 - val_prc: 0.1060\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0021 - tn: 112936.0000 - auc: 0.7018 - prc: 0.1077 - val_loss: 0.1513 - val_Sensitivity: 0.0105 - val_tn: 37580.0000 - val_auc: 0.6722 - val_prc: 0.1060\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0021 - tn: 112932.0000 - auc: 0.7017 - prc: 0.1091 - val_loss: 0.1520 - val_Sensitivity: 0.0030 - val_tn: 37595.0000 - val_auc: 0.6751 - val_prc: 0.1070\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0021 - tn: 112944.0000 - auc: 0.7008 - prc: 0.1094 - val_loss: 0.1523 - val_Sensitivity: 0.0015 - val_tn: 37602.0000 - val_auc: 0.6740 - val_prc: 0.1081\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1347 - Sensitivity: 5.2083e-04 - tn: 112942.0000 - auc: 0.7034 - prc: 0.1074 - val_loss: 0.1483 - val_Sensitivity: 0.0015 - val_tn: 37602.0000 - val_auc: 0.6755 - val_prc: 0.1085\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0010 - tn: 112946.0000 - auc: 0.7014 - prc: 0.1087 - val_loss: 0.1494 - val_Sensitivity: 0.0045 - val_tn: 37598.0000 - val_auc: 0.6746 - val_prc: 0.1071\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1344 - Sensitivity: 0.0013 - tn: 112938.0000 - auc: 0.7052 - prc: 0.1097 - val_loss: 0.1471 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.6717 - val_prc: 0.1057\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0029 - tn: 112934.0000 - auc: 0.7065 - prc: 0.1116 - val_loss: 0.1516 - val_Sensitivity: 0.0038 - val_tn: 37596.0000 - val_auc: 0.6750 - val_prc: 0.1072\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1341 - Sensitivity: 0.0021 - tn: 112941.0000 - auc: 0.7062 - prc: 0.1119 - val_loss: 0.1478 - val_Sensitivity: 0.0075 - val_tn: 37579.0000 - val_auc: 0.6738 - val_prc: 0.1045\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1345 - Sensitivity: 0.0016 - tn: 112933.0000 - auc: 0.7050 - prc: 0.1118 - val_loss: 0.1479 - val_Sensitivity: 0.0030 - val_tn: 37597.0000 - val_auc: 0.6748 - val_prc: 0.1072\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1341 - Sensitivity: 0.0029 - tn: 112938.0000 - auc: 0.7075 - prc: 0.1159 - val_loss: 0.1494 - val_Sensitivity: 0.0053 - val_tn: 37594.0000 - val_auc: 0.6751 - val_prc: 0.1062\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1345 - Sensitivity: 0.0010 - tn: 112938.0000 - auc: 0.7032 - prc: 0.1121 - val_loss: 0.1490 - val_Sensitivity: 0.0045 - val_tn: 37593.0000 - val_auc: 0.6766 - val_prc: 0.1075\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1334 - Sensitivity: 0.0026 - tn: 112934.0000 - auc: 0.7142 - prc: 0.1168 - val_loss: 0.1488 - val_Sensitivity: 0.0015 - val_tn: 37602.0000 - val_auc: 0.6763 - val_prc: 0.1060\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 0.0026 - tn: 112936.0000 - auc: 0.7088 - prc: 0.1142 - val_loss: 0.1471 - val_Sensitivity: 0.0030 - val_tn: 37592.0000 - val_auc: 0.6742 - val_prc: 0.1052\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0018 - tn: 112943.0000 - auc: 0.7097 - prc: 0.1141 - val_loss: 0.1497 - val_Sensitivity: 0.0000e+00 - val_tn: 37602.0000 - val_auc: 0.6729 - val_prc: 0.1045\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1336 - Sensitivity: 0.0029 - tn: 112936.0000 - auc: 0.7118 - prc: 0.1169 - val_loss: 0.1502 - val_Sensitivity: 0.0105 - val_tn: 37570.0000 - val_auc: 0.6720 - val_prc: 0.1043\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 0.0026 - tn: 112942.0000 - auc: 0.7128 - prc: 0.1202 - val_loss: 0.1463 - val_Sensitivity: 0.0038 - val_tn: 37594.0000 - val_auc: 0.6737 - val_prc: 0.1067\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 0.0029 - tn: 112933.0000 - auc: 0.7089 - prc: 0.1213 - val_loss: 0.1499 - val_Sensitivity: 0.0075 - val_tn: 37580.0000 - val_auc: 0.6749 - val_prc: 0.1050\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1325 - Sensitivity: 0.0029 - tn: 112941.0000 - auc: 0.7176 - prc: 0.1263 - val_loss: 0.1467 - val_Sensitivity: 0.0068 - val_tn: 37586.0000 - val_auc: 0.6738 - val_prc: 0.1053\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 0.0044 - tn: 112931.0000 - auc: 0.7121 - prc: 0.1175 - val_loss: 0.1522 - val_Sensitivity: 0.0279 - val_tn: 37550.0000 - val_auc: 0.6743 - val_prc: 0.1054\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1327 - Sensitivity: 0.0023 - tn: 112940.0000 - auc: 0.7174 - prc: 0.1235 - val_loss: 0.1490 - val_Sensitivity: 0.0105 - val_tn: 37584.0000 - val_auc: 0.6720 - val_prc: 0.1053\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 0.0021 - tn: 112935.0000 - auc: 0.7180 - prc: 0.1196 - val_loss: 0.1506 - val_Sensitivity: 0.0075 - val_tn: 37587.0000 - val_auc: 0.6725 - val_prc: 0.1048\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0026 - tn: 112936.0000 - auc: 0.7155 - prc: 0.1222 - val_loss: 0.1475 - val_Sensitivity: 0.0090 - val_tn: 37581.0000 - val_auc: 0.6746 - val_prc: 0.1053\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 0.0023 - tn: 112935.0000 - auc: 0.7187 - prc: 0.1210 - val_loss: 0.1473 - val_Sensitivity: 0.0075 - val_tn: 37584.0000 - val_auc: 0.6740 - val_prc: 0.1062\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 0.0052 - tn: 112932.0000 - auc: 0.7172 - prc: 0.1228 - val_loss: 0.1502 - val_Sensitivity: 0.0038 - val_tn: 37592.0000 - val_auc: 0.6729 - val_prc: 0.1045\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 0.0031 - tn: 112935.0000 - auc: 0.7160 - prc: 0.1236 - val_loss: 0.1473 - val_Sensitivity: 0.0023 - val_tn: 37600.0000 - val_auc: 0.6724 - val_prc: 0.1033\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1323 - Sensitivity: 0.0026 - tn: 112935.0000 - auc: 0.7221 - prc: 0.1251 - val_loss: 0.1476 - val_Sensitivity: 0.0060 - val_tn: 37589.0000 - val_auc: 0.6722 - val_prc: 0.1041\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1328 - Sensitivity: 0.0047 - tn: 112938.0000 - auc: 0.7160 - prc: 0.1235 - val_loss: 0.1490 - val_Sensitivity: 0.0090 - val_tn: 37578.0000 - val_auc: 0.6732 - val_prc: 0.1050\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1324 - Sensitivity: 0.0047 - tn: 112924.0000 - auc: 0.7222 - prc: 0.1214 - val_loss: 0.1494 - val_Sensitivity: 0.0038 - val_tn: 37592.0000 - val_auc: 0.6739 - val_prc: 0.1031\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 0.0044 - tn: 112939.0000 - auc: 0.7147 - prc: 0.1263 - val_loss: 0.1463 - val_Sensitivity: 0.0090 - val_tn: 37575.0000 - val_auc: 0.6728 - val_prc: 0.1042\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1324 - Sensitivity: 0.0039 - tn: 112935.0000 - auc: 0.7213 - prc: 0.1229 - val_loss: 0.1482 - val_Sensitivity: 0.0120 - val_tn: 37573.0000 - val_auc: 0.6750 - val_prc: 0.1059\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1324 - Sensitivity: 0.0026 - tn: 112932.0000 - auc: 0.7202 - prc: 0.1252 - val_loss: 0.1472 - val_Sensitivity: 0.0053 - val_tn: 37590.0000 - val_auc: 0.6725 - val_prc: 0.1053\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1322 - Sensitivity: 0.0063 - tn: 112932.0000 - auc: 0.7213 - prc: 0.1301 - val_loss: 0.1479 - val_Sensitivity: 0.0053 - val_tn: 37590.0000 - val_auc: 0.6746 - val_prc: 0.1063\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1319 - Sensitivity: 0.0039 - tn: 112940.0000 - auc: 0.7237 - prc: 0.1292 - val_loss: 0.1474 - val_Sensitivity: 0.0113 - val_tn: 37580.0000 - val_auc: 0.6735 - val_prc: 0.1049\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1319 - Sensitivity: 0.0055 - tn: 112932.0000 - auc: 0.7250 - prc: 0.1285 - val_loss: 0.1498 - val_Sensitivity: 0.0105 - val_tn: 37576.0000 - val_auc: 0.6738 - val_prc: 0.1041\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1328 - Sensitivity: 0.0039 - tn: 112931.0000 - auc: 0.7164 - prc: 0.1232 - val_loss: 0.1485 - val_Sensitivity: 0.0045 - val_tn: 37591.0000 - val_auc: 0.6726 - val_prc: 0.1041\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1321 - Sensitivity: 0.0042 - tn: 112931.0000 - auc: 0.7247 - prc: 0.1262 - val_loss: 0.1462 - val_Sensitivity: 0.0045 - val_tn: 37590.0000 - val_auc: 0.6719 - val_prc: 0.1037\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1322 - Sensitivity: 0.0026 - tn: 112944.0000 - auc: 0.7243 - prc: 0.1277 - val_loss: 0.1470 - val_Sensitivity: 0.0090 - val_tn: 37582.0000 - val_auc: 0.6728 - val_prc: 0.1048\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1320 - Sensitivity: 0.0055 - tn: 112940.0000 - auc: 0.7232 - prc: 0.1298 - val_loss: 0.1492 - val_Sensitivity: 0.0068 - val_tn: 37585.0000 - val_auc: 0.6743 - val_prc: 0.1033\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1318 - Sensitivity: 0.0042 - tn: 112937.0000 - auc: 0.7255 - prc: 0.1290 - val_loss: 0.1465 - val_Sensitivity: 0.0053 - val_tn: 37587.0000 - val_auc: 0.6699 - val_prc: 0.1038\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1316 - Sensitivity: 0.0052 - tn: 112938.0000 - auc: 0.7300 - prc: 0.1276 - val_loss: 0.1477 - val_Sensitivity: 0.0045 - val_tn: 37590.0000 - val_auc: 0.6733 - val_prc: 0.1028\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1312 - Sensitivity: 0.0021 - tn: 112938.0000 - auc: 0.7305 - prc: 0.1296 - val_loss: 0.1470 - val_Sensitivity: 0.0090 - val_tn: 37579.0000 - val_auc: 0.6731 - val_prc: 0.1035\n",
      "Epoch 81/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1312 - Sensitivity: 0.0049 - tn: 112934.0000 - auc: 0.7311 - prc: 0.1327 - val_loss: 0.1477 - val_Sensitivity: 0.0264 - val_tn: 37547.0000 - val_auc: 0.6740 - val_prc: 0.1036\n",
      "Epoch 82/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1316 - Sensitivity: 0.0047 - tn: 112929.0000 - auc: 0.7265 - prc: 0.1290 - val_loss: 0.1466 - val_Sensitivity: 0.0143 - val_tn: 37574.0000 - val_auc: 0.6733 - val_prc: 0.1050\n",
      "Epoch 83/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1313 - Sensitivity: 0.0052 - tn: 112932.0000 - auc: 0.7309 - prc: 0.1323 - val_loss: 0.1472 - val_Sensitivity: 0.0128 - val_tn: 37571.0000 - val_auc: 0.6743 - val_prc: 0.1045\n",
      "Epoch 84/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1315 - Sensitivity: 0.0031 - tn: 112932.0000 - auc: 0.7276 - prc: 0.1308 - val_loss: 0.1482 - val_Sensitivity: 0.0113 - val_tn: 37574.0000 - val_auc: 0.6738 - val_prc: 0.1025\n",
      "Epoch 85/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1316 - Sensitivity: 0.0057 - tn: 112926.0000 - auc: 0.7286 - prc: 0.1281 - val_loss: 0.1471 - val_Sensitivity: 0.0060 - val_tn: 37581.0000 - val_auc: 0.6725 - val_prc: 0.1040\n",
      "Epoch 86/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1315 - Sensitivity: 0.0055 - tn: 112936.0000 - auc: 0.7287 - prc: 0.1306 - val_loss: 0.1482 - val_Sensitivity: 0.0053 - val_tn: 37585.0000 - val_auc: 0.6699 - val_prc: 0.1026\n",
      "Epoch 87/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1308 - Sensitivity: 0.0049 - tn: 112936.0000 - auc: 0.7348 - prc: 0.1339 - val_loss: 0.1474 - val_Sensitivity: 0.0105 - val_tn: 37579.0000 - val_auc: 0.6717 - val_prc: 0.1049\n",
      "Epoch 88/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1311 - Sensitivity: 0.0052 - tn: 112933.0000 - auc: 0.7336 - prc: 0.1302 - val_loss: 0.1468 - val_Sensitivity: 0.0098 - val_tn: 37578.0000 - val_auc: 0.6712 - val_prc: 0.1045\n",
      "Epoch 89/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1310 - Sensitivity: 0.0044 - tn: 112940.0000 - auc: 0.7305 - prc: 0.1371 - val_loss: 0.1488 - val_Sensitivity: 0.0211 - val_tn: 37564.0000 - val_auc: 0.6693 - val_prc: 0.1033\n",
      "Epoch 90/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1310 - Sensitivity: 0.0063 - tn: 112925.0000 - auc: 0.7313 - prc: 0.1349 - val_loss: 0.1475 - val_Sensitivity: 0.0113 - val_tn: 37573.0000 - val_auc: 0.6700 - val_prc: 0.1027\n",
      "Epoch 91/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1304 - Sensitivity: 0.0052 - tn: 112923.0000 - auc: 0.7367 - prc: 0.1366 - val_loss: 0.1461 - val_Sensitivity: 0.0105 - val_tn: 37576.0000 - val_auc: 0.6702 - val_prc: 0.1021\n",
      "Epoch 92/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1306 - Sensitivity: 0.0057 - tn: 112929.0000 - auc: 0.7360 - prc: 0.1349 - val_loss: 0.1453 - val_Sensitivity: 0.0060 - val_tn: 37582.0000 - val_auc: 0.6741 - val_prc: 0.1034\n",
      "Epoch 93/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1310 - Sensitivity: 0.0049 - tn: 112939.0000 - auc: 0.7320 - prc: 0.1354 - val_loss: 0.1481 - val_Sensitivity: 0.0113 - val_tn: 37574.0000 - val_auc: 0.6725 - val_prc: 0.1023\n",
      "Epoch 94/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1310 - Sensitivity: 0.0042 - tn: 112931.0000 - auc: 0.7301 - prc: 0.1336 - val_loss: 0.1486 - val_Sensitivity: 0.0151 - val_tn: 37571.0000 - val_auc: 0.6716 - val_prc: 0.1021\n",
      "Epoch 95/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1302 - Sensitivity: 0.0060 - tn: 112933.0000 - auc: 0.7360 - prc: 0.1407 - val_loss: 0.1450 - val_Sensitivity: 0.0105 - val_tn: 37575.0000 - val_auc: 0.6708 - val_prc: 0.1032\n",
      "Epoch 96/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1307 - Sensitivity: 0.0065 - tn: 112934.0000 - auc: 0.7341 - prc: 0.1378 - val_loss: 0.1463 - val_Sensitivity: 0.0083 - val_tn: 37578.0000 - val_auc: 0.6707 - val_prc: 0.1028\n",
      "Epoch 97/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1307 - Sensitivity: 0.0044 - tn: 112923.0000 - auc: 0.7346 - prc: 0.1336 - val_loss: 0.1474 - val_Sensitivity: 0.0075 - val_tn: 37585.0000 - val_auc: 0.6688 - val_prc: 0.1018\n",
      "Epoch 98/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1308 - Sensitivity: 0.0039 - tn: 112924.0000 - auc: 0.7351 - prc: 0.1334 - val_loss: 0.1466 - val_Sensitivity: 0.0075 - val_tn: 37584.0000 - val_auc: 0.6685 - val_prc: 0.1024\n",
      "Epoch 99/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1300 - Sensitivity: 0.0081 - tn: 112929.0000 - auc: 0.7378 - prc: 0.1440 - val_loss: 0.1470 - val_Sensitivity: 0.0143 - val_tn: 37566.0000 - val_auc: 0.6694 - val_prc: 0.1020\n",
      "Epoch 100/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1296 - Sensitivity: 0.0049 - tn: 112929.0000 - auc: 0.7411 - prc: 0.1432 - val_loss: 0.1469 - val_Sensitivity: 0.0173 - val_tn: 37568.0000 - val_auc: 0.6717 - val_prc: 0.1021\n",
      "Epoch 101/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1301 - Sensitivity: 0.0057 - tn: 112922.0000 - auc: 0.7407 - prc: 0.1400 - val_loss: 0.1486 - val_Sensitivity: 0.0098 - val_tn: 37578.0000 - val_auc: 0.6692 - val_prc: 0.1000\n",
      "Epoch 102/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1300 - Sensitivity: 0.0065 - tn: 112928.0000 - auc: 0.7378 - prc: 0.1433 - val_loss: 0.1506 - val_Sensitivity: 0.0203 - val_tn: 37560.0000 - val_auc: 0.6705 - val_prc: 0.1015\n",
      "Epoch 103/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1307 - Sensitivity: 0.0089 - tn: 112925.0000 - auc: 0.7357 - prc: 0.1362 - val_loss: 0.1465 - val_Sensitivity: 0.0128 - val_tn: 37571.0000 - val_auc: 0.6708 - val_prc: 0.1015\n",
      "Epoch 104/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1304 - Sensitivity: 0.0057 - tn: 112925.0000 - auc: 0.7357 - prc: 0.1388 - val_loss: 0.1483 - val_Sensitivity: 0.0271 - val_tn: 37541.0000 - val_auc: 0.6706 - val_prc: 0.1017\n",
      "Epoch 105/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1294 - Sensitivity: 0.0070 - tn: 112931.0000 - auc: 0.7457 - prc: 0.1414 - val_loss: 0.1486 - val_Sensitivity: 0.0196 - val_tn: 37565.0000 - val_auc: 0.6690 - val_prc: 0.1023\n",
      "Epoch 106/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1295 - Sensitivity: 0.0078 - tn: 112934.0000 - auc: 0.7440 - prc: 0.1422 - val_loss: 0.1479 - val_Sensitivity: 0.0241 - val_tn: 37550.0000 - val_auc: 0.6672 - val_prc: 0.1019\n",
      "Epoch 107/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1293 - Sensitivity: 0.0057 - tn: 112923.0000 - auc: 0.7435 - prc: 0.1439 - val_loss: 0.1489 - val_Sensitivity: 0.0188 - val_tn: 37559.0000 - val_auc: 0.6670 - val_prc: 0.0998\n",
      "Epoch 108/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1298 - Sensitivity: 0.0068 - tn: 112923.0000 - auc: 0.7418 - prc: 0.1412 - val_loss: 0.1497 - val_Sensitivity: 0.0226 - val_tn: 37556.0000 - val_auc: 0.6689 - val_prc: 0.1013\n",
      "Epoch 109/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1297 - Sensitivity: 0.0117 - tn: 112924.0000 - auc: 0.7419 - prc: 0.1452 - val_loss: 0.1459 - val_Sensitivity: 0.0158 - val_tn: 37565.0000 - val_auc: 0.6666 - val_prc: 0.1007\n",
      "Epoch 110/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1298 - Sensitivity: 0.0065 - tn: 112933.0000 - auc: 0.7409 - prc: 0.1444 - val_loss: 0.1476 - val_Sensitivity: 0.0181 - val_tn: 37566.0000 - val_auc: 0.6710 - val_prc: 0.1019\n",
      "Epoch 111/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1297 - Sensitivity: 0.0068 - tn: 112924.0000 - auc: 0.7407 - prc: 0.1446 - val_loss: 0.1482 - val_Sensitivity: 0.0226 - val_tn: 37555.0000 - val_auc: 0.6700 - val_prc: 0.1024\n",
      "Epoch 112/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1295 - Sensitivity: 0.0076 - tn: 112926.0000 - auc: 0.7441 - prc: 0.1441 - val_loss: 0.1481 - val_Sensitivity: 0.0143 - val_tn: 37568.0000 - val_auc: 0.6664 - val_prc: 0.1024\n",
      "Epoch 113/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1294 - Sensitivity: 0.0089 - tn: 112922.0000 - auc: 0.7427 - prc: 0.1450 - val_loss: 0.1471 - val_Sensitivity: 0.0151 - val_tn: 37567.0000 - val_auc: 0.6694 - val_prc: 0.1022\n",
      "Epoch 114/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1300 - Sensitivity: 0.0068 - tn: 112934.0000 - auc: 0.7371 - prc: 0.1467 - val_loss: 0.1481 - val_Sensitivity: 0.0181 - val_tn: 37563.0000 - val_auc: 0.6664 - val_prc: 0.1018\n",
      "Epoch 115/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1297 - Sensitivity: 0.0049 - tn: 112930.0000 - auc: 0.7429 - prc: 0.1411 - val_loss: 0.1470 - val_Sensitivity: 0.0158 - val_tn: 37568.0000 - val_auc: 0.6659 - val_prc: 0.1018\n",
      "Epoch 116/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1290 - Sensitivity: 0.0073 - tn: 112929.0000 - auc: 0.7460 - prc: 0.1469 - val_loss: 0.1467 - val_Sensitivity: 0.0173 - val_tn: 37560.0000 - val_auc: 0.6656 - val_prc: 0.1027\n",
      "Epoch 117/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1292 - Sensitivity: 0.0063 - tn: 112928.0000 - auc: 0.7448 - prc: 0.1454 - val_loss: 0.1482 - val_Sensitivity: 0.0136 - val_tn: 37572.0000 - val_auc: 0.6663 - val_prc: 0.1009\n",
      "Epoch 118/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1283 - Sensitivity: 0.0065 - tn: 112927.0000 - auc: 0.7526 - prc: 0.1500 - val_loss: 0.1470 - val_Sensitivity: 0.0158 - val_tn: 37568.0000 - val_auc: 0.6639 - val_prc: 0.1012\n",
      "Epoch 119/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1285 - Sensitivity: 0.0107 - tn: 112914.0000 - auc: 0.7508 - prc: 0.1518 - val_loss: 0.1485 - val_Sensitivity: 0.0279 - val_tn: 37553.0000 - val_auc: 0.6654 - val_prc: 0.1005\n",
      "Epoch 120/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1285 - Sensitivity: 0.0083 - tn: 112908.0000 - auc: 0.7503 - prc: 0.1500 - val_loss: 0.1469 - val_Sensitivity: 0.0173 - val_tn: 37561.0000 - val_auc: 0.6670 - val_prc: 0.1004\n"
     ]
    }
   ],
   "source": [
    "#split each training set into train/val, preserving 60/20/20 ratio, and fit the model\n",
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    #reset the model with untrained weights\n",
    "    model4 = keras.models.load_model('model4.h5')\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate auroc for each of the 5 folds\n",
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6737232279245302,\n",
       " 0.6688388867287957,\n",
       " 0.6833989422242278,\n",
       " 0.6745952381912016,\n",
       " 0.6860710197074006]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.677 (0.668-0.686)\n"
     ]
    }
   ],
   "source": [
    "#find the mean auroc and 95% CI\n",
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print(round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(dX_train[0], dy_train[0])\n",
    "def rf_feat_importance(model, X):\n",
    "    return pd.DataFrame({'cols':X.columns, 'imp':model.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "fi = rf_feat_importance(model, X)\n",
    "fi[:10]\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fit(dX_train[0], dy_train[0])\n",
    "fi = rf_feat_importance(model2, X)\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'lrpreds' (list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchen/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#train the logistic regression model, no regularization to create \"standard\" model, no validation set needed for LR (no early stopping), warm_start to False to reset model before each fit/test\n",
    "lrpreds = []\n",
    "model3 = LogisticRegression(penalty='none', warm_start=False)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))\n",
    "%store lrpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score the LR model on 5 folds\n",
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.633 (0.62-0.647)\n"
     ]
    }
   ],
   "source": [
    "#find the mean and 95% CI\n",
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.104 (0.094-0.113)\n",
      "Logistic Regression: 0.056 (0.051-0.061)\n"
     ]
    }
   ],
   "source": [
    "#write results to txt file\n",
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')\n",
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')\n",
    "with open('col_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_col_ann_tpr' (list)\n",
      "Stored 'mean_col_ann_fpr' (list)\n",
      "Stored 'mean_col_lr_tpr' (list)\n",
      "Stored 'mean_col_lr_fpr' (list)\n"
     ]
    }
   ],
   "source": [
    "#generate fpr and tpr to create ROC curves\n",
    "col_ann_tpr = []\n",
    "col_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_tpr.append(tpr)\n",
    "    col_ann_fpr.append(fpr)\n",
    "\n",
    "#sklearn's roc_curve function created fpr/tpr of different lengths (different number of thresholds)\n",
    "#averaging different length lists results in truncated ROC curves\n",
    "#randomly delete values to make each list the same length\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_tpr[x]))\n",
    "        col_ann_tpr[x] = np.delete(col_ann_tpr[x],ind)\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_fpr[x]))\n",
    "        col_ann_fpr[x] = np.delete(col_ann_fpr[x],ind)\n",
    "\n",
    "#find the mean tpr's and fpr's\n",
    "mean_col_ann_tpr = [np.mean(k) for k in zip(*col_ann_tpr)]\n",
    "mean_col_ann_fpr = [np.mean(k) for k in zip(*col_ann_fpr)]\n",
    "\n",
    "#save the means to use in the curves notebook\n",
    "%store mean_col_ann_tpr\n",
    "%store mean_col_ann_fpr\n",
    "\n",
    "#same process for LR\n",
    "col_lr_tpr = []\n",
    "col_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_tpr.append(tpr)\n",
    "    col_lr_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_tpr[x]))\n",
    "        col_lr_tpr[x] = np.delete(col_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_fpr[x]))\n",
    "        col_lr_fpr[x] = np.delete(col_lr_fpr[x],ind)\n",
    "\n",
    "mean_col_lr_tpr = [np.mean(k) for k in zip(*col_lr_tpr)]\n",
    "mean_col_lr_fpr = [np.mean(k) for k in zip(*col_lr_fpr)]\n",
    "%store mean_col_lr_tpr\n",
    "%store mean_col_lr_fpr\n",
    "\n",
    "#same process for precision-recall curves\n",
    "col_lr_rec = []\n",
    "col_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_rec.append(rec)\n",
    "    col_lr_prec.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38717\n",
      "38717\n",
      "38802\n",
      "38802\n",
      "38732\n",
      "38732\n",
      "38831\n",
      "38831\n",
      "38636\n",
      "38636\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(col_lr_rec[x]))\n",
    "    print(len(col_lr_prec[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_col_lr_rec' (list)\n",
      "Stored 'mean_col_lr_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_rec[x]) - 38600\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_rec[x]))\n",
    "        col_lr_rec[x] = np.delete(col_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_prec[x]) - 38600\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_prec[x]))\n",
    "        col_lr_prec[x] = np.delete(col_lr_prec[x],ind)\n",
    "\n",
    "mean_col_lr_rec = [np.mean(k) for k in zip(*col_lr_rec)]\n",
    "\n",
    "mean_col_lr_prec = [np.mean(k) for k in zip(*col_lr_prec)]\n",
    "%store mean_col_lr_rec\n",
    "%store mean_col_lr_prec\n",
    "col_ann_rec = []\n",
    "col_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_rec.append(rec)\n",
    "    col_ann_prec.append(prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38626\n",
      "38626\n",
      "38698\n",
      "38698\n",
      "38620\n",
      "38620\n",
      "38731\n",
      "38731\n",
      "38409\n",
      "38409\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(len(col_ann_rec[x]))\n",
    "    print(len(col_ann_prec[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_col_ann_rec' (list)\n",
      "Stored 'mean_col_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_rec[x]) - 38400\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_rec[x]))\n",
    "        col_ann_rec[x] = np.delete(col_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_prec[x]) - 38400\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_prec[x]))\n",
    "        col_ann_prec[x] = np.delete(col_ann_prec[x],ind)\n",
    "\n",
    "mean_col_ann_rec = [np.mean(k) for k in zip(*col_ann_rec)]\n",
    "\n",
    "mean_col_ann_prec = [np.mean(k) for k in zip(*col_ann_prec)]\n",
    "%store mean_col_ann_rec\n",
    "%store mean_col_ann_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import specificity_score\n",
    "thresh = np.arange(0, 1, 0.01)\n",
    "#calculate recall at 10 thresholds\n",
    "annrecall_list = []\n",
    "for i in thresh:\n",
    "    annrecall_list.append(recall_score(dy_test[1], annpreds[1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "annspec_list = []\n",
    "for i in thresh:\n",
    "    annspec_list.append(specificity_score(dy_test[1], annpreds[1] > i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, annrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, annspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "thresh = np.arange(0, 1, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculate recall at 10 thresholds\n",
    "lrrecall_list = []\n",
    "for i in thresh:\n",
    "    lrrecall_list.append(recall_score(dy_test[1], lrpreds[1][:,1] > i))\n",
    "#calculate spec at 10 thresholds\n",
    "lrspec_list = []\n",
    "for i in thresh:\n",
    "    lrspec_list.append(specificity_score(dy_test[1], lrpreds[1][:,1] > i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plot recall vs threshold\n",
    "plt.plot(thresh, lrrecall_list, label = 'Sensitivity')\n",
    "plt.plot(thresh, lrspec_list, color = 'red', label = 'Specificity')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Sensitivity and Specificity')\n",
    "plt.title('Sensitivity and Specificity by Threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708204334365325\n",
      "0.5025372618826217\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(recall_score(dy_test[1], lrpreds[1][:,1] > 0.028))\n",
    "print(specificity_score(dy_test[1], lrpreds[1][:,1] > 0.028))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6795665634674922\n",
      "0.5667791386593692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(recall_score(dy_test[1], annpreds[1] > 0.04))\n",
    "print(specificity_score(dy_test[1], annpreds[1] > 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "\n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for Delong test adapted from https://biasedml.com/roc-comparison/\n",
    "p = []\n",
    "z = []\n",
    "# ANN vs LR\n",
    "for x in range(1,5):\n",
    "    preds_A = annpreds[x]\n",
    "    preds_B = lrpreds[x][:,1]\n",
    "    actual = dy_test[x]\n",
    "\n",
    "    actual = actual.array\n",
    "\n",
    "    def group_preds_by_label(preds, actual):\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "\n",
    "\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "    # Two tailed test\n",
    "    z.append(z_score(var_A, var_B, covar_AB, auc_A, auc_B))\n",
    "    p.append(st.norm.sf(abs(z[x-1]))*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005433035551304245,\n",
       " 1.3355345210816105e-14,\n",
       " 1.3253357733890525e-15,\n",
       " 1.0699467525922371e-14]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00013582588878895114"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p)/(len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_ann_tpr = []\n",
    "col_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_tpr.append(tpr)\n",
    "    col_ann_fpr.append(fpr)\n",
    "col_ann_tpr_array = [np.array(x) for x in col_ann_tpr]\n",
    "mean_col_ann_tpr = [np.mean(k) for k in zip(*col_ann_tpr_array)]\n",
    "col_ann_fpr_array = [np.array(x) for x in col_ann_fpr]\n",
    "mean_col_ann_fpr = [np.mean(k) for k in zip(*col_ann_fpr_array)]\n",
    "%store mean_col_ann_tpr\n",
    "%store mean_col_ann_fpr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_lr_tpr = []\n",
    "col_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_tpr.append(tpr)\n",
    "    col_lr_fpr.append(fpr)\n",
    "col_lr_tpr_array = [np.array(x) for x in col_lr_tpr]\n",
    "mean_col_lr_tpr = [np.mean(k) for k in zip(*col_lr_tpr_array)]\n",
    "col_lr_fpr_array = [np.array(x) for x in col_lr_fpr]\n",
    "mean_col_lr_fpr = [np.mean(k) for k in zip(*col_lr_fpr_array)]\n",
    "%store mean_col_lr_tpr\n",
    "%store mean_col_lr_fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_ann_rec = []\n",
    "col_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_rec.append(rec)\n",
    "    col_ann_prec.append(prec)\n",
    "col_ann_rec_array = [np.array(x) for x in col_ann_rec]\n",
    "mean_col_ann_rec = [np.mean(k) for k in zip(*col_ann_rec_array)]\n",
    "col_ann_prec_array = [np.array(x) for x in col_ann_prec]\n",
    "mean_col_ann_prec = [np.mean(k) for k in zip(*col_ann_prec_array)]\n",
    "%store mean_col_ann_rec\n",
    "%store mean_col_ann_prec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_lr_rec = []\n",
    "col_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_rec.append(rec)\n",
    "    col_lr_prec.append(prec)\n",
    "col_lr_rec_array = [np.array(x) for x in col_lr_rec]\n",
    "mean_col_lr_rec = [np.mean(k) for k in zip(*col_lr_rec_array)]\n",
    "col_lr_prec_array = [np.array(x) for x in col_lr_prec]\n",
    "mean_col_lr_prec = [np.mean(k) for k in zip(*col_lr_prec_array)]\n",
    "%store mean_col_lr_rec\n",
    "%store mean_col_lr_prec\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
