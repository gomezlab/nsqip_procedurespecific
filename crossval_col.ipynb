{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats as st\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 200\n",
    "data = pd.read_csv('clean_col.csv', index_col='CASEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COL_STEROID</th>\n",
       "      <th>COL_MECH_BOWEL_PREP</th>\n",
       "      <th>COL_ORAL_ANTIBIOTIC</th>\n",
       "      <th>COL_CHEMO</th>\n",
       "      <th>COL_INDICATION</th>\n",
       "      <th>COL_ICD9_INDICATION</th>\n",
       "      <th>COL_EMERGENT</th>\n",
       "      <th>COL_ICD9_EMERGENT</th>\n",
       "      <th>COL_APPROACH</th>\n",
       "      <th>COL_ANASTOMOTIC</th>\n",
       "      <th>SEX</th>\n",
       "      <th>PRNCPTX</th>\n",
       "      <th>CPT</th>\n",
       "      <th>WORKRVU</th>\n",
       "      <th>INOUT</th>\n",
       "      <th>TRANST</th>\n",
       "      <th>AGE</th>\n",
       "      <th>OPERYR</th>\n",
       "      <th>ANESTHES</th>\n",
       "      <th>SURGSPEC</th>\n",
       "      <th>ELECTSURG</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>DYSPNEA</th>\n",
       "      <th>FNSTATUS2</th>\n",
       "      <th>VENTILAT</th>\n",
       "      <th>HXCOPD</th>\n",
       "      <th>ASCITES</th>\n",
       "      <th>HXCHF</th>\n",
       "      <th>HYPERMED</th>\n",
       "      <th>RENAFAIL</th>\n",
       "      <th>DIALYSIS</th>\n",
       "      <th>DISCANCR</th>\n",
       "      <th>WNDINF</th>\n",
       "      <th>STEROID</th>\n",
       "      <th>WTLOSS</th>\n",
       "      <th>BLEEDDIS</th>\n",
       "      <th>TRANSFUS</th>\n",
       "      <th>PRSEPIS</th>\n",
       "      <th>PRSODM</th>\n",
       "      <th>PRBUN</th>\n",
       "      <th>PRCREAT</th>\n",
       "      <th>PRWBC</th>\n",
       "      <th>PRHCT</th>\n",
       "      <th>PRPLATE</th>\n",
       "      <th>EMERGNCY</th>\n",
       "      <th>WNDCLAS</th>\n",
       "      <th>ASACLAS</th>\n",
       "      <th>OPTIME</th>\n",
       "      <th>HTOODAY</th>\n",
       "      <th>SSSIPATOS</th>\n",
       "      <th>DSSIPATOS</th>\n",
       "      <th>OSSIPATOS</th>\n",
       "      <th>PNAPATOS</th>\n",
       "      <th>VENTPATOS</th>\n",
       "      <th>UTIPATOS</th>\n",
       "      <th>SEPSISPATOS</th>\n",
       "      <th>SEPSHOCKPATOS</th>\n",
       "      <th>COL_ICD10_INDICATION</th>\n",
       "      <th>COL_ICD10_EMERGENT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>RACE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASEID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6551967</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.8542</td>\n",
       "      <td>97.975872</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>8.4034</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>7.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>28.497449</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552344</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>83.914520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43.4</td>\n",
       "      <td>373.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>31.754826</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552431</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6764</td>\n",
       "      <td>81.646560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>4.7</td>\n",
       "      <td>37.7</td>\n",
       "      <td>309.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>29.052438</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552941</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.4986</td>\n",
       "      <td>47.627160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>6.7</td>\n",
       "      <td>37.2</td>\n",
       "      <td>453.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>21.207195</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>68.945984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>7.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "      <td>123</td>\n",
       "      <td>26.090451</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         COL_STEROID  COL_MECH_BOWEL_PREP  COL_ORAL_ANTIBIOTIC  COL_CHEMO  \\\n",
       "CASEID                                                                      \n",
       "6551967            0                    2                    0          0   \n",
       "6552344            0                    1                    1          0   \n",
       "6552431            0                    2                    2          0   \n",
       "6552941            2                    0                    2          0   \n",
       "6552982            0                    0                    0          0   \n",
       "\n",
       "         COL_INDICATION  COL_ICD9_INDICATION  COL_EMERGENT  COL_ICD9_EMERGENT  \\\n",
       "CASEID                                                                          \n",
       "6551967               3                  451             6                155   \n",
       "6552344               8                  451             6                155   \n",
       "6552431               2                  451             6                155   \n",
       "6552941               5                  451             6                155   \n",
       "6552982               3                  451             6                155   \n",
       "\n",
       "         COL_APPROACH  COL_ANASTOMOTIC  SEX  PRNCPTX  CPT  WORKRVU  INOUT  \\\n",
       "CASEID                                                                      \n",
       "6551967             0                0    2        6    4        3      0   \n",
       "6552344             0                0    1        8    5        2      0   \n",
       "6552431             0                0    1        7    6        5      0   \n",
       "6552941             0                0    1        6    4        3      0   \n",
       "6552982             0                1    1        6    4        3      0   \n",
       "\n",
       "         TRANST   AGE  OPERYR  ANESTHES  SURGSPEC  ELECTSURG  HEIGHT  \\\n",
       "CASEID                                                                 \n",
       "6551967       1  64.0       5         1         1          2  1.8542   \n",
       "6552344       1  44.0       5         1         1          2  1.6256   \n",
       "6552431       1  30.0       5         1         1          2  1.6764   \n",
       "6552941       1  45.0       5         1         1          2  1.4986   \n",
       "6552982       1  50.0       5         1         1          2  1.6256   \n",
       "\n",
       "            WEIGHT  DIABETES  SMOKE  DYSPNEA  FNSTATUS2  VENTILAT  HXCOPD  \\\n",
       "CASEID                                                                      \n",
       "6551967  97.975872         2      0        2          0         0       0   \n",
       "6552344  83.914520         1      0        2          0         0       0   \n",
       "6552431  81.646560         1      0        2          0         0       0   \n",
       "6552941  47.627160         1      0        2          0         0       0   \n",
       "6552982  68.945984         1      0        2          0         0       0   \n",
       "\n",
       "         ASCITES  HXCHF  HYPERMED  RENAFAIL  DIALYSIS  DISCANCR  WNDINF  \\\n",
       "CASEID                                                                    \n",
       "6551967        0      0         2         0         0         0       0   \n",
       "6552344        0      0         2         0         0         0       0   \n",
       "6552431        0      0         0         0         0         0       0   \n",
       "6552941        0      0         0         0         0         0       0   \n",
       "6552982        0      0         0         0         0         0       0   \n",
       "\n",
       "         STEROID  WTLOSS  BLEEDDIS  TRANSFUS  PRSEPIS  PRSODM    PRBUN  \\\n",
       "CASEID                                                                   \n",
       "6551967        0       0         0         0        0   139.0   8.4034   \n",
       "6552344        0       0         0         0        0   139.0  14.0000   \n",
       "6552431        0       0         0         0        0   138.0   3.0000   \n",
       "6552941        2       0         0         0        0   141.0  13.0000   \n",
       "6552982        0       0         0         0        0   138.0  11.0000   \n",
       "\n",
       "         PRCREAT  PRWBC  PRHCT  PRPLATE  EMERGNCY  WNDCLAS  ASACLAS  OPTIME  \\\n",
       "CASEID                                                                        \n",
       "6551967   0.8032    7.1   38.0    190.0         0        1        2   121.0   \n",
       "6552344   0.8600   11.0   43.4    373.0         0        1        1    64.0   \n",
       "6552431   0.6000    4.7   37.7    309.0         0        2        1    99.0   \n",
       "6552941   0.5700    6.7   37.2    453.0         0        1        1    77.0   \n",
       "6552982   0.7300    7.3   29.7    432.0         0        1        1   134.0   \n",
       "\n",
       "         HTOODAY  SSSIPATOS  DSSIPATOS  OSSIPATOS  PNAPATOS  VENTPATOS  \\\n",
       "CASEID                                                                   \n",
       "6551967      0.0          0          0          0         0          0   \n",
       "6552344      0.0          0          0          0         0          0   \n",
       "6552431      0.0          0          0          0         0          0   \n",
       "6552941      0.0          0          0          0         0          0   \n",
       "6552982      0.0          0          0          0         0          0   \n",
       "\n",
       "         UTIPATOS  SEPSISPATOS  SEPSHOCKPATOS  COL_ICD10_INDICATION  \\\n",
       "CASEID                                                                \n",
       "6551967         0            0              0                   445   \n",
       "6552344         0            0              0                   445   \n",
       "6552431         0            0              0                   445   \n",
       "6552941         0            0              0                   445   \n",
       "6552982         0            0              0                   445   \n",
       "\n",
       "         COL_ICD10_EMERGENT        BMI  RACE  \n",
       "CASEID                                        \n",
       "6551967                 123  28.497449     5  \n",
       "6552344                 123  31.754826     6  \n",
       "6552431                 123  29.052438     6  \n",
       "6552941                 123  21.207195     6  \n",
       "6552982                 123  26.090451     5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.drop('CASEID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define outcome \"y\" and predictors \"X\"\n",
    "y = data['COL_ANASTOMOTIC']\n",
    "X = data.drop(['COL_ANASTOMOTIC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 5 folds of cross-validation, with 5 train/test sets, export to csv\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "fold_no=0\n",
    "for train_index, test_index in skf.split(data, y):\n",
    "    train = data.loc[train_index,:]\n",
    "    test = data.loc[test_index,:]\n",
    "    train_filename = 'train' + str(fold_no) + '.csv'\n",
    "    test_filename = 'test' + str(fold_no) + '.csv' \n",
    "    train.to_csv('splits/' + train_filename, index=False)\n",
    "    test.to_csv('splits/' + test_filename, index=False) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv's back into a dictionary, naming them train or test 1-5\n",
    "d = {}\n",
    "for x in range(0,5):\n",
    "    d['train{}'.format(x)] = pd.read_csv('splits/train{}.csv'.format(x), low_memory=False)\n",
    "    d['test{}'.format(x)] = pd.read_csv('splits/test{}.csv'.format(x), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of training data for X and y\n",
    "dX_train = []\n",
    "dy_train = []\n",
    "for x in d:\n",
    "    if 'train' in x:\n",
    "        dX_train.append(d[x].drop(columns=['COL_ANASTOMOTIC'], axis=1))\n",
    "        dy_train.append(d[x]['COL_ANASTOMOTIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of test data for X and y\n",
    "dX_test = []\n",
    "dy_test = []\n",
    "for x in d:\n",
    "    if 'test' in x:\n",
    "        dX_test.append(d[x].drop(columns=['COL_ANASTOMOTIC'], axis=1))\n",
    "        dy_test.append(d[x]['COL_ANASTOMOTIC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfpreds = []\n",
    "xgbpreds = []\n",
    "model = RandomForestClassifier(n_estimators=1250, min_samples_split=2, min_samples_leaf=8, max_features='auto', max_depth=20, bootstrap=True)\n",
    "model2 = XGBClassifier(n_estimators=50, subsample=0.6, min_child_weight=10, max_depth=6, learning_rate=0.1, colsample_bytree=0.8)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model.fit(X, y)\n",
    "    model2.fit(X, y)\n",
    "    rfpreds.append(model.predict_proba(X_test))\n",
    "    xgbpreds.append(model2.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store rfpreds\n",
    "%store xgbpreds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], rfpreds[x][:,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in range(0,5):\n",
    "    print(roc_auc_score(dy_test[x], xgbpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define keras model with 2 layers, 1000 nodes, followed by batch norm and dropout\n",
    "from tensorflow import keras\n",
    "input_shape = [X.shape[1]]\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "for _ in range(2):\n",
    "    model4.add(keras.layers.Dense(1000))\n",
    "    model4.add(keras.layers.BatchNormalization())\n",
    "    model4.add(keras.layers.Dropout(0.8))\n",
    "    model4.add(keras.layers.Activation(\"relu\"))\n",
    "model4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "metrics = [keras.metrics.Recall(name='Sensitivity'), keras.metrics.TrueNegatives(name='tn'), keras.metrics.AUC(name='auc'), keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=metrics,)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.00001,\n",
    "    restore_best_weights=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "229/229 [==============================] - 6s 15ms/step - loss: 0.2828 - Sensitivity: 0.0680 - tn: 107075.0000 - auc: 0.5062 - prc: 0.0362 - val_loss: 0.2309 - val_Sensitivity: 0.0521 - val_tn: 37468.0000 - val_auc: 0.6568 - val_prc: 0.0830\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1922 - Sensitivity: 0.0221 - tn: 112167.0000 - auc: 0.5396 - prc: 0.0451 - val_loss: 0.2064 - val_Sensitivity: 0.1104 - val_tn: 37179.0000 - val_auc: 0.6626 - val_prc: 0.0867\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1790 - Sensitivity: 0.0239 - tn: 112386.0000 - auc: 0.5609 - prc: 0.0489 - val_loss: 0.1983 - val_Sensitivity: 0.1041 - val_tn: 37265.0000 - val_auc: 0.6609 - val_prc: 0.0908\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1686 - Sensitivity: 0.0210 - tn: 112542.0000 - auc: 0.5755 - prc: 0.0517 - val_loss: 0.2131 - val_Sensitivity: 0.1041 - val_tn: 37262.0000 - val_auc: 0.6564 - val_prc: 0.0906\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1604 - Sensitivity: 0.0118 - tn: 112654.0000 - auc: 0.5891 - prc: 0.0540 - val_loss: 0.1981 - val_Sensitivity: 0.0994 - val_tn: 37338.0000 - val_auc: 0.6575 - val_prc: 0.0913\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1543 - Sensitivity: 0.0105 - tn: 112774.0000 - auc: 0.5994 - prc: 0.0589 - val_loss: 0.2036 - val_Sensitivity: 0.1009 - val_tn: 37327.0000 - val_auc: 0.6553 - val_prc: 0.0918\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1506 - Sensitivity: 0.0082 - tn: 112803.0000 - auc: 0.6156 - prc: 0.0666 - val_loss: 0.2050 - val_Sensitivity: 0.1025 - val_tn: 37284.0000 - val_auc: 0.6554 - val_prc: 0.0903\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1474 - Sensitivity: 0.0062 - tn: 112837.0000 - auc: 0.6262 - prc: 0.0698 - val_loss: 0.2048 - val_Sensitivity: 0.1033 - val_tn: 37277.0000 - val_auc: 0.6558 - val_prc: 0.0894\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1465 - Sensitivity: 0.0013 - tn: 112859.0000 - auc: 0.6238 - prc: 0.0714 - val_loss: 0.2031 - val_Sensitivity: 0.1002 - val_tn: 37315.0000 - val_auc: 0.6561 - val_prc: 0.0893\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1451 - Sensitivity: 0.0015 - tn: 112873.0000 - auc: 0.6336 - prc: 0.0751 - val_loss: 0.1972 - val_Sensitivity: 0.0970 - val_tn: 37362.0000 - val_auc: 0.6574 - val_prc: 0.0885\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1439 - Sensitivity: 0.0021 - tn: 112877.0000 - auc: 0.6399 - prc: 0.0770 - val_loss: 0.2050 - val_Sensitivity: 0.0970 - val_tn: 37359.0000 - val_auc: 0.6577 - val_prc: 0.0886\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1436 - Sensitivity: 5.1295e-04 - tn: 112881.0000 - auc: 0.6413 - prc: 0.0769 - val_loss: 0.2014 - val_Sensitivity: 0.0978 - val_tn: 37353.0000 - val_auc: 0.6593 - val_prc: 0.0893\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1426 - Sensitivity: 7.6943e-04 - tn: 112889.0000 - auc: 0.6455 - prc: 0.0836 - val_loss: 0.1882 - val_Sensitivity: 0.0678 - val_tn: 37481.0000 - val_auc: 0.6610 - val_prc: 0.0903\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1421 - Sensitivity: 0.0000e+00 - tn: 112886.0000 - auc: 0.6496 - prc: 0.0843 - val_loss: 0.1880 - val_Sensitivity: 0.0781 - val_tn: 37434.0000 - val_auc: 0.6618 - val_prc: 0.0903\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1421 - Sensitivity: 0.0013 - tn: 112890.0000 - auc: 0.6492 - prc: 0.0847 - val_loss: 0.1878 - val_Sensitivity: 0.0718 - val_tn: 37469.0000 - val_auc: 0.6614 - val_prc: 0.0889\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1415 - Sensitivity: 0.0000e+00 - tn: 112887.0000 - auc: 0.6551 - prc: 0.0839 - val_loss: 0.1942 - val_Sensitivity: 0.0868 - val_tn: 37389.0000 - val_auc: 0.6602 - val_prc: 0.0884\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1414 - Sensitivity: 5.1295e-04 - tn: 112887.0000 - auc: 0.6556 - prc: 0.0860 - val_loss: 0.1842 - val_Sensitivity: 0.0473 - val_tn: 37532.0000 - val_auc: 0.6643 - val_prc: 0.0902\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1407 - Sensitivity: 2.5648e-04 - tn: 112890.0000 - auc: 0.6611 - prc: 0.0865 - val_loss: 0.1807 - val_Sensitivity: 0.0103 - val_tn: 37627.0000 - val_auc: 0.6632 - val_prc: 0.0892\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1405 - Sensitivity: 5.1295e-04 - tn: 112889.0000 - auc: 0.6615 - prc: 0.0881 - val_loss: 0.1772 - val_Sensitivity: 0.0063 - val_tn: 37640.0000 - val_auc: 0.6640 - val_prc: 0.0898\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1401 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.6623 - prc: 0.0915 - val_loss: 0.1760 - val_Sensitivity: 0.0000e+00 - val_tn: 37661.0000 - val_auc: 0.6639 - val_prc: 0.0911\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1401 - Sensitivity: 5.1295e-04 - tn: 112892.0000 - auc: 0.6656 - prc: 0.0898 - val_loss: 0.1843 - val_Sensitivity: 0.0568 - val_tn: 37503.0000 - val_auc: 0.6640 - val_prc: 0.0909\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1399 - Sensitivity: 2.5648e-04 - tn: 112889.0000 - auc: 0.6659 - prc: 0.0920 - val_loss: 0.1767 - val_Sensitivity: 0.0016 - val_tn: 37653.0000 - val_auc: 0.6648 - val_prc: 0.0912\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1395 - Sensitivity: 2.5648e-04 - tn: 112890.0000 - auc: 0.6697 - prc: 0.0924 - val_loss: 0.1718 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6652 - val_prc: 0.0916\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1396 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6674 - prc: 0.0911 - val_loss: 0.1751 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6660 - val_prc: 0.0927\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1390 - Sensitivity: 0.0000e+00 - tn: 112891.0000 - auc: 0.6741 - prc: 0.0932 - val_loss: 0.1730 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6664 - val_prc: 0.0932\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1388 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6782 - prc: 0.0945 - val_loss: 0.1714 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6647 - val_prc: 0.0905\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1392 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.6731 - prc: 0.0937 - val_loss: 0.1714 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6665 - val_prc: 0.0930\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1387 - Sensitivity: 2.5648e-04 - tn: 112887.0000 - auc: 0.6776 - prc: 0.0960 - val_loss: 0.1652 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6696 - val_prc: 0.0943\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1383 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.6792 - prc: 0.0985 - val_loss: 0.1658 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6681 - val_prc: 0.0940\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1376 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6884 - prc: 0.0984 - val_loss: 0.1666 - val_Sensitivity: 0.0016 - val_tn: 37658.0000 - val_auc: 0.6690 - val_prc: 0.0917\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1385 - Sensitivity: 0.0000e+00 - tn: 112889.0000 - auc: 0.6802 - prc: 0.0961 - val_loss: 0.1667 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6690 - val_prc: 0.0945\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1379 - Sensitivity: 7.6943e-04 - tn: 112892.0000 - auc: 0.6824 - prc: 0.1026 - val_loss: 0.1600 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6690 - val_prc: 0.0951\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1381 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6808 - prc: 0.1019 - val_loss: 0.1622 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6705 - val_prc: 0.0974\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1379 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.6853 - prc: 0.0982 - val_loss: 0.1613 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6718 - val_prc: 0.0952\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1377 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.6823 - prc: 0.1052 - val_loss: 0.1602 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6704 - val_prc: 0.0935\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1378 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6863 - prc: 0.0985 - val_loss: 0.1598 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6717 - val_prc: 0.0976\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1374 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6891 - prc: 0.1031 - val_loss: 0.1596 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6720 - val_prc: 0.0973\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1377 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6848 - prc: 0.1005 - val_loss: 0.1603 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6722 - val_prc: 0.0953\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1376 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6878 - prc: 0.1014 - val_loss: 0.1611 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6703 - val_prc: 0.0947\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1374 - Sensitivity: 0.0000e+00 - tn: 112891.0000 - auc: 0.6894 - prc: 0.1018 - val_loss: 0.1611 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6732 - val_prc: 0.0942\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1369 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6935 - prc: 0.1033 - val_loss: 0.1565 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6737 - val_prc: 0.0943\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1370 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6926 - prc: 0.1023 - val_loss: 0.1578 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6740 - val_prc: 0.0974\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1373 - Sensitivity: 0.0000e+00 - tn: 112890.0000 - auc: 0.6898 - prc: 0.1009 - val_loss: 0.1532 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6738 - val_prc: 0.0972\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1369 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.6929 - prc: 0.1048 - val_loss: 0.1586 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6718 - val_prc: 0.0951\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1369 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.6921 - prc: 0.1021 - val_loss: 0.1544 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6733 - val_prc: 0.0950\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1363 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6983 - prc: 0.1073 - val_loss: 0.1532 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6732 - val_prc: 0.0947\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1363 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.6990 - prc: 0.1068 - val_loss: 0.1541 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6742 - val_prc: 0.0963\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1370 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.6923 - prc: 0.1063 - val_loss: 0.1570 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6729 - val_prc: 0.0953\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1368 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6936 - prc: 0.1051 - val_loss: 0.1510 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6756 - val_prc: 0.0959\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1366 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.6952 - prc: 0.1047 - val_loss: 0.1551 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6739 - val_prc: 0.0962\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1365 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6958 - prc: 0.1052 - val_loss: 0.1513 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6749 - val_prc: 0.0967\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6978 - prc: 0.1077 - val_loss: 0.1546 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0963\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1358 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7039 - prc: 0.1088 - val_loss: 0.1518 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6748 - val_prc: 0.0967\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1362 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.6997 - prc: 0.1089 - val_loss: 0.1526 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6745 - val_prc: 0.0972\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1358 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7034 - prc: 0.1097 - val_loss: 0.1517 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6743 - val_prc: 0.0985\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7056 - prc: 0.1092 - val_loss: 0.1513 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6762 - val_prc: 0.0965\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1364 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.6989 - prc: 0.1071 - val_loss: 0.1506 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6738 - val_prc: 0.0977\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7050 - prc: 0.1107 - val_loss: 0.1493 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6758 - val_prc: 0.0976\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1357 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7038 - prc: 0.1122 - val_loss: 0.1485 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6765 - val_prc: 0.0996\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7064 - prc: 0.1106 - val_loss: 0.1510 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.1005\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7058 - prc: 0.1135 - val_loss: 0.1490 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6770 - val_prc: 0.0981\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1358 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7038 - prc: 0.1093 - val_loss: 0.1511 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6779 - val_prc: 0.0983\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1356 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7060 - prc: 0.1103 - val_loss: 0.1499 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0964\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1357 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7033 - prc: 0.1103 - val_loss: 0.1502 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6772 - val_prc: 0.0985\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1355 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.7063 - prc: 0.1114 - val_loss: 0.1482 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6737 - val_prc: 0.0991\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1351 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7111 - prc: 0.1140 - val_loss: 0.1497 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6734 - val_prc: 0.0973\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7101 - prc: 0.1090 - val_loss: 0.1478 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6768 - val_prc: 0.0986\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7068 - prc: 0.1133 - val_loss: 0.1494 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6755 - val_prc: 0.0984\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7116 - prc: 0.1151 - val_loss: 0.1477 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6774 - val_prc: 0.0986\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.7120 - prc: 0.1154 - val_loss: 0.1474 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6752 - val_prc: 0.0968\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7114 - prc: 0.1133 - val_loss: 0.1486 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6741 - val_prc: 0.0964\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7108 - prc: 0.1159 - val_loss: 0.1480 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6771 - val_prc: 0.0975\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7108 - prc: 0.1159 - val_loss: 0.1480 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6750 - val_prc: 0.0967\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7130 - prc: 0.1133 - val_loss: 0.1475 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6763 - val_prc: 0.0984\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1351 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7088 - prc: 0.1117 - val_loss: 0.1521 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6728 - val_prc: 0.0953\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1345 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7156 - prc: 0.1153 - val_loss: 0.1492 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0958\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7123 - prc: 0.1158 - val_loss: 0.1475 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6763 - val_prc: 0.0980\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7130 - prc: 0.1154 - val_loss: 0.1476 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6755 - val_prc: 0.0964\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1346 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7163 - prc: 0.1148 - val_loss: 0.1466 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6761 - val_prc: 0.0979\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1345 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7141 - prc: 0.1170 - val_loss: 0.1463 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6766 - val_prc: 0.0988\n",
      "Epoch 81/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7112 - prc: 0.1167 - val_loss: 0.1509 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6745 - val_prc: 0.0984\n",
      "Epoch 82/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7182 - prc: 0.1193 - val_loss: 0.1469 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6745 - val_prc: 0.0967\n",
      "Epoch 83/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1344 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7148 - prc: 0.1191 - val_loss: 0.1480 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6741 - val_prc: 0.0971\n",
      "Epoch 84/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0000e+00 - tn: 112890.0000 - auc: 0.7224 - prc: 0.1195 - val_loss: 0.1468 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6757 - val_prc: 0.0985\n",
      "Epoch 85/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1345 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7144 - prc: 0.1188 - val_loss: 0.1481 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6742 - val_prc: 0.0976\n",
      "Epoch 86/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1343 - Sensitivity: 0.0000e+00 - tn: 112890.0000 - auc: 0.7168 - prc: 0.1197 - val_loss: 0.1483 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6754 - val_prc: 0.0960\n",
      "Epoch 87/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1339 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7203 - prc: 0.1202 - val_loss: 0.1456 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6764 - val_prc: 0.0986\n",
      "Epoch 88/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7245 - prc: 0.1217 - val_loss: 0.1465 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0968\n",
      "Epoch 89/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1338 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7227 - prc: 0.1193 - val_loss: 0.1465 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6786 - val_prc: 0.1008\n",
      "Epoch 90/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7168 - prc: 0.1190 - val_loss: 0.1482 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6768 - val_prc: 0.0968\n",
      "Epoch 91/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1344 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7177 - prc: 0.1179 - val_loss: 0.1442 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6773 - val_prc: 0.1020\n",
      "Epoch 92/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1341 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7183 - prc: 0.1188 - val_loss: 0.1482 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6773 - val_prc: 0.0993\n",
      "Epoch 93/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1334 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7252 - prc: 0.1201 - val_loss: 0.1459 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6752 - val_prc: 0.1013\n",
      "Epoch 94/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1334 - Sensitivity: 0.0000e+00 - tn: 112893.0000 - auc: 0.7238 - prc: 0.1233 - val_loss: 0.1456 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6760 - val_prc: 0.0990\n",
      "Epoch 95/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1344 - Sensitivity: 0.0000e+00 - tn: 112892.0000 - auc: 0.7152 - prc: 0.1195 - val_loss: 0.1496 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6753 - val_prc: 0.0986\n",
      "Epoch 96/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1334 - Sensitivity: 7.6943e-04 - tn: 112893.0000 - auc: 0.7255 - prc: 0.1222 - val_loss: 0.1467 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6756 - val_prc: 0.0981\n",
      "Epoch 97/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1340 - Sensitivity: 2.5648e-04 - tn: 112893.0000 - auc: 0.7199 - prc: 0.1227 - val_loss: 0.1481 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6745 - val_prc: 0.0984\n",
      "Epoch 98/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1336 - Sensitivity: 5.1295e-04 - tn: 112890.0000 - auc: 0.7242 - prc: 0.1241 - val_loss: 0.1465 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6760 - val_prc: 0.0995\n",
      "Epoch 99/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1338 - Sensitivity: 2.5648e-04 - tn: 112891.0000 - auc: 0.7213 - prc: 0.1213 - val_loss: 0.1451 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6757 - val_prc: 0.1003\n",
      "Epoch 100/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1336 - Sensitivity: 0.0000e+00 - tn: 112890.0000 - auc: 0.7231 - prc: 0.1219 - val_loss: 0.1449 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6769 - val_prc: 0.0997\n",
      "Epoch 101/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1338 - Sensitivity: 2.5648e-04 - tn: 112892.0000 - auc: 0.7229 - prc: 0.1195 - val_loss: 0.1457 - val_Sensitivity: 0.0000e+00 - val_tn: 37663.0000 - val_auc: 0.6752 - val_prc: 0.0991\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1373 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6887 - prc: 0.1109 - val_loss: 0.1458 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7101 - val_prc: 0.1191\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.6958 - prc: 0.1101 - val_loss: 0.1411 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7113 - val_prc: 0.1192\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1363 - Sensitivity: 0.0000e+00 - tn: 112881.0000 - auc: 0.6981 - prc: 0.1151 - val_loss: 0.1398 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7102 - val_prc: 0.1167\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1367 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.6950 - prc: 0.1111 - val_loss: 0.1419 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7056 - val_prc: 0.1154\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1358 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7050 - prc: 0.1114 - val_loss: 0.1418 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7070 - val_prc: 0.1161\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1360 - Sensitivity: 0.0000e+00 - tn: 112883.0000 - auc: 0.7025 - prc: 0.1134 - val_loss: 0.1411 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7060 - val_prc: 0.1164\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1358 - Sensitivity: 0.0000e+00 - tn: 112882.0000 - auc: 0.7016 - prc: 0.1150 - val_loss: 0.1409 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7057 - val_prc: 0.1148\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 5.1164e-04 - tn: 112881.0000 - auc: 0.7087 - prc: 0.1169 - val_loss: 0.1405 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7046 - val_prc: 0.1155\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1360 - Sensitivity: 2.5582e-04 - tn: 112882.0000 - auc: 0.7002 - prc: 0.1152 - val_loss: 0.1419 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7031 - val_prc: 0.1137\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1356 - Sensitivity: 0.0000e+00 - tn: 112881.0000 - auc: 0.7048 - prc: 0.1181 - val_loss: 0.1413 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7033 - val_prc: 0.1149\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 2.5582e-04 - tn: 112883.0000 - auc: 0.7054 - prc: 0.1165 - val_loss: 0.1408 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7009 - val_prc: 0.1140\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112881.0000 - auc: 0.7103 - prc: 0.1193 - val_loss: 0.1419 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7010 - val_prc: 0.1136\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1352 - Sensitivity: 2.5582e-04 - tn: 112881.0000 - auc: 0.7076 - prc: 0.1179 - val_loss: 0.1438 - val_Sensitivity: 0.0000e+00 - val_tn: 37673.0000 - val_auc: 0.7007 - val_prc: 0.1136\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1362 - Sensitivity: 2.5800e-04 - tn: 112914.0000 - auc: 0.6932 - prc: 0.1084 - val_loss: 0.1416 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7175 - val_prc: 0.1162\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1367 - Sensitivity: 2.5800e-04 - tn: 112911.0000 - auc: 0.6893 - prc: 0.1041 - val_loss: 0.1429 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7144 - val_prc: 0.1184\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1362 - Sensitivity: 0.0000e+00 - tn: 112914.0000 - auc: 0.6947 - prc: 0.1058 - val_loss: 0.1430 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7133 - val_prc: 0.1165\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1359 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6955 - prc: 0.1075 - val_loss: 0.1434 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7132 - val_prc: 0.1152\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1354 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.7009 - prc: 0.1092 - val_loss: 0.1400 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7132 - val_prc: 0.1139\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1357 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.6981 - prc: 0.1087 - val_loss: 0.1431 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7093 - val_prc: 0.1159\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1361 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6926 - prc: 0.1067 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7104 - val_prc: 0.1134\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6991 - prc: 0.1116 - val_loss: 0.1442 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7089 - val_prc: 0.1137\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6978 - prc: 0.1082 - val_loss: 0.1430 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7103 - val_prc: 0.1151\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1355 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.6986 - prc: 0.1099 - val_loss: 0.1428 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7056 - val_prc: 0.1124\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.7061 - prc: 0.1128 - val_loss: 0.1425 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7070 - val_prc: 0.1126\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1347 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.7068 - prc: 0.1121 - val_loss: 0.1437 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7058 - val_prc: 0.1122\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1351 - Sensitivity: 0.0000e+00 - tn: 112915.0000 - auc: 0.7044 - prc: 0.1100 - val_loss: 0.1445 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7046 - val_prc: 0.1114\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.7029 - prc: 0.1097 - val_loss: 0.1427 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7057 - val_prc: 0.1111\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1342 - Sensitivity: 0.0000e+00 - tn: 112916.0000 - auc: 0.7119 - prc: 0.1143 - val_loss: 0.1425 - val_Sensitivity: 0.0000e+00 - val_tn: 37640.0000 - val_auc: 0.7059 - val_prc: 0.1102\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1349 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.6913 - prc: 0.1088 - val_loss: 0.1452 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7226 - val_prc: 0.1177\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1348 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.6923 - prc: 0.1092 - val_loss: 0.1454 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7191 - val_prc: 0.1193\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1350 - Sensitivity: 2.6116e-04 - tn: 112963.0000 - auc: 0.6897 - prc: 0.1069 - val_loss: 0.1456 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7165 - val_prc: 0.1170\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1342 - Sensitivity: 2.6116e-04 - tn: 112961.0000 - auc: 0.6960 - prc: 0.1121 - val_loss: 0.1467 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7138 - val_prc: 0.1159\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1344 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.6957 - prc: 0.1082 - val_loss: 0.1453 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7165 - val_prc: 0.1175\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1340 - Sensitivity: 2.6116e-04 - tn: 112963.0000 - auc: 0.7007 - prc: 0.1116 - val_loss: 0.1438 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7162 - val_prc: 0.1155\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1341 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.6983 - prc: 0.1086 - val_loss: 0.1446 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7152 - val_prc: 0.1156\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7006 - prc: 0.1125 - val_loss: 0.1463 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7137 - val_prc: 0.1160\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1336 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.7032 - prc: 0.1121 - val_loss: 0.1452 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7142 - val_prc: 0.1125\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1335 - Sensitivity: 7.8349e-04 - tn: 112962.0000 - auc: 0.7032 - prc: 0.1168 - val_loss: 0.1452 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7128 - val_prc: 0.1118\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1331 - Sensitivity: 0.0021 - tn: 112958.0000 - auc: 0.7077 - prc: 0.1152 - val_loss: 0.1449 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7115 - val_prc: 0.1134\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1330 - Sensitivity: 5.2233e-04 - tn: 112962.0000 - auc: 0.7092 - prc: 0.1160 - val_loss: 0.1470 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7093 - val_prc: 0.1125\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1332 - Sensitivity: 2.6116e-04 - tn: 112962.0000 - auc: 0.7056 - prc: 0.1171 - val_loss: 0.1452 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7115 - val_prc: 0.1125\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1326 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.7128 - prc: 0.1162 - val_loss: 0.1467 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7082 - val_prc: 0.1112\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1334 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7049 - prc: 0.1123 - val_loss: 0.1431 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7133 - val_prc: 0.1119\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7078 - prc: 0.1174 - val_loss: 0.1445 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7109 - val_prc: 0.1115\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1330 - Sensitivity: 7.8349e-04 - tn: 112962.0000 - auc: 0.7093 - prc: 0.1167 - val_loss: 0.1450 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7105 - val_prc: 0.1099\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1333 - Sensitivity: 0.0000e+00 - tn: 112961.0000 - auc: 0.7042 - prc: 0.1155 - val_loss: 0.1468 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7083 - val_prc: 0.1099\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1326 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7132 - prc: 0.1170 - val_loss: 0.1439 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7125 - val_prc: 0.1111\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1329 - Sensitivity: 0.0000e+00 - tn: 112960.0000 - auc: 0.7094 - prc: 0.1199 - val_loss: 0.1463 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7090 - val_prc: 0.1118\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1329 - Sensitivity: 2.6116e-04 - tn: 112960.0000 - auc: 0.7083 - prc: 0.1190 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7088 - val_prc: 0.1113\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1324 - Sensitivity: 2.6116e-04 - tn: 112960.0000 - auc: 0.7155 - prc: 0.1184 - val_loss: 0.1461 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7061 - val_prc: 0.1098\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1328 - Sensitivity: 0.0000e+00 - tn: 112963.0000 - auc: 0.7127 - prc: 0.1133 - val_loss: 0.1450 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7099 - val_prc: 0.1108\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0000e+00 - tn: 112961.0000 - auc: 0.7093 - prc: 0.1170 - val_loss: 0.1459 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7107 - val_prc: 0.1104\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1328 - Sensitivity: 0.0000e+00 - tn: 112962.0000 - auc: 0.7113 - prc: 0.1190 - val_loss: 0.1446 - val_Sensitivity: 0.0000e+00 - val_tn: 37593.0000 - val_auc: 0.7100 - val_prc: 0.1091\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1346 - Sensitivity: 2.6042e-04 - tn: 112953.0000 - auc: 0.6982 - prc: 0.1080 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7139 - val_prc: 0.1286\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1349 - Sensitivity: 0.0013 - tn: 112953.0000 - auc: 0.6980 - prc: 0.1066 - val_loss: 0.1432 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7111 - val_prc: 0.1267\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1343 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7008 - prc: 0.1077 - val_loss: 0.1428 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7141 - val_prc: 0.1262\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1343 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7014 - prc: 0.1071 - val_loss: 0.1447 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7109 - val_prc: 0.1261\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1341 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7034 - prc: 0.1096 - val_loss: 0.1419 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7102 - val_prc: 0.1259\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1339 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7044 - prc: 0.1115 - val_loss: 0.1445 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7091 - val_prc: 0.1244\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 2s 11ms/step - loss: 0.1339 - Sensitivity: 2.6042e-04 - tn: 112952.0000 - auc: 0.7069 - prc: 0.1105 - val_loss: 0.1445 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7064 - val_prc: 0.1232\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 2.6042e-04 - tn: 112952.0000 - auc: 0.7062 - prc: 0.1116 - val_loss: 0.1437 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7053 - val_prc: 0.1236\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1337 - Sensitivity: 2.6042e-04 - tn: 112952.0000 - auc: 0.7085 - prc: 0.1130 - val_loss: 0.1428 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7067 - val_prc: 0.1259\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1335 - Sensitivity: 2.6042e-04 - tn: 112952.0000 - auc: 0.7085 - prc: 0.1171 - val_loss: 0.1423 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7076 - val_prc: 0.1260\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1336 - Sensitivity: 2.6042e-04 - tn: 112952.0000 - auc: 0.7095 - prc: 0.1094 - val_loss: 0.1428 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7062 - val_prc: 0.1256\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1330 - Sensitivity: 0.0000e+00 - tn: 112952.0000 - auc: 0.7154 - prc: 0.1137 - val_loss: 0.1426 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7041 - val_prc: 0.1227\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1334 - Sensitivity: 0.0000e+00 - tn: 112953.0000 - auc: 0.7094 - prc: 0.1142 - val_loss: 0.1442 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7037 - val_prc: 0.1219\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 3s 11ms/step - loss: 0.1332 - Sensitivity: 2.6042e-04 - tn: 112951.0000 - auc: 0.7127 - prc: 0.1143 - val_loss: 0.1435 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7048 - val_prc: 0.1228\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 3s 12ms/step - loss: 0.1338 - Sensitivity: 2.6042e-04 - tn: 112952.0000 - auc: 0.7056 - prc: 0.1135 - val_loss: 0.1435 - val_Sensitivity: 0.0000e+00 - val_tn: 37603.0000 - val_auc: 0.7042 - val_prc: 0.1209\n"
     ]
    }
   ],
   "source": [
    "#split each training set into train/val, preserving 60/20/20 ratio, and fit the model\n",
    "annpreds = []\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=0)\n",
    "    model4.fit(X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping])\n",
    "    annpreds.append(model4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate auroc for each of the 5 folds\n",
    "ann_score = []\n",
    "for x in range(0,5):\n",
    "    ann_score.append(roc_auc_score(dy_test[x], annpreds[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6796136849853632,\n",
       " 0.7188704847756474,\n",
       " 0.7203801109727683,\n",
       " 0.7082039539384952,\n",
       " 0.7319146120276016]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712 (0.687-0.736)\n"
     ]
    }
   ],
   "source": [
    "#find the mean auroc and 95% CI\n",
    "ann_mean = np.mean(ann_score)\n",
    "ann_confidence = st.t.interval(0.95, len(ann_score)-1, loc=ann_mean, scale=st.sem(ann_score))\n",
    "\n",
    "print(round(ann_mean,3), '('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(dX_train[0], dy_train[0])\n",
    "def rf_feat_importance(model, X):\n",
    "    return pd.DataFrame({'cols':X.columns, 'imp':model.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "fi = rf_feat_importance(model, X)\n",
    "fi[:10]\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fit(dX_train[0], dy_train[0])\n",
    "fi = rf_feat_importance(model2, X)\n",
    "plot_fi(fi[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'lrpreds' (list)\n"
     ]
    }
   ],
   "source": [
    "#train the logistic regression model, max_iter set to max due to large dataset size, no validation set needed for LR (no early stopping)\n",
    "lrpreds = []\n",
    "model3 = LogisticRegression(max_iter=9999)\n",
    "for X, y, X_test in zip(dX_train, dy_train, dX_test):\n",
    "    model3.fit(X, y)\n",
    "    lrpreds.append(model3.predict_proba(X_test))\n",
    "%store lrpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score the LR model on 5 folds\n",
    "lr_score = []\n",
    "for x in range(0,5):\n",
    "    lr_score.append(roc_auc_score(dy_test[x], lrpreds[x][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.678 (0.675-0.681)\n"
     ]
    }
   ],
   "source": [
    "#find the mean and 95% CI\n",
    "lr_mean = np.mean(lr_score)\n",
    "lr_confidence = st.t.interval(0.95, len(lr_score)-1, loc=lr_mean, scale=st.sem(lr_score))\n",
    "\n",
    "print('Logistic Regression:', round(lr_mean,3), '('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 0.122 (0.106-0.137)\n",
      "Logistic Regression: 0.096 (0.09-0.103)\n"
     ]
    }
   ],
   "source": [
    "#write results to txt file\n",
    "ann_prc = []\n",
    "for x in range(0,5):\n",
    "    ann_prc.append(average_precision_score(dy_test[x], annpreds[x]))\n",
    "\n",
    "ann_prc_mean = np.mean(ann_prc)\n",
    "ann_prc_confidence = st.t.interval(0.95, len(ann_prc)-1, loc=ann_prc_mean, scale=st.sem(ann_prc))\n",
    "\n",
    "print('Neural Network:', round(ann_prc_mean,3), '('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')\n",
    "lr_prc = []\n",
    "for x in range(0,5):\n",
    "    lr_prc.append(average_precision_score(dy_test[x], lrpreds[x][:,1]))\n",
    "\n",
    "lr_prc_mean = np.mean(lr_prc)\n",
    "lr_prc_confidence = st.t.interval(0.95, len(lr_prc)-1, loc=lr_prc_mean, scale=st.sem(lr_prc))\n",
    "\n",
    "print('Logistic Regression:', round(lr_prc_mean,3), '('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')')\n",
    "with open('col_results.txt', 'w') as f:\n",
    "    f.write('Logistic Regression: '+str(round(lr_mean,3))+' ('+str(round(lr_confidence[0],3))+'-'+str(round(lr_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_mean,3))+' ('+str(round(ann_confidence[0],3))+'-'+str(round(ann_confidence[1],3))+')\\n')\n",
    "    f.write('AUPRC\\n')\n",
    "    f.write('Logistic Regression: '+str(round(lr_prc_mean,3))+' ('+str(round(lr_prc_confidence[0],3))+'-'+str(round(lr_prc_confidence[1],3))+')\\n')\n",
    "    f.write('Neural Network: '+str(round(ann_prc_mean,3))+' ('+str(round(ann_prc_confidence[0],3))+'-'+str(round(ann_prc_confidence[1],3))+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mean_col_ann_tpr' (list)\n",
      "Stored 'mean_col_ann_fpr' (list)\n",
      "Stored 'mean_col_lr_tpr' (list)\n",
      "Stored 'mean_col_lr_fpr' (list)\n",
      "Stored 'mean_col_lr_rec' (list)\n",
      "Stored 'mean_col_lr_prec' (list)\n",
      "Stored 'mean_col_ann_rec' (list)\n",
      "Stored 'mean_col_ann_prec' (list)\n"
     ]
    }
   ],
   "source": [
    "#generate fpr and tpr to create ROC curves\n",
    "col_ann_tpr = []\n",
    "col_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_tpr.append(tpr)\n",
    "    col_ann_fpr.append(fpr)\n",
    "\n",
    "#sklearn's roc_curve function created fpr/tpr of different lengths (different number of thresholds)\n",
    "#averaging different length lists results in truncated ROC curves\n",
    "#randomly delete values to make each list the same length\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_tpr[x]))\n",
    "        col_ann_tpr[x] = np.delete(col_ann_tpr[x],ind)\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_fpr[x]))\n",
    "        col_ann_fpr[x] = np.delete(col_ann_fpr[x],ind)\n",
    "\n",
    "#find the mean tpr's and fpr's\n",
    "mean_col_ann_tpr = [np.mean(k) for k in zip(*col_ann_tpr)]\n",
    "mean_col_ann_fpr = [np.mean(k) for k in zip(*col_ann_fpr)]\n",
    "\n",
    "#save the means to use in the curves notebook\n",
    "%store mean_col_ann_tpr\n",
    "%store mean_col_ann_fpr\n",
    "\n",
    "#same process for LR\n",
    "col_lr_tpr = []\n",
    "col_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_tpr.append(tpr)\n",
    "    col_lr_fpr.append(fpr)\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_tpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_tpr[x]))\n",
    "        col_lr_tpr[x] = np.delete(col_lr_tpr[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_fpr[x]) - 1000\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_fpr[x]))\n",
    "        col_lr_fpr[x] = np.delete(col_lr_fpr[x],ind)\n",
    "\n",
    "mean_col_lr_tpr = [np.mean(k) for k in zip(*col_lr_tpr)]\n",
    "mean_col_lr_fpr = [np.mean(k) for k in zip(*col_lr_fpr)]\n",
    "%store mean_col_lr_tpr\n",
    "%store mean_col_lr_fpr\n",
    "\n",
    "#same process for precision-recall curves\n",
    "col_lr_rec = []\n",
    "col_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_rec.append(rec)\n",
    "    col_lr_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_rec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_rec[x]))\n",
    "        col_lr_rec[x] = np.delete(col_lr_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_lr_prec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_lr_prec[x]))\n",
    "        col_lr_prec[x] = np.delete(col_lr_prec[x],ind)\n",
    "\n",
    "mean_col_lr_rec = [np.mean(k) for k in zip(*col_lr_rec)]\n",
    "\n",
    "mean_col_lr_prec = [np.mean(k) for k in zip(*col_lr_prec)]\n",
    "%store mean_col_lr_rec\n",
    "%store mean_col_lr_prec\n",
    "col_ann_rec = []\n",
    "col_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_rec.append(rec)\n",
    "    col_ann_prec.append(prec)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_rec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_rec[x]))\n",
    "        col_ann_rec[x] = np.delete(col_ann_rec[x],ind)\n",
    "\n",
    "for x in range(0,5):\n",
    "    diff = len(col_ann_prec[x]) - 4500\n",
    "    for _ in range(diff):\n",
    "        ind = randrange(len(col_ann_prec[x]))\n",
    "        col_ann_prec[x] = np.delete(col_ann_prec[x],ind)\n",
    "\n",
    "mean_col_ann_rec = [np.mean(k) for k in zip(*col_ann_rec)]\n",
    "\n",
    "mean_col_ann_prec = [np.mean(k) for k in zip(*col_ann_prec)]\n",
    "%store mean_col_ann_rec\n",
    "%store mean_col_ann_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_ann_tpr = []\n",
    "col_ann_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_tpr.append(tpr)\n",
    "    col_ann_fpr.append(fpr)\n",
    "col_ann_tpr_array = [np.array(x) for x in col_ann_tpr]\n",
    "mean_col_ann_tpr = [np.mean(k) for k in zip(*col_ann_tpr_array)]\n",
    "col_ann_fpr_array = [np.array(x) for x in col_ann_fpr]\n",
    "mean_col_ann_fpr = [np.mean(k) for k in zip(*col_ann_fpr_array)]\n",
    "%store mean_col_ann_tpr\n",
    "%store mean_col_ann_fpr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_lr_tpr = []\n",
    "col_lr_fpr = []\n",
    "for x in range(0,5):\n",
    "    fpr, tpr, _ = roc_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_tpr.append(tpr)\n",
    "    col_lr_fpr.append(fpr)\n",
    "col_lr_tpr_array = [np.array(x) for x in col_lr_tpr]\n",
    "mean_col_lr_tpr = [np.mean(k) for k in zip(*col_lr_tpr_array)]\n",
    "col_lr_fpr_array = [np.array(x) for x in col_lr_fpr]\n",
    "mean_col_lr_fpr = [np.mean(k) for k in zip(*col_lr_fpr_array)]\n",
    "%store mean_col_lr_tpr\n",
    "%store mean_col_lr_fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_ann_rec = []\n",
    "col_ann_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], annpreds[x])\n",
    "    col_ann_rec.append(rec)\n",
    "    col_ann_prec.append(prec)\n",
    "col_ann_rec_array = [np.array(x) for x in col_ann_rec]\n",
    "mean_col_ann_rec = [np.mean(k) for k in zip(*col_ann_rec_array)]\n",
    "col_ann_prec_array = [np.array(x) for x in col_ann_prec]\n",
    "mean_col_ann_prec = [np.mean(k) for k in zip(*col_ann_prec_array)]\n",
    "%store mean_col_ann_rec\n",
    "%store mean_col_ann_prec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col_lr_rec = []\n",
    "col_lr_prec = []\n",
    "for x in range(0,5):\n",
    "    prec, rec, _ = precision_recall_curve(dy_test[x], lrpreds[x][:,1])\n",
    "    col_lr_rec.append(rec)\n",
    "    col_lr_prec.append(prec)\n",
    "col_lr_rec_array = [np.array(x) for x in col_lr_rec]\n",
    "mean_col_lr_rec = [np.mean(k) for k in zip(*col_lr_rec_array)]\n",
    "col_lr_prec_array = [np.array(x) for x in col_lr_prec]\n",
    "mean_col_lr_prec = [np.mean(k) for k in zip(*col_lr_prec_array)]\n",
    "%store mean_col_lr_rec\n",
    "%store mean_col_lr_prec\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
